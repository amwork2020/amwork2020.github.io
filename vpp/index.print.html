<!DOCTYPE html>
<html lang="en" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="height=device-height, width=device-width, initial-scale=1.0, minimum-scale=1.0">
    <meta name="generator" content="Hugo 0.110.0">
    <meta name="generator" content="Relearn 5.10.2+tip">
    <meta name="description" content="Documentation for Hugo Relearn Theme">
    <meta name="author" content="amwork2010">
    <title>VPP :: amwork2010 blog</title>
    <link href="https://example.com/vpp/index.html" rel="canonical" type="text/html" title="VPP :: amwork2010 blog">
    <link href="../vpp/index.xml" rel="alternate" type="application/rss+xml" title="VPP :: amwork2010 blog">
    <!-- https://github.com/filamentgroup/loadCSS/blob/master/README.md#how-to-use -->
    <link href="../css/fontawesome-all.min.css?1675933203" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="../css/fontawesome-all.min.css?1675933203" rel="stylesheet"></noscript>
    <link href="../css/auto-complete.css?1675933203" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="../css/auto-complete.css?1675933203" rel="stylesheet"></noscript>
    <link href="../css/perfect-scrollbar.min.css?1675933203" rel="stylesheet">
    <link href="../css/nucleus.css?1675933203" rel="stylesheet">
    <link href="../css/fonts.css?1675933203" rel="stylesheet" media="print" onload="this.media='all';this.onload=null;"><noscript><link href="../css/fonts.css?1675933203" rel="stylesheet"></noscript>
    <link href="../css/theme.css?1675933203" rel="stylesheet">
    <link href="../css/theme-relearn-light.css?1675933203" rel="stylesheet" id="variant-style">
    <link href="../css/ie.css?1675933203" rel="stylesheet">
    <link href="../css/variant.css?1675933203" rel="stylesheet">
    <link href="../css/print.css?1675933203" rel="stylesheet" media="print">
    <link href="../css/format-print.css?1675933203" rel="stylesheet">
    <script src="../js/url.js?1675933203"></script>
    <script src="../js/variant.js?1675933203"></script>
    <script>
      // hack to let hugo tell us how to get to the root when using relativeURLs, it needs to be called *url= for it to do its magic:
      // https://github.com/gohugoio/hugo/blob/145b3fcce35fbac25c7033c91c1b7ae6d1179da8/transform/urlreplacers/absurlreplacer.go#L72
      window.index_js_url="../index.search.js";
      var root_url="../";
      var baseUri=root_url.replace(/\/$/, '');
      // translations
      window.T_Copy_to_clipboard = 'Copy to clipboard';
      window.T_Copied_to_clipboard = 'Copied to clipboard!';
      window.T_Copy_link_to_clipboard = 'Copy link to clipboard';
      window.T_Link_copied_to_clipboard = 'Copied link to clipboard!';
      window.T_No_results_found = 'No results found for \u0022{0}\u0022';
      window.T_N_results_found = '{1} results found for \u0022{0}\u0022';
      // some further base stuff
      var baseUriFull='https:\/\/example.com/';
      window.variants && variants.init( [ 'relearn-light', 'relearn-dark', 'learn', 'neon', 'blue', 'green', 'red' ] );
    </script>
    <script src="../js/jquery.min.js?1675933203" defer></script>
  </head>
  <body class="mobile-support print disableInlineCopyToClipboard" data-url="../vpp/index.html">
    <div id="body" class="default-animation">
      <div id="sidebar-overlay"></div>
      <div id="toc-overlay"></div>
      <nav id="topbar" class="highlightable" dir="ltr">
        <div>
          <div id="breadcrumbs">
            <span id="sidebar-toggle-span">
              <a href="#" id="sidebar-toggle" title='Menu (CTRL+ALT+n)'><i class="fas fa-bars fa-fw"></i></a>
            </span>
            <ol class="links" itemscope itemtype="http://schema.org/BreadcrumbList">
              <li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement"><a itemprop="item" href="../index.html"><span itemprop="name">blogs of amwork2010</span></a><meta itemprop="position" content="1"> > </li>
              <li itemscope itemtype="https://schema.org/ListItem" itemprop="itemListElement"><span itemprop="name">VPP</span><meta itemprop="position" content="2"></li>
            </ol>
          </div>
        </div>
      </nav>
      <main id="body-inner" class="highlightable default" tabindex="-1">
        <div class="flex-block-wrapper">
          <div id="head-tags">
          </div>
          <article class="default">
<h1 id="vpp">VPP</h1>

<h2 id="blogs-of-amwork2010">blogs of amwork2010</h2>
<img src="../img/vpp/fdio-vpp.png" width="500px">
<p>日期: 2023-02-04</p>

            <footer class="footline">
            </footer>
          </article>

          <section>
            <h1 class="a11y-only">Subsections of VPP</h1>
          <article class="default">
<h1 id="0-vpp-u2204">0. vpp-u2204</h1>

<p>date: 2023-02-04 <br>
<a href="https://wiki.fd.io/view/VPP/Progressive_VPP_Tutorial" target="_blank">https://wiki.fd.io/view/VPP/Progressive_VPP_Tutorial</a>         <br>
<a href="https://s3-docs.fd.io/vpp/22.10/" target="_blank">https://s3-docs.fd.io/vpp/22.10/</a></p>
<p>vpp u2204    vmware : 网卡均为vmxnet3

<a href="#image-a4cefffb57a8d5e3723d5422adaa18e7">
<img src="../img/vpp/1675491967190.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-a4cefffb57a8d5e3723d5422adaa18e7">
<img src="../img/vpp/1675491967190.png" alt=""loading="lazy">
</a>

<a href="#image-1b09d63be5eb2d0be7094ea86a132115">
<img src="../img/vpp/1675492038037.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-1b09d63be5eb2d0be7094ea86a132115">
<img src="../img/vpp/1675492038037.png" alt=""loading="lazy">
</a>
<a href="https://packagecloud.io/fdio" target="_blank">https://packagecloud.io/fdio</a>       <br>

<a href="#image-cbb1eb95cedb202387785df85f4086cb">
<img src="../img/vpp/1675492103211.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-cbb1eb95cedb202387785df85f4086cb">
<img src="../img/vpp/1675492103211.png" alt=""loading="lazy">
</a>

<a href="#image-006e8c4e661f51042c95d0ca49559acf">
<img src="../img/vpp/1675492117477.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-006e8c4e661f51042c95d0ca49559acf">
<img src="../img/vpp/1675492117477.png" alt=""loading="lazy">
</a></p>
<pre tabindex="0"><code>apt install driverctl
driverctl list-devices | grep vmxnet3

0000:0b:00.0 vmxnet3
0000:13:00.0 vmxnet3
0000:1b:00.0 vmxnet3

driverctl set-override 0000:13:00.0 vfio-pci   # 重启后仍然绑定
driverctl unset-override 0000:13:00.0        # 解除绑定
dpdk-devbind.py -b vfio-pci 0000:13:00.0     # 重启后解除绑定
apt install dpdk dpdk-dev -y

## 22.10
curl -s https://packagecloud.io/install/repositories/fdio/release/script.deb.sh | sudo bash -vx
cat /etc/apt/sources.list.d/fdio_release.list 

apt-get update
apt-get install vpp vpp-plugin-core vpp-plugin-dpdk
# apt-get install vpp-api-python python3-vpp-api vpp-dbg vpp-dev
# Uninstall the Packages
# apt-get remove --purge vpp*
# systemctl enable vpp
systemctl status vpp

vi /lib/systemd/system/vpp.service
ExecStartPre=-/sbin/modprobe vfio-pci

vi /etc/vpp/startup.conf
dpdk {
	uio-driver vfio-pci
	dev 0000:13:00.0
	dev 0000:1b:00.0
}

systemctl daemon-reload &amp;&amp; systemctl restart vpp &amp;&amp; systemctl status vpp

vppctl show ver

vppctl
提示符：
vpp#
set interface state GigabitEthernet13/0/0  up
set interface state GigabitEthernet1b/0/0  up

set interface ip address GigabitEthernet13/0/0  1.1.1.1/24
set interface ip address GigabitEthernet1b/0/0  2.1.1.1/24

show interface
</code></pre><p>
<a href="#image-c3c54a72aba50184816debf95ebc00ff">
<img src="../img/vpp/1675492185670.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-c3c54a72aba50184816debf95ebc00ff">
<img src="../img/vpp/1675492185670.png" alt=""loading="lazy">
</a>

<a href="#image-05155464fa9cdee1b986127696e8de39">
<img src="../img/vpp/1675492889133.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-05155464fa9cdee1b986127696e8de39">
<img src="../img/vpp/1675492889133.png" alt=""loading="lazy">
</a></p>
<p>ubuntu0:   <br>

<a href="#image-eaa0f75f8a91592b9564d654f40c167c">
<img src="../img/vpp/1675493013634.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-eaa0f75f8a91592b9564d654f40c167c">
<img src="../img/vpp/1675493013634.png" alt=""loading="lazy">
</a></p>
<p>ubuntu1:                 <br>

<a href="#image-8f6d36ff342ca08b84713b65635c648c">
<img src="../img/vpp/1675493063908.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-8f6d36ff342ca08b84713b65635c648c">
<img src="../img/vpp/1675493063908.png" alt=""loading="lazy">
</a></p>
<p>ubuntu0 与 ubuntu1 可以互相ping通。 <br>

<a href="#image-62f5225cac79ed9baaf93a4049a9f5bf">
<img src="../img/vpp/1675493132374.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-62f5225cac79ed9baaf93a4049a9f5bf">
<img src="../img/vpp/1675493132374.png" alt=""loading="lazy">
</a>

<a href="#image-66a307bd5cd5e196905e05002ebee35f">
<img src="../img/vpp/1675493175692.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-66a307bd5cd5e196905e05002ebee35f">
<img src="../img/vpp/1675493175692.png" alt=""loading="lazy">
</a></p>
<p>ubuntu0:   <br>
iperf -s -i 1</p>
<p>ubuntu1:   <br>
iperf -t 10 -i 1 -c 1.1.1.2  <br>

<a href="#image-961bc827cd715e479779b5457de9282b">
<img src="../img/vpp/1675493241295.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-961bc827cd715e479779b5457de9282b">
<img src="../img/vpp/1675493241295.png" alt=""loading="lazy">
</a>
dpdk有2-3G，网卡是10G</p>
<p>不用dpdk，重启后：</p>
<pre tabindex="0"><code>ip l s ens224 up
ip l s ens256 up
ip a a 1.1.1.1/24 dev ens224
ip a a 2.1.1.1/24 dev ens256
</code></pre><p>路由方式1G，这个对的。   <br>

<a href="#image-f8e62b78395439c31d8970d9aac2a379">
<img src="../img/vpp/1675493343777.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-f8e62b78395439c31d8970d9aac2a379">
<img src="../img/vpp/1675493343777.png" alt=""loading="lazy">
</a></p>
<p>clean:</p>
<pre tabindex="0"><code>ip a flush ens224
ip a flush ens256
ip l s ens224 down
ip l s ens256 down
</code></pre>
            <footer class="footline">
            </footer>
          </article>

          <article class="default">
<h1 id="0-vpp-qemu">0. vpp-qemu</h1>

<p>date: 2023-02-04</p>
<pre tabindex="0"><code># vm.xml
		&lt;interface type=&#39;bridge&#39;&gt;
			&lt;source bridge=&#39;br0&#39;/&gt;
			&lt;model type=&#39;vmxnet3&#39;/&gt;
		&lt;/interface&gt;
		&lt;interface type=&#39;bridge&#39;&gt;
			&lt;source bridge=&#39;br0&#39;/&gt;
			&lt;model type=&#39;vmxnet3&#39;/&gt;
		&lt;/interface&gt;

## HOST : 10.1.1.8
../startVMs/startvm.sh vpp_5.151 jammy-server-cloudimg-amd64.20221210.pw1-10.1.5.3.img 4 8

######### 0.prepare
apt update
apt -y full-upgrade
ln -sf ../usr/share/zoneinfo/Asia/Shanghai /etc/localtime
[ -f /var/run/reboot-required ] &amp;&amp; reboot -f

######### 1. 启用rc.local
cat &lt;&lt; EOF &gt;&gt; /etc/rc.local
#!/bin/bash
echo 1 &gt; /sys/module/vfio/parameters/enable_unsafe_noiommu_mode
EOF

chmod +x /etc/rc.local

cat &lt;&lt; EOF &gt;&gt; /lib/systemd/system/rc-local.service

[Install]
WantedBy=multi-user.target
EOF

cat /lib/systemd/system/rc-local.service

# 启用服务
systemctl enable rc-local
systemctl start rc-local
systemctl status rc-local
# 查看是否成功
cat /sys/module/vfio/parameters/enable_unsafe_noiommu_mode

echo &#34;vfio-pci&#34; &gt; /etc/modules-load.d/95-vpp.conf

######### 2. hugepages 
cat &lt;&lt;EOF &gt;&gt; /etc/sysctl.conf 
vm.nr_hugepages = 2048
EOF

sysctl -p

######### 3. vpp &amp;&amp; dpdk
lshw -businfo -c network
apt install dpdk dpdk-dev -y

## https://packagecloud.io/fdio
#curl -s https://packagecloud.io/install/repositories/fdio/release/script.deb.sh | sudo bash
curl -s https://packagecloud.io/install/repositories/fdio/2210/script.deb.sh | sudo bash
cat /etc/apt/sources.list.d/fdio_2210.list
apt update
apt install vpp vpp-plugin-core vpp-plugin-dpdk -y
# systemctl enable vpp
# systemctl disable vpp
systemctl status vpp
mkdir -p /var/log/vpp
cat /etc/sysctl.d/80-vpp.conf 

vi /lib/systemd/system/vpp.service
ExecStartPre=-/sbin/modprobe vfio-pci
ExecStartPre=-/bin/bash -c &#39;echo 1 &gt; /sys/module/vfio/parameters/enable_unsafe_noiommu_mode &amp;&amp; sleep 2&#39;

vi /etc/vpp/startup.conf
dpdk {
	uio-driver vfio-pci
	dev 0000:00:04.0
}

systemctl daemon-reload &amp;&amp; systemctl restart vpp
## CPU0 100%
vppctl show ver

vppctl
show interface
</code></pre><pre tabindex="0"><code>apt install docker.io -y
docker pull ubuntu:22.04

vi Dockerfile 
FROM ubuntu:22.04
RUN sed -i &#39;s/archive.ubuntu.com/mirrors.ustc.edu.cn/g&#39; /etc/apt/sources.list &amp;&amp; \
    sed -i &#39;s/security.ubuntu.com/mirrors.ustc.edu.cn/g&#39; /etc/apt/sources.list
RUN apt-get update -y &amp;&amp; apt-get install -y tzdata &amp;&amp; \
    ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime &amp;&amp; \
    dpkg-reconfigure -f noninteractive tzdata
RUN apt-get install dpdk kmod curl vim -y
RUN curl -s https://packagecloud.io/install/repositories/fdio/release/script.deb.sh | bash &amp;&amp; \
    apt-get update -y &amp;&amp; apt-get install vpp vpp-plugin-core vpp-plugin-dpdk -y
RUN mkdir -p /var/log/vpp
### vpp v22.10-release
### dpdk 21.11

docker build -t amwork2010/vppdpdk:22.10 .

docker run --privileged \
 -v /sys/bus/pci/devices:/sys/bus/pci/devices \
 -v /sys/kernel/mm/hugepages:/sys/kernel/mm/hugepages \
 -v /sys/devices/system/node:/sys/devices/system/node \
 -v /lib/modules:/lib/modules \
 -v /dev:/dev \
 -it amwork2010/vppdpdk:22.10 bash

docker run --privileged \
 -v /sys/bus/pci/devices:/sys/bus/pci/devices \
 -v /sys/kernel/mm/hugepages:/sys/kernel/mm/hugepages \
 -v /sys/devices/system/node:/sys/devices/system/node \
 -v /lib/modules:/lib/modules \
 -v /dev:/dev \
 -d amwork2010/vppdpdk:22.10 sleep infinity

vi /etc/vpp/startup.conf
dpdk {
	uio-driver vfio-pci
	dev 0000:00:04.0
}

vpp -c /etc/vpp/startup.conf &amp;


#### 
docker run --privileged \
 -v /lib/modules:/lib/modules \
 -d amwork2010/vppdpdk:22.10 sleep infinity
# 也可以banding ,  kmod  ： the kmod package would provide modinfo, modprobe and other related tools.
# root@714c8d6fc89e:/# modprobe vfio-pci
# modprobe: FATAL: Module vfio-pci not found in directory /lib/modules/5.15.0-58-generic
# so:  -v /lib/modules:/lib/modules
# vpp 启动 带不到dpdk网卡，docker stop ， 再start，再 vpp -c /etc/vpp/startup.conf &amp; 才可以banding
</code></pre>
            <footer class="footline">
            </footer>
          </article>

          <article class="default">
<h1 id="0-vpp-rocky87">0. vpp-rocky87</h1>

<p>date: 2023-02-04 <br>
<a href="https://wiki.fd.io/view/VPP/Progressive_VPP_Tutorial" target="_blank">https://wiki.fd.io/view/VPP/Progressive_VPP_Tutorial</a>         <br>
<a href="https://s3-docs.fd.io/vpp/22.10/" target="_blank">https://s3-docs.fd.io/vpp/22.10/</a></p>
<p>Rocky Linux 8.7</p>
<pre tabindex="0"><code># rocky87
vi /etc/default/grub 
GRUB_CMDLINE_LINUX=&#34;........... default_hugepagesz=1G hugepagesz=1G hugepages=4 iommu=pt intel_iommu=on&#34;
grub2-mkconfig -o /boot/grub2/grub.cfg
reboot
[root@rocky87 ~]# lshw -c network -businfo
Bus info          Device          Class      Description
========================================================
pci@0000:04:00.0  ens161          network    VMXNET3 Ethernet Controller
pci@0000:0b:00.0  ens192          network    VMXNET3 Ethernet Controller
pci@0000:13:00.0  ens224          network    VMXNET3 Ethernet Controller
pci@0000:1b:00.0  ens256          network    VMXNET3 Ethernet Controller

yum install -y driverctl
driverctl list-devices | grep -i vmxnet3
[root@rocky87 ~]# driverctl list-devices | grep -i vmxnet3
0000:04:00.0 vmxnet3
0000:0b:00.0 vmxnet3
0000:13:00.0 vmxnet3
0000:1b:00.0 vmxnet3

yum install -y epel-release
sed -e &#39;s|^metalink=|#metalink=|g&#39; \
         -e &#39;s|^#baseurl=https\?://download.fedoraproject.org/pub/epel/|baseurl=https://mirrors.ustc.edu.cn/epel/|g&#39; \
         -e &#39;s|^#baseurl=https\?://download.example/pub/epel/|baseurl=https://mirrors.ustc.edu.cn/epel/|g&#39; \
         -i.bak \
         /etc/yum.repos.d/epel.repo
yum install -y mbedtls

# https://packagecloud.io/fdio
# curl -s https://packagecloud.io/install/repositories/fdio/release/script.rpm.sh | sudo bash
curl -s https://packagecloud.io/install/repositories/fdio/2106/script.rpm.sh | sudo bash

yum install -y vpp vpp-plugins vpp-devel vpp-debuginfo vpp-api-python3 vpp-api-lua vpp-ext-deps
#yum install -y vpp vpp-plugins vpp-devel vpp-debuginfo vpp-api-python3 vpp-api-lua vpp-ext-deps ### auto install vpp-lib vpp-selinux-policy

[root@rocky87 yum.repos.d]# yum install vpp vpp-plugins vpp-devel vpp-debuginfo vpp-api-python3 vpp-api-lua vpp-ext-deps
Last metadata expiration check: 0:02:59 ago on Sun 01 Jan 2023 10:30:25 AM CST.
Error: 
 Problem: cannot install the best candidate for the job
  - nothing provides libmbedcrypto.so.3()(64bit) needed by vpp-plugins-21.06-release.x86_64
  - nothing provides libmbedtls.so.12()(64bit) needed by vpp-plugins-21.06-release.x86_64
  - nothing provides libmbedx509.so.0()(64bit) needed by vpp-plugins-21.06-release.x86_64
(try to add &#39;--skip-broken&#39; to skip uninstallable packages or &#39;--nobest&#39; to use not only best candidate packages)

[root@rocky87 yum.repos.d]# rpm -ql mbedtls
...
/usr/lib64/libmbedcrypto.so.2.28.1
/usr/lib64/libmbedcrypto.so.7
/usr/lib64/libmbedtls.so.14
/usr/lib64/libmbedtls.so.2.28.1
/usr/lib64/libmbedx509.so.1
/usr/lib64/libmbedx509.so.2.28.1
...

cd /usr/lib64/
ln -s libmbedcrypto.so.7 libmbedcrypto.so.3
ln -s libmbedtls.so.14 libmbedtls.so.12
ln -s libmbedx509.so.1 libmbedx509.so.0

yum install -y vpp vpp-devel vpp-debuginfo vpp-api-python3 vpp-api-lua vpp-ext-deps
wget --content-disposition https://packagecloud.io/fdio/2106/packages/el/8/vpp-plugins-21.06.0-3~gbb25fbf28~b50.x86_64.rpm/download.rpm?distro_version_id=205
rpm -ivh vpp-plugins-21.06.0-3~gbb25fbf28~b50.x86_64.rpm --nodeps

修改配置同ubuntu

ip l s ens224 down
ip l s ens256 down
systemctl restart vpp
systemctl status vpp

自己build 失败！make install-dep 依赖包安装不全，名字也对不上，比如：python36-ply实际能安装python3-ply，python-virtualenv 实际 python3-virtualenv
devtoolset-9 devtoolset-9-libasan-devel 根本没有 gcc9的，安装的是gcc version 8.5.0 20210514 (Red Hat 8.5.0-15) (GCC) 

# build OK, 见下篇文档
</code></pre>
            <footer class="footline">
            </footer>
          </article>

          <article class="default">
<h1 id="1-vpp-rocky87-build">1. vpp-rocky87-build</h1>

<p>date: 2023-02-04 <br>
<a href="https://wiki.fd.io/view/VPP/Progressive_VPP_Tutorial" target="_blank">https://wiki.fd.io/view/VPP/Progressive_VPP_Tutorial</a>         <br>
<a href="https://s3-docs.fd.io/vpp/22.10/" target="_blank">https://s3-docs.fd.io/vpp/22.10/</a></p>
<p>Rocky Linux 8.7</p>
<pre tabindex="0"><code>vi /etc/default/grub 
GRUB_CMDLINE_LINUX=&#34;..... default_hugepagesz=1G hugepagesz=1G hugepages=4 iommu=pt intel_iommu=on&#34;

grub2-mkconfig -o /boot/grub2/grub.cfg
reboot

cp /etc/os-release /etc/os-release.bak
vi /etc/os-release
ID=&#34;rocky&#34; --&gt; centos
VERSION_ID=&#34;8.7&#34; --&gt; VERSION_ID=&#34;8&#34;
</code></pre><p>
<a href="#image-e0677080dbd2265dec9616b08991a1cb">
<img src="../img/vpp/1675493926849.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-e0677080dbd2265dec9616b08991a1cb">
<img src="../img/vpp/1675493926849.png" alt=""loading="lazy">
</a></p>
<pre tabindex="0"><code>yum -y groupinstall &#34;Development Tools&#34;
yum -y install git

export https_proxy=http://10.1.1.12:8118
export http_proxy=http://10.1.1.12:8118
git clone https://github.com/FDio/vpp.git
cd vpp
git branch -a
git checkout -b 2210 origin/stable/2210
git branch -a

make install-dep
make install-ext-dep
make pkg-rpm

#make build
#make build-release

cd /root
mkdir -p rpm
cd rpm/
mv /root/vpp/build/external/vpp-ext-deps-22.10-9.x86_64.rpm ./
mv /root/vpp/build-root/vpp-*rpm ./

[root@rocky87 rpm]# ll
total 201588
-rw-r--r-- 1 root root   255504 Jan 27 08:39 vpp-22.10.0-3~gb89dcf824.x86_64.rpm
-rw-r--r-- 1 root root    31608 Jan 27 08:39 vpp-api-lua-22.10.0-3~gb89dcf824.x86_64.rpm
-rw-r--r-- 1 root root    71388 Jan 27 08:39 vpp-api-python3-22.10.0-3~gb89dcf824.x86_64.rpm
-rw-r--r-- 1 root root  1212796 Jan 27 08:39 vpp-debuginfo-22.10.0-3~gb89dcf824.x86_64.rpm
-rw-r--r-- 1 root root  5591264 Jan 27 08:39 vpp-debugsource-22.10.0-3~gb89dcf824.x86_64.rpm
-rw-r--r-- 1 root root  1886688 Jan 27 08:39 vpp-devel-22.10.0-3~gb89dcf824.x86_64.rpm
-rw-r--r-- 1 root root 84047680 Jan 27 08:03 vpp-ext-deps-22.10-9.x86_64.rpm
-rw-r--r-- 1 root root  6461424 Jan 27 08:39 vpp-lib-22.10.0-3~gb89dcf824.x86_64.rpm
-rw-r--r-- 1 root root 50819564 Jan 27 08:39 vpp-lib-debuginfo-22.10.0-3~gb89dcf824.x86_64.rpm
-rw-r--r-- 1 root root 11108604 Jan 27 08:39 vpp-plugins-22.10.0-3~gb89dcf824.x86_64.rpm
-rw-r--r-- 1 root root 44888316 Jan 27 08:40 vpp-plugins-debuginfo-22.10.0-3~gb89dcf824.x86_64.rpm
-rw-r--r-- 1 root root    18520 Jan 27 08:39 vpp-selinux-policy-22.10.0-3~gb89dcf824.x86_64.rpm

yum install *.rpm
# 报错，conflicts with files，有冲突
yum install *.rpm --downloadonly
# 先把需要的依赖包安装
cd /var/cache/dnf/.....
yum install ....
# 用rpm强制安装
cd /root/rpm
rpm -ivh --force *.rpm

systemctl status vpp

vi /lib/systemd/system/vpp.service
ExecStartPre=-/sbin/modprobe vfio-pci

vi /etc/vpp/startup.conf
dpdk {
	uio-driver vfio-pci
	dev 0000:13:00.0
	dev 0000:1b:00.0
}
plugins {
	path /usr/lib/vpp_plugins
}

systemctl daemon-reload &amp;&amp; systemctl restart vpp &amp;&amp; systemctl status vpp
/opt/vpp/external/x86_64/bin/dpdk-devbind.py -s

vppctl show ver
vppctl show int
vppctl show plugins

export PATH=$PATH:/opt/vpp/external/x86_64/bin/
</code></pre><p>vmware:</p>
<pre tabindex="0"><code>### vmware
modprobe vfio-pci
ip l s ens224 down
ip l s ens256 down
dpdk-devbind.py -b vfio-pci 0000:13:00.0 0000:1b:00.0

vi /lib/systemd/system/vpp.service
ExecStartPre=-/usr/sbin/ip l s ens224 down
ExecStartPre=-/usr/sbin/ip l s ens256 down
ExecStartPre=-/sbin/modprobe vfio-pci

## 查看网卡信息
/opt/vpp/external/x86_64/bin/dpdk-devbind.py -s

vi /etc/vpp/startup.conf
dpdk {
	uio-driver vfio-pci
	dev 0000:13:00.0
	dev 0000:1b:00.0
}

plugins {
	path /usr/lib/vpp_plugins
}

systemctl daemon-reload &amp;&amp; systemctl restart vpp &amp;&amp; systemctl status vpp
/opt/vpp/external/x86_64/bin/dpdk-devbind.py -s
</code></pre><p>qemu:</p>
<pre tabindex="0"><code>### qemu
### 以下可以不用做 BEGIN
echo &#34;vfio-pci&#34; &gt; /etc/modules-load.d/95-vpp.conf

cat &lt;&lt; EOF &gt;&gt; /etc/rc.local
echo 1 &gt; /sys/module/vfio/parameters/enable_unsafe_noiommu_mode
EOF

chmod +x /etc/rc.local
### 可以不用做 END

vi /lib/systemd/system/vpp.service
After=syslog.target network.target auditd.service NetworkManager-wait-online.service

ExecStartPre=-/usr/sbin/ip l s eth1 down
ExecStartPre=-/usr/sbin/ip l s eth2 down
ExecStartPre=-/sbin/modprobe vfio-pci
ExecStartPre=-/bin/bash -c &#39;echo 1 &gt; /sys/module/vfio/parameters/enable_unsafe_noiommu_mode &amp;&amp; sleep 2&#39;

## 查看网卡信息
export PATH=$PATH:/opt/vpp/external/x86_64/bin/
/opt/vpp/external/x86_64/bin/dpdk-devbind.py -s

vi /etc/vpp/startup.conf
dpdk {
	uio-driver vfio-pci
	dev 0000:00:04.0
	dev 0000:00:05.0
}

plugins {
	path /usr/lib/vpp_plugins
}

systemctl daemon-reload &amp;&amp; systemctl restart vpp &amp;&amp; systemctl status vpp
/opt/vpp/external/x86_64/bin/dpdk-devbind.py -s

vppctl show ver
vppctl show int
vppctl show plugins

top -H
# CPU0 100%
</code></pre>
            <footer class="footline">
            </footer>
          </article>

          <article class="default">
<h1 id="2-vpp-tutorial">2. VPP Tutorial</h1>

<p>date: 2023-02-04</p>
<ol>
<li>Create a veth interface     <br>
<a href="https://s3-docs.fd.io/vpp/22.10/gettingstarted/progressivevpp/index.html" target="_blank">https://s3-docs.fd.io/vpp/22.10/gettingstarted/progressivevpp/index.html</a></li>
</ol>
<pre tabindex="0"><code>ip link add name ns1host type veth peer name ns1vpp
ip netns add vns1
ip link set ns1host netns vns1
ip netns exec vns1 ifconfig ns1host 1.1.1.2/24 up
ip netns exec vns1 route add -net 2.2.2.0/24 gw 1.1.1.1

ip link add name ns2host type veth peer name ns2vpp
ip netns add vns2
ip link set ns2host netns vns2
ip netns exec vns2 ifconfig ns2host 2.2.2.2/24 up
ip netns exec vns2 route add -net 1.1.1.0/24 gw 2.2.2.1
</code></pre><p>
<a href="#image-a61e64912306afce5bca06bc190283e0">
<img src="../img/vpp/1675495074042.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-a61e64912306afce5bca06bc190283e0">
<img src="../img/vpp/1675495074042.png" alt=""loading="lazy">
</a></p>
<p>vppctl</p>
<pre tabindex="0"><code>create host-interface name ns1vpp
create host-interface name ns2vpp
</code></pre><p>
<a href="#image-969b90ea69edae360cce30672388dd5c">
<img src="../img/vpp/1675495146074.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-969b90ea69edae360cce30672388dd5c">
<img src="../img/vpp/1675495146074.png" alt=""loading="lazy">
</a></p>
<pre tabindex="0"><code>show interface 

set int state host-ns1vpp up
set int state host-ns2vpp up
set int ip address host-ns1vpp 1.1.1.1/24
set int ip address host-ns2vpp 2.2.2.1/24

show ip fib
show int addr

trace add af-packet-input 10
show trace
clear trace
</code></pre><p>ip netns exec vns1 ip a   <br>
ip netns exec vns1 ping -c 1 2.2.2.2</p>
<pre tabindex="0"><code>show ip neighbors
show ip fib
show hardware-interfaces

host-ns1vpp --&gt; 02:fe:f8:9f:d7:c3
host-ns2vpp --&gt; 02:fe:5b:e3:0b:b4
</code></pre><pre tabindex="0"><code>ip netns exec vns1 ip a
ns1host  32:52:66:6e:76:44

ip netns exec vns2 ip a
ns2host  f6:b5:34:dc:25:c5
</code></pre><p>ICMP: 1.1.1.2 -&gt; 2.2.2.2</p>
<p>32:52:66:6e:76:44 -&gt; 02:fe:f8:9f:d7:c3</p>
<p>02:fe:5b:e3:0b:b4 -&gt; f6:b5:34:dc:25:c5</p>
<p><a href="https://wiki.fd.io/view/VPP/Configure_VPP_As_A_Router_Between_Namespaces" target="_blank">https://wiki.fd.io/view/VPP/Configure_VPP_As_A_Router_Between_Namespaces</a></p>
<pre tabindex="0"><code># 1.Setup
#!/bin/bash

if [ $USER != &#34;root&#34; ] ; then
    echo &#34;Restarting script with sudo...&#34;
    sudo $0 ${*}
    exit
fi

# delete previous incarnations if they exist
ip link del dev veth_vpp1
ip link del dev veth_vpp2
ip netns del vpp1
ip netns del vpp2

#create namespaces
ip netns add vpp1
ip netns add vpp2

# create and configure 1st veth pair
ip link add name veth_vpp1 type veth peer name vpp1
ip link set dev vpp1 up
ip link set dev veth_vpp1 up netns vpp1

ip netns exec vpp1 \
  bash -c &#34;
    ip link set dev lo up
    ip addr add 172.16.1.2/24 dev veth_vpp1
    ip route add 172.16.2.0/24 via 172.16.1.1
&#34;

# create and configure 2st veth pair
ip link add name veth_vpp2 type veth peer name vpp2
ip link set dev vpp2 up
ip link set dev veth_vpp2 up netns vpp2

ip netns exec vpp2 \
  bash -c &#34;
    ip link set dev lo up
    ip addr add 172.16.2.2/24 dev veth_vpp2
    ip route add 172.16.1.0/24 via 172.16.2.1
&#34;

# 2.Configure Interfaces
sudo vppctl create host-interface name vpp1
sudo vppctl create host-interface name vpp2
sudo vppctl set int state host-vpp1 up
sudo vppctl set int state host-vpp2 up
sudo vppctl set int ip address host-vpp1 172.16.1.1/24
sudo vppctl set int ip address host-vpp2 172.16.2.1/24

# 3.Test
$ sudo ip netns exec vpp1 ping 172.16.2.1 -c 1
PING 172.16.2.2 (172.16.2.2) 56(84) bytes of data.
64 bytes from 172.16.2.2: icmp_seq=1 ttl=63 time=0.135 ms

--- 172.16.2.2 ping statistics ---
1 packets transmitted, 1 received, 0% packet loss, time 0ms
rtt min/avg/max/mdev = 0.135/0.135/0.135/0.000 ms

vpp# show ip arp
    Time      FIB        IP4      Stat      Ethernet              Interface
   1050.5729   0     172.16.1.2         5a:df:31:28:dc:5c         host-vpp1
   1050.5768   0     172.16.2.2         12:fa:19:cb:39:e3         host-vpp2
vpp# show interface
vpp# show ip fib
</code></pre><ol start="2">
<li>host1 : 10.1.5.151    host2 : 10.1.5.152</li>
</ol>
<pre tabindex="0"><code> # 151
set interface state GigabitEthernet0/4/0  up
set interface ip address GigabitEthernet0/4/0  1.1.1.1/24

# 152
set interface state GigabitEthernet0/4/0  up
set interface ip address GigabitEthernet0/4/0  1.1.1.2/24

ping ok!
</code></pre><ol start="3">
<li>memif   <br>
<a href="https://s3-docs.fd.io/vpp/22.10/gettingstarted/progressivevpp/twovppinstances.html" target="_blank">https://s3-docs.fd.io/vpp/22.10/gettingstarted/progressivevpp/twovppinstances.html</a></li>
<li>routing   <br>
<a href="https://s3-docs.fd.io/vpp/22.10/gettingstarted/progressivevpp/routing.html" target="_blank">https://s3-docs.fd.io/vpp/22.10/gettingstarted/progressivevpp/routing.html</a></li>
<li>switching    <br>
<a href="https://s3-docs.fd.io/vpp/22.10/gettingstarted/progressivevpp/switching.html" target="_blank">https://s3-docs.fd.io/vpp/22.10/gettingstarted/progressivevpp/switching.html</a></li>
</ol>

            <footer class="footline">
            </footer>
          </article>

          <article class="default">
<h1 id="3-frrdocker-dockervpp">3. frrdocker-dockervpp</h1>

<p>date: 2023-02-05      <br>
VM: 10.1.5.153

<a href="#image-f7c6aab261a91e3a02a6c1d177b83411">
<img src="../img/vpp/1675562552504.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-f7c6aab261a91e3a02a6c1d177b83411">
<img src="../img/vpp/1675562552504.png" alt=""loading="lazy">
</a></p>
<pre tabindex="0"><code>docker pull frrdocker/dockervpp:vpp1
docker run --privileged --name VPP1 --cap-add=NET_ADMIN --cap-add=SYS_ADMIN --rm -it frrdocker/dockervpp:vpp1
vpp -c /etc/vpp/startup.conf &amp;
docker run --privileged --name VPP2 --cap-add=NET_ADMIN --cap-add=SYS_ADMIN --rm -it frrdocker/dockervpp:vpp1
vpp -c /etc/vpp/startup.conf &amp;
docker run --privileged --name VPP3 --cap-add=NET_ADMIN --cap-add=SYS_ADMIN --rm -it frrdocker/dockervpp:vpp1
vpp -c /etc/vpp/startup.conf &amp;
</code></pre><p>
<a href="#image-92fca0c769d5f3261792f38d82489346">
<img src="../img/vpp/1675562646130.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-92fca0c769d5f3261792f38d82489346">
<img src="../img/vpp/1675562646130.png" alt=""loading="lazy">
</a>

<a href="#image-637736b0de6e178b9c8f55634d8140fb">
<img src="../img/vpp/1675562682668.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-637736b0de6e178b9c8f55634d8140fb">
<img src="../img/vpp/1675562682668.png" alt=""loading="lazy">
</a></p>
<pre tabindex="0"><code>vi 04-docker-network-remove-default.sh 
#! bi/bash
VPP1=$(docker exec -it VPP1 bash -c hostname | tr -d &#39;\r\n&#39;)
VPP2=$(docker exec -it VPP2 bash -c hostname | tr -d &#39;\r\n&#39;)
VPP3=$(docker exec -it VPP3 bash -c hostname | tr -d &#39;\r\n&#39;)
echo $VPP1
echo $VPP2
echo $VPP3
### Disconnect VPP1 from default bridge docker0
docker network disconnect bridge $VPP1
### Disconnect VPP2 from default bridge docker0
docker network disconnect bridge $VPP2
### Disconnect VPP3 from default bridge docker0
docker network disconnect bridge $VPP3

bash -xv 04-docker-network-remove-default.sh 
</code></pre><p>
<a href="#image-f9f7668ee0c50d40cf7b40d417d251ed">
<img src="../img/vpp/1675562735715.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-f9f7668ee0c50d40cf7b40d417d251ed">
<img src="../img/vpp/1675562735715.png" alt=""loading="lazy">
</a></p>
<pre tabindex="0"><code>docker network create --driver=bridge --subnet=11.11.0.0/16 vpp_subnet1
docker network create --driver=bridge --subnet=12.12.0.0/16 vpp_subnet2
</code></pre><p>
<a href="#image-2299c22a4fb492c6564506fd96ff5f8b">
<img src="../img/vpp/1675562815815.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-2299c22a4fb492c6564506fd96ff5f8b">
<img src="../img/vpp/1675562815815.png" alt=""loading="lazy">
</a>

<a href="#image-21b546af994e5edbda3148490f18de9a">
<img src="../img/vpp/1675562865110.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-21b546af994e5edbda3148490f18de9a">
<img src="../img/vpp/1675562865110.png" alt=""loading="lazy">
</a></p>
<pre tabindex="0"><code>#! bi/bash
VPP1=$(docker exec -it VPP1 bash -c hostname | tr -d &#39;\r\n&#39;)
VPP2=$(docker exec -it VPP2 bash -c hostname | tr -d &#39;\r\n&#39;)
VPP3=$(docker exec -it VPP3 bash -c hostname | tr -d &#39;\r\n&#39;)
echo $VPP1
echo $VPP2
echo $VPP3

### Connect vpp_subnet1 to VPP1 and VPP2
docker network connect vpp_subnet1 $VPP1
docker network connect vpp_subnet1 $VPP2

### Connect vpp_subnet2 to VPP2 and VPP3
docker network connect vpp_subnet2 $VPP2
docker network connect vpp_subnet2 $VPP3
</code></pre><p>
<a href="#image-5a065c0f419389d0b3eac75a6d8f6ef7">
<img src="../img/vpp/1675562915349.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-5a065c0f419389d0b3eac75a6d8f6ef7">
<img src="../img/vpp/1675562915349.png" alt=""loading="lazy">
</a></p>
<pre tabindex="0"><code># VPP1
ifconfig eth1 0.0.0.0
vppctl
create host-interface name eth1
show int
set interface state host-eth1 up
set interface ip address host-eth1 11.11.0.11/16
show int addr
ip route add 12.12.0.0/16 via 11.11.0.12

# VPP2
ifconfig eth1 0.0.0.0
ifconfig eth2 0.0.0.0
vppctl
create host-interface name eth1
create host-interface name eth2
set interface state host-eth1 up
set interface state host-eth2 up
set interface ip address host-eth1 11.11.0.12/16
set interface ip address host-eth1 12.12.0.21/16

# VPP3
ifconfig eth1 0.0.0.0
vppctl
create host-interface name eth1
set interface state host-eth1 up
set interface ip address host-eth1 12.12.0.22/16
ip route add 11.11.0.0/16 via 12.12.0.21
</code></pre>
            <footer class="footline">
            </footer>
          </article>

          <article class="default">
<h1 id="4-vpp_tap_int_containers1">4. VPP_TAP_INT_Containers1</h1>

<p>date: 2023-02-05  <br>
<a href="https://github.com/MarioDoman/VPP_TAP_INT_WITH_Containers" target="_blank">https://github.com/MarioDoman/VPP_TAP_INT_WITH_Containers</a>              <br>

<a href="#image-bfc7330da1a2564e1fcc8f42976b12a7">
<img src="../img/vpp/1675564374195.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-bfc7330da1a2564e1fcc8f42976b12a7">
<img src="../img/vpp/1675564374195.png" alt=""loading="lazy">
</a></p>
<pre tabindex="0"><code>echo &#34;Remove old netns simlink&#34;
sudo rm -Rf /var/run/netns
sudo mkdir /var/run/netns

# veth pair for docker1
sudo ip link add veth10 type veth peer name veth11
sudo ip link set veth10 up
sudo ip link set veth11 up

# veth pair for docker2
sudo ip link add veth20 type veth peer name veth21
sudo ip link set veth20 up
sudo ip link set veth21 up

#Create docker containers
docker pull busybox
docker run -d --name &#34;docker1&#34; busybox sleep 36000
docker run -d --name &#34;docker2&#34; busybox sleep 36000

#Wait for containers
sleep 10

#Expose containers to the &#39;ip netns exec&#39; tools
pid1=`docker inspect -f &#39;{{.State.Pid}}&#39; docker1`
ln -s /proc/$pid1/ns/net /var/run/netns/docker1

pid2=`docker inspect -f &#39;{{.State.Pid}}&#39; docker2`
ln -s /proc/$pid2/ns/net /var/run/netns/docker2

ip netns ls

# Move the veth10 into docker1 network namespace respectivley.
ip link set veth10 netns docker1
ip netns exec docker1 ip addr add 192.168.1.2/24 dev veth10
ip netns exec docker1 ip link set veth10 up
ip netns exec docker1 ip route add 192.168.2.0/24 via 192.168.1.1
ip netns exec docker1 ip route add 192.168.3.0/24 via 192.168.1.1

ip netns exec docker1 ip a
ip netns exec docker1 ip r
ip netns exec docker1 route -n

# Move the veth20 into docker1 network namespace respectivley.
ip link set veth20 netns docker2
ip netns exec docker2 ip addr add 192.168.3.2/24 dev veth20
ip netns exec docker2 ip link set veth20 up
ip netns exec docker2 ip route add 192.168.2.0/24 via 192.168.3.1
ip netns exec docker2 ip route add 192.168.1.0/24 via 192.168.3.1

# connect veth on the host to vpp
vppctl create host-interface name veth11
vppctl set int ip address host-veth11 192.168.1.1/24
vppctl set int state host-veth11 up

# connect veth on the host to vpp
vppctl create host-interface name veth21
vppctl set int ip address host-veth21 192.168.3.1/24
vppctl set int state host-veth21 up

# Create vpp tap interface
vppctl create tap
vppctl set interface state tap0 up
vppctl set interface ip address tap0 192.168.2.1/24

#Assign tap interface address on kernel interface
sudo ip addr add 192.168.2.2/24 dev tap0
sudo ip link set tap0 up

# Add routing from host to containers via tap
ip route add 192.168.1.0/24 via 192.168.2.1
ip route add 192.168.3.0/24 via 192.168.2.1

# Start iperf server on hostvm
#iperf -sDB 192.168.2.2

#TEST!
echo &#34;Pinging container1 via host &gt; TAP-VPP &gt; Container1&#34;
ping -c3 192.168.1.2

echo &#34;Pinging container2 via host &gt; TAP-VPP &gt; Container2&#34;
ping -c3 192.168.3.2

#sudo docker exec docker1 apt update
#sudo docker exec docker1 apt -y install iputils-ping net-tools iperf
echo &#34;Ping from container1 via container &gt; VPP-TAP &gt; Host&#34;
docker exec docker1 ping -c3 192.168.2.2
#docker exec docker1 iperf -s -D
#sudo docker exec docker2 apt update
#sudo docker exec docker2 apt -y install iputils-ping net-tools iperf
#echo &#34;Ping from container2 via container &gt; VPP-TAP &gt; Host&#34;
docker exec docker2 ping -c3 192.168.2.2
#docker exec docker2 iperf -s -D
echo &#34;Ping from Container1 to Container2 via VPP&#34;
docker exec docker1 ping -c3 192.168.3.2
</code></pre><pre tabindex="0"><code># Create vpp tap interface
vppctl create tap
vppctl set interface state tap1 up
vppctl set interface ip address tap1 192.168.2.1/24

#Assign tap interface address on kernel interface
sudo ip addr add 192.168.2.2/24 dev tap0
sudo ip link set tap1 up
</code></pre>
            <footer class="footline">
            </footer>
          </article>

          <article class="default">
<h1 id="5-vpp_tap_int_containers2">5. VPP_TAP_INT_Containers2</h1>

<p>date: 2023-02-05  <br>

<a href="#image-17e4621c03caf6e3e7f8097e61f51753">
<img src="../img/vpp/1675564749292.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-17e4621c03caf6e3e7f8097e61f51753">
<img src="../img/vpp/1675564749292.png" alt=""loading="lazy">
</a></p>
<pre tabindex="0"><code>docker pull alpine:3.17.1

cat &lt;&lt; EOF &gt; Dockerfile 
FROM alpine:3.17.1
ENV TZ Asia/Shanghai
RUN sed -i &#39;s/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g&#39; /etc/apk/repositories
RUN apk add --update --no-cache bash tcpdump iperf busybox-extras iproute2 iputils tzdata
RUN cp /usr/share/zoneinfo/\${TZ} /etc/localtime \
    &amp;&amp; echo \${TZ} &gt; /etc/timezone &amp;&amp;  rm -rf /var/cache/apk/*
EOF

docker build -t amwork2010/alpine:iperf .
</code></pre><pre tabindex="0"><code># 153
vppctl

set interface state GigabitEthernet0/4/0  up
set interface ip address GigabitEthernet0/4/0  1.1.1.1/24
create tap
set interface state tap0 up
set interface ip address tap0 2.2.2.1/24
ip route add 3.3.3.0/24 via 1.1.1.2

#Assign tap interface address on kernel interface
#sudo ip addr add 2.2.2.2/24 dev tap0
#sudo ip link set tap0 up
#ip route add 3.3.3.0/24 via 2.2.2.1
#ping 3.3.3.1 --&gt; OK

ip addr flush dev tap0
#Create docker containers
docker pull amwork2010/alpine:iperf
docker run -d --name &#34;docker&#34; amwork2010/alpine:iperf sleep 36000
#Wait for containers
sleep 10
pid1=`docker inspect -f &#39;{{.State.Pid}}&#39; docker`
ln -s /proc/$pid1/ns/net /var/run/netns/docker
docker network disconnect bridge $(docker exec -it docker bash -c hostname | tr -d &#39;\r\n&#39;)

ip link set tap0 netns docker
ip netns exec docker ip addr add 2.2.2.2/24 dev tap0
ip netns exec docker ip link set tap0 up
ip netns exec docker ip route add 3.3.3.0/24 via 2.2.2.1


# 154
vppctl

set interface state GigabitEthernet0/4/0  up
set interface ip address GigabitEthernet0/4/0  1.1.1.2/24
create tap
set interface state tap0 up
set interface ip address tap0 3.3.3.1/24
ip route add 2.2.2.0/24 via 1.1.1.1
# ip route del 2.2.2.0/24 via 1.1.1.2

#Create docker containers
docker pull amwork2010/alpine:iperf
docker run -d --name &#34;docker&#34; amwork2010/alpine:iperf sleep 36000
#Wait for containers
sleep 10
pid2=`docker inspect -f &#39;{{.State.Pid}}&#39; docker`
ln -s /proc/$pid2/ns/net /var/run/netns/docker
docker network disconnect bridge $(docker exec -it docker bash -c hostname | tr -d &#39;\r\n&#39;)

ip link set tap0 netns docker
ip netns exec docker ip addr add 3.3.3.2/24 dev tap0
ip netns exec docker ip link set tap0 up
ip netns exec docker ip route add 2.2.2.0/24 via 3.3.3.1

ip netns exec docker ping 2.2.2.2
# ping OK
</code></pre>
            <footer class="footline">
            </footer>
          </article>

          <article class="default">
<h1 id="6-vpp_kernel_interaction">6. VPP_kernel_interaction</h1>

<p>date: 2023-02-05  <br>
VPP和Linux内核协议栈通信方法   <br>
<a href="https://blog.csdn.net/turbock/article/details/103912015" target="_blank">https://blog.csdn.net/turbock/article/details/103912015</a>  <br>

<a href="#image-c55f603c7f24af47ac1ff354b0475a59">
<img src="../img/vpp/1675565161003.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-c55f603c7f24af47ac1ff354b0475a59">
<img src="../img/vpp/1675565161003.png" alt=""loading="lazy">
</a></p>
<p>10.1.5.153</p>
<pre tabindex="0"><code>2: ens3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 52:54:00:f8:00:83 brd ff:ff:ff:ff:ff:ff
    altname enp0s3
    inet 10.1.5.153/21 brd 10.1.7.255 scope global ens3
       valid_lft forever preferred_lft forever
    inet6 fe80::5054:ff:fef8:83/64 scope link 
       valid_lft forever preferred_lft forever
3: ens4: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000
    link/ether 52:54:00:0d:bb:f4 brd ff:ff:ff:ff:ff:ff
    altname enp0s4
4: ens5: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000
    link/ether 52:54:00:94:18:27 brd ff:ff:ff:ff:ff:ff
    altname enp0s5
</code></pre><pre tabindex="0"><code>ip l s ens3 down
systemctl start vpp
# vpp纳管ens4 
ip a
2: ens3: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc mq state DOWN group default qlen 1000
    link/ether 52:54:00:f8:00:83 brd ff:ff:ff:ff:ff:ff
    altname enp0s3
4: ens5: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000
    link/ether 52:54:00:94:18:27 brd ff:ff:ff:ff:ff:ff
    altname enp0s5
</code></pre><pre tabindex="0"><code>vppctl
#创建tap0网卡
create tap ?
create tap host-ip4-addr 10.1.5.153/21 host-ip4-gw 10.1.1.1 host-mac-addr 52:54:00:0d:bb:f4 host-if-name ens4

#将网络接口tap0和g0绑定到网桥1上,并启动。可以不设ip
set int l2 bridge tap0 1
set int l2 bridge GigabitEthernet0/4/0 1
set int state GigabitEthernet0/4/0 up
set int state tap0 up
</code></pre><pre tabindex="0"><code>ip a a 10.1.5.153/21 dev ens4

# FROM other VM
ping OK
ssh 10.1.1.5.153 --&gt; OK
</code></pre><pre tabindex="0"><code>ip netns add ns0
ip link add vpp0 type veth peer name vethns0
ip link set vethns0 netns ns0
ip netns exec ns0 ip link set lo up
ip netns exec ns0 ip link set vethns0 up
ip netns exec ns0 ip addr add 192.168.1.1/24 dev vethns0
ip netns exec ns0 ethtool -K vethns0 rx off tx off
</code></pre><pre tabindex="0"><code>vpp# set int l2 bridge GigabitEthernet4/0/0 1
vpp# set int state GigabitEthernet4/0/0 up
vpp# create host-interface name vpp0
vpp# set interface state host-vpp0 up
vpp# set interface l2 bridge host-vpp0 1
</code></pre>
            <footer class="footline">
            </footer>
          </article>

          <article class="default">
<h1 id="7-vpp-qemu-vhost">7. vpp qemu vhost</h1>

<p>date: 2023-02-05</p>
<p>HOST : 10.1.1.8    &ndash;&gt; VM: 10.1.5.155      <br>
<a href="https://s3-docs.fd.io/vpp/22.10/usecases/vhost/index.html" target="_blank">https://s3-docs.fd.io/vpp/22.10/usecases/vhost/index.html</a>   <br>
<a href="https://wiki.fd.io/view/VPP/Use_VPP_to_connect_VMs_Using_Vhost-User_Interface" target="_blank">https://wiki.fd.io/view/VPP/Use_VPP_to_connect_VMs_Using_Vhost-User_Interface</a></p>
<pre tabindex="0"><code># vm.xml
		&lt;interface type=&#39;bridge&#39;&gt;
			&lt;source bridge=&#39;br0&#39;/&gt;
			&lt;model type=&#39;vmxnet3&#39;/&gt;
		&lt;/interface&gt;
		&lt;interface type=&#39;bridge&#39;&gt;
			&lt;source bridge=&#39;br0&#39;/&gt;
			&lt;model type=&#39;vmxnet3&#39;/&gt;
		&lt;/interface&gt;

## HOST : 10.1.1.8
../startVMs/startvm.sh vpp_5.155 jammy-server-cloudimg-amd64.20221210.pw1-10.1.5.3.img 4 8

######### 0.prepare
apt update
apt -y full-upgrade
ln -sf ../usr/share/zoneinfo/Asia/Shanghai /etc/localtime
[ -f /var/run/reboot-required ] &amp;&amp; reboot -f

######### 1. 启用rc.local
cat &lt;&lt; EOF &gt;&gt; /etc/rc.local
#!/bin/bash
echo 1 &gt; /sys/module/vfio/parameters/enable_unsafe_noiommu_mode
EOF

chmod +x /etc/rc.local

cat &lt;&lt; EOF &gt;&gt; /lib/systemd/system/rc-local.service

[Install]
WantedBy=multi-user.target
EOF

cat /lib/systemd/system/rc-local.service

# 启用服务
systemctl enable rc-local
systemctl start rc-local
systemctl status rc-local

cat /sys/module/vfio/parameters/enable_unsafe_noiommu_mode

######### 2. vfio &amp;&amp; hugepages (可不做，具体见下80-vpp.conf vpp.service)
echo &#34;vfio-pci&#34; &gt; /etc/modules-load.d/95-vpp.conf

cat &lt;&lt;EOF &gt;&gt; /etc/sysctl.conf 
vm.nr_hugepages = 2048
EOF

sysctl -p

######### 3. vpp 22.10 &amp;&amp; dpdk
lshw -businfo -c network
#apt install dpdk dpdk-dev -y
apt install dpdk -y

## https://packagecloud.io/fdio
#curl -s https://packagecloud.io/install/repositories/fdio/release/script.deb.sh | sudo bash
curl -s https://packagecloud.io/install/repositories/fdio/2210/script.deb.sh | sudo bash
cat /etc/apt/sources.list.d/fdio_2210.list
apt update
apt install vpp vpp-plugin-core vpp-plugin-dpdk -y
# systemctl enable vpp
# systemctl disable vpp
systemctl status vpp
mkdir -p /var/log/vpp
vi /etc/sysctl.d/80-vpp.conf 
vm.nr_hugepages=2048

vi /lib/systemd/system/vpp.service
ExecStartPre=-/sbin/modprobe vfio-pci
ExecStartPre=-/bin/bash -c &#39;echo 1 &gt; /sys/module/vfio/parameters/enable_unsafe_noiommu_mode &amp;&amp; sleep 2&#39;

vi /etc/vpp/startup.conf
dpdk {
	uio-driver vfio-pci
	dev 0000:00:04.0
}

systemctl daemon-reload &amp;&amp; systemctl restart vpp
## CPU0 100%
vppctl show ver

vppctl
show interface
</code></pre><pre tabindex="0"><code>apt update
egrep -c &#39;(vmx|svm)&#39; /proc/cpuinfo
grep -E --color &#39;(vmx|svm)&#39; /proc/cpuinfo
apt install -y cpu-checker
kvm-ok
apt install -y qemu-kvm virt-manager libvirt-daemon-system virtinst libvirt-clients qemu-utils bridge-utils
systemctl status libvirtd

cat &gt;&gt; /etc/libvirt/qemu.conf &lt;&lt; EOF
user = &#34;root&#34;
group = &#34;root&#34;
EOF
systemctl restart libvirtd.service

vppctl #
create vhost-user socket /tmp/vm00.sock
show vhost-user
show int
set interface state VirtualEthernet0/0/0 up
set interface state GigabitEthernet0/4/0 up
set interface l2 bridge VirtualEthernet0/0/0 100
set interface l2 bridge GigabitEthernet0/4/0 100
show bridge
# clean
# delete vhost-user VirtualEthernet0/0/0

mv alpine-virt-3.16.1-iperf3-x86_64.qcow2 disk
virsh define vm.xml
virsh start alpine1
virsh console alpine1
ip link set eth0 up
ip addr add 10.1.5.188/21 dev eth0
ip a
ping 10.1.1.1 ### OK
</code></pre><p><strong>vm.xml</strong></p>
<pre tabindex="0"><code>&lt;domain type=&#39;kvm&#39;&gt;
	&lt;name&gt;alpine1&lt;/name&gt;
	&lt;vcpu placement=&#39;static&#39;&gt;2&lt;/vcpu&gt;
	&lt;memory&gt;1048576&lt;/memory&gt;
	&lt;memoryBacking&gt;
		&lt;hugepages&gt;
			&lt;page size=&#39;2048&#39; unit=&#39;KiB&#39;/&gt;
		&lt;/hugepages&gt;
	&lt;/memoryBacking&gt;
	&lt;os&gt;
		&lt;type arch=&#39;x86_64&#39; machine=&#39;pc&#39;&gt;hvm&lt;/type&gt;
		&lt;bootmenu enable=&#39;yes&#39;/&gt;
	&lt;/os&gt;
	&lt;features&gt;
		&lt;acpi/&gt;
		&lt;apic/&gt;
		&lt;pae/&gt;
	&lt;/features&gt;
	&lt;cpu mode=&#34;host-passthrough&#34;&gt;
		&lt;numa&gt;
			&lt;cell id=&#39;0&#39; cpus=&#39;0-1&#39; memory=&#39;1048576&#39; unit=&#39;KiB&#39; memAccess=&#39;shared&#39;/&gt;
		&lt;/numa&gt;
	&lt;/cpu&gt;
	&lt;clock offset=&#39;utc&#39;/&gt;
	&lt;on_poweroff&gt;destroy&lt;/on_poweroff&gt;
	&lt;on_reboot&gt;restart&lt;/on_reboot&gt;
	&lt;on_crash&gt;destroy&lt;/on_crash&gt;
	&lt;devices&gt;
		&lt;emulator&gt;/usr/bin/kvm&lt;/emulator&gt;
		&lt;disk type=&#39;file&#39; device=&#39;disk&#39;&gt;
			&lt;driver name=&#39;qemu&#39; type=&#39;qcow2&#39;/&gt;
			&lt;source file=&#39;/root/vms/alpine1/disk&#39;/&gt;
			&lt;target dev=&#39;vda&#39; bus=&#39;virtio&#39;/&gt;
			&lt;boot order=&#39;1&#39;/&gt;
		&lt;/disk&gt;
		&lt;interface type=&#39;vhostuser&#39;&gt;
			&lt;mac address=&#39;00:00:00:00:00:01&#39;/&gt;
			&lt;source type=&#39;unix&#39; path=&#39;/tmp/vm00.sock&#39; mode=&#39;server&#39;/&gt;
			&lt;target dev=&#39;vnet1&#39;/&gt;
			&lt;model type=&#39;virtio&#39;/&gt;
			&lt;driver queues=&#39;2&#39;&gt;
				 &lt;host mrg_rxbuf=&#39;off&#39;/&gt;
			&lt;/driver&gt;
		&lt;/interface&gt;
		&lt;serial type=&#39;pty&#39;&gt;
		  &lt;target port=&#39;0&#39;/&gt;
		&lt;/serial&gt;
		&lt;console type=&#39;pty&#39;&gt;
		  &lt;target type=&#39;serial&#39; port=&#39;0&#39;/&gt;
		&lt;/console&gt;
		&lt;graphics type=&#39;vnc&#39; port=&#39;-1&#39; autoport=&#39;yes&#39; listen=&#39;0.0.0.0&#39;/&gt;
		&lt;video&gt;
			&lt;model type=&#39;cirrus&#39; vram=&#39;65536&#39; heads=&#39;1&#39;/&gt;
		&lt;/video&gt;
		&lt;input type=&#39;tablet&#39; bus=&#39;usb&#39;/&gt;
		&lt;input type=&#39;mouse&#39; bus=&#39;ps2&#39;/&gt;
	&lt;/devices&gt;
&lt;/domain&gt;
</code></pre>
            <footer class="footline">
            </footer>
          </article>

          <article class="default">
<h1 id="8-vpp-commands">8. vpp commands</h1>

<p>date: 2023-02-05</p>
<p>vpp常用命令与基本操作</p>
<pre tabindex="0"><code>vpp的基本操作：
1，up/down
vpp# set int state G0 up
vpp# set int state G0 down

2，配置IP
vpp#set int ip address G0 192.168.59.134/24

3，配置mac
vpp#set int mac address G0 00:00:00:00:00:00

4,查看基本信息
vpp# show int 
vpp# show int addr

5，配置路由
vpp# ip route add 0.0.0.0/0 via 0.0.0.0
vpp# ip table [add|del] &lt;table-id&gt;   //创建IPv4 table表
vpp# ip6 table [add|del] &lt;table-id&gt;  //创建IPv6 table表
vpp# show ip fib  //查看路由
vpp# show ip arp  //查看arp 表

6，创建桥
vpp# create bridge-domain 100  //桥100
vpp# show bridge-domain 100 detail  //查看桥下信息

7，创建vlan
vpp# create sub-interfaces G1 100    //G1.100
vpp# delete sub-interface G1.100  //del

8，将vlan放桥下
vpp#set int l2 bridge G1.100 100  

9，删除桥,再删除桥
vpp#set int l3 G0
vpp#create bridge-domain 100 del

10，配置路由
vpp#ip route add 0.0.0.0/0 via 端口IP
//删除路由
vpp# ip route del 0.0.0.0/0 via 端口IP
//查看路由
vpp#show ip fib

11，配置snat
vpp# nat44 add interface address G0   //将G0 作为地址池
vpp# set interface nat44 in G1 out G2 //  设置出入口
vpp# show nat44 address  //查看地址池
vpp# show nat44 interfaces  //查看接口
vpp# nat44 forwarding enable|disable //nat转发的启动/禁止

12，配置dnat
vpp# set interface nat44 in &lt;intfc&gt; out &lt;intfc&gt; [output-feature] [del]
vpp# nat44 add address &lt;ip4-range-start&gt; [- &lt;ip4-range-end&gt;] [tenant-vrf &lt;vrf-id&gt;]
vpp# nat44 add static mapping tcp|udp|icmp local &lt;ip4-addr&gt; [&lt;port&gt;] external (&lt;ip4-addr&gt;|&lt;intfc&gt;) [&lt;port&gt;] [vrf &lt;table-id&gt;] [twice-nat] [out2in-only] [del]

13，配置wireguard
vpp# wireguard create listen-port 8899 private-key 私钥内容 src 公网IP  //创建加配置
vpp# show wireguard interface
vpp# show wireguard peer
//添加peer
vpp# wireguard peer add &lt;wg_int&gt; public-key &lt;pub_key_other&gt;endpoint &lt;ip4_dst&gt; allowed-ip &lt;prefix&gt;dst-port [port_dst] persistent-keepalive [keepalive_interval]
vpp# wireguard peer remove &lt;index&gt;   //删除peer

14，配置loopback
vpp# create loopback interface mac 00:0c:29:1f:ce:07 instance 100
vpp# delete loopback interface intfc loop100

15，配置gre
vpp# create gre tunnel src 1.1.1.2/24 dst 2.2.2.3/24 instance 100 outer-fib-id 0  //创建gre100
vpp# create gre tunnel src 1.1.1.2/24 dst 2.2.2.3/24 instance 100 outer-fib-id 0 del //删除gre100
vpp# show gre tunnel 

16，配置memif
vpp# create memif id 0 /run/vpp/contiv/memif1.sock
vpp# create interface memif id 0 socket-id 1 master mode ip secret vpp123
vpp# set int state memif0 up
vpp# set int ip address memif0 192.168.1.1/24
vpp# ping 192.168.1.2     //另一个vpp 的memif 设为192.168.1.2/24
vpp# delete interface memif memif1/0   //删除操作
vpp# delete memif socket id 0   //删除操作

17，查看版本
vpp# show version

18，查看启动插件
vpp# show plugin

19, 查看vpp线程
vpp# show threads

vpp+ show run

20, 创建tap
# create tap id 10 host-if-name host   //创建tap10  linux主机上的接口为host
# set int state tap10 up   //up
# set int ip address tap10 192.168.100.100/24  //设IP
</code></pre>
            <footer class="footline">
            </footer>
          </article>

          <article class="default">
<h1 id="9-calico-vpp-vmware">9. calico vpp vmware</h1>

<p>date: 2023-02-05</p>
<p><a href="https://projectcalico.docs.tigera.io/getting-started/kubernetes/vpp/getting-started" target="_blank">https://projectcalico.docs.tigera.io/getting-started/kubernetes/vpp/getting-started</a>        <br>
VMware VM u2204 3台，双网卡: vmxnet3</p>
<pre tabindex="0"><code>hostnamectl set-hostname vpp61
cat &lt;&lt; EOF &gt;&gt; /etc/hosts
192.168.68.61 vpp61
192.168.68.62 vpp62
192.168.68.63 vpp63
EOF

apt update
apt -y full-upgrade
[ -f /var/run/reboot-required ] &amp;&amp; reboot -f
ln -sf ../usr/share/zoneinfo/Asia/Shanghai /etc/localtime

vi /etc/default/grub 
GRUB_CMDLINE_LINUX=&#34;iommu=pt intel_iommu=on&#34;
#GRUB_CMDLINE_LINUX=&#34;default_hugepagesz=1G hugepagesz=1G hugepages=4 iommu=pt intel_iommu=on&#34;

update-grub
echo &#34;vfio-pci&#34; &gt; /etc/modules-load.d/95-vpp.conf
echo &#34;vm.nr_hugepages = 1024&#34; &gt;&gt; /etc/sysctl.conf
reboot

cat /proc/cmdline | grep -e iommu=pt -e intel_iommu=on -e huge
dmesg | grep -e DMAR -e IOMMU
cat /proc/meminfo | grep Huge
lscpu | grep NUMA
lshw -businfo -c network
    pci@0000:0b:00.0              network    VMXNET3 Ethernet Controller
    pci@0000:13:00.0              network    VMXNET3 Ethernet Controller

apt install driverctl -y
driverctl list-devices
...    0000:0b:00.0 vmxnet3
...    0000:13:00.0 vmxnet3

swapoff -a
sed -i &#39;/swap/ s/^\(.*\)$/#\1/g&#39; /etc/fstab
free -h

tee /etc/modules-load.d/containerd.conf &lt;&lt;EOF
overlay
br_netfilter
EOF

tee /etc/modules-load.d/ipvs.conf &lt;&lt;EOF
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
br_netfilter
bridge
nf_conntrack
EOF

cat &lt;&lt;EOF &gt;&gt; /etc/sysctl.conf 
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
vm.swappiness=0
EOF
sysctl -p

export https_proxy=http://10.1.1.12:8118
wget https://github.com/containerd/containerd/releases/download/v1.6.14/cri-containerd-cni-1.6.14-linux-amd64.tar.gz
tar zxvf cri-containerd-cni-1.6.14-linux-amd64.tar.gz  -C /
mv /etc/cni/net.d/10-containerd-net.conflist /etc/cni/net.d/10-containerd-net.conflist.bak
mkdir -p /etc/containerd
containerd config default &gt; /etc/containerd/config.toml
sed -i &#39;s#registry.k8s.io#registry.aliyuncs.com/google_containers#g&#39; /etc/containerd/config.toml
# pause:3.6 ---&gt;&gt;&gt; pause:3.8
sed -i &#39;s/pause:3.6/pause:3.8/g&#39; /etc/containerd/config.toml
sed -i &#39;s/SystemdCgroup = false/SystemdCgroup = true/g&#39; /etc/containerd/config.toml
systemctl daemon-reload
systemctl enable  containerd.service
systemctl restart containerd.service
systemctl status containerd.service

apt install -y apt-transport-https socat
curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - 
cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF
apt update
apt install -y kubeadm=1.25.5-00 kubelet=1.25.5-00 kubectl=1.25.5-00

systemctl enable kubelet.service --now

kubeadm config print init-defaults --component-configs KubeletConfiguration
# cgroupDriver: systemd # 
# kubeadm config images pull
kubeadm config images pull --kubernetes-version=v1.25.5 --image-repository registry.aliyuncs.com/google_containers

kubeadm init --kubernetes-version=v1.25.5 \
 --image-repository registry.aliyuncs.com/google_containers \
 --pod-network-cidr=10.244.0.0/16 \
 --apiserver-advertise-address=192.168.68.61

cat &lt;&lt;EOF &gt;&gt; ~/.bashrc
alias kp=&#39;kubectl get pod -o wide --all-namespaces&#39;
alias wkp=&#39;watch -n 1 kubectl get pod -o wide --all-namespaces&#39;
alias ks=&#39;kubectl get svc -o wide --all-namespaces&#39;
alias kn=&#39;kubectl get node -o wide --all-namespaces&#39;
alias k=&#39;kubectl&#39;
EOF

#kubectl taint nodes --all node-role.kubernetes.io/control-plane-
#kubectl taint nodes --all node-role.kubernetes.io/master-

wget https://raw.githubusercontent.com/projectcalico/calico/v3.24.5/manifests/tigera-operator.yaml

cat &lt;&lt; EOF &gt; calicovpp.img.pull.sh
time crictl pull docker.io/calico/apiserver:v3.24.5
time crictl pull docker.io/calico/cni:v3.24.5
time crictl pull docker.io/calico/kube-controllers:v3.24.5
time crictl pull docker.io/calico/node:v3.24.5
time crictl pull docker.io/calico/pod2daemon-flexvol:v3.24.5
time crictl pull docker.io/calico/typha:v3.24.5
time crictl pull docker.io/calicovpp/agent:v3.24.0
time crictl pull docker.io/calicovpp/vpp:v3.24.0
time crictl pull quay.io/tigera/operator:v1.28.5
EOF

bash -xv calicovpp.img.pull.sh

kubectl create -f tigera-operator.yaml

wget https://raw.githubusercontent.com/projectcalico/vpp-dataplane/v3.24.0/yaml/calico/installation-default.yaml

vi installation-default.yaml
apiVersion: operator.tigera.io/v1
kind: Installation
metadata:
  name: default
spec:
  # Configures Calico networking.
  calicoNetwork:
    linuxDataplane: VPP
    ipPools:
    - blockSize: 24
      cidr: 10.244.0.0/16
      encapsulation: None                ##### NO IPIP &amp;&amp; NO VXLAN
---

kubectl create -f installation-default.yaml

# If you have configured hugepages on your machines
wget https://raw.githubusercontent.com/projectcalico/vpp-dataplane/v3.24.0/yaml/generated/calico-vpp.yaml

vi calico-vpp.yaml
...
    socksvr {
        socket-name /var/run/vpp/vpp-api.sock
    }
    dpdk {
        dev 0000:13:00.0 { num-rx-queues 1  num-tx-queues 1 }
    }
    plugins {
        plugin default { enable }
        plugin dpdk_plugin.so { enable }
        plugin calico_plugin.so { enable }
        plugin ping_plugin.so { enable }
        plugin dispatch_trace_plugin.so { enable }
    }
    buffers {
      buffers-per-numa 131072
    }
  vpp_dataplane_interface: ens192
  vpp_uplink_driver: &#34;none&#34;
...

kubectl create -f calico-vpp.yaml

curl https://raw.githubusercontent.com/projectcalico/vpp-dataplane/v3.24.0/test/scripts/vppdev.sh | tee /usr/local/bin/calivppctl
chmod +x /usr/local/bin/calivppctl
</code></pre><p>
<a href="#image-aaf3e97588a65dd866d8bade736621a2">
<img src="../img/vpp/1675566035137.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-aaf3e97588a65dd866d8bade736621a2">
<img src="../img/vpp/1675566035137.png" alt=""loading="lazy">
</a>

<a href="#image-8638029d6b2ada3b2fd86c3b832dda44">
<img src="../img/vpp/1675566068693.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-8638029d6b2ada3b2fd86c3b832dda44">
<img src="../img/vpp/1675566068693.png" alt=""loading="lazy">
</a></p>
<pre tabindex="0"><code>driverctl list-devices
    0000:0b:00.0 vmxnet3
    0000:13:00.0 vfio-pci   #### vfio-pci  dpdk 纳管
</code></pre><p>calivppctl vppctl [NODENAME]   <br>
calivppctl vppctl vpp62    <br>

<a href="#image-889eb4a0d46f8c2c3969b9c1436e8b93">
<img src="../img/vpp/1675566152548.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-889eb4a0d46f8c2c3969b9c1436e8b93">
<img src="../img/vpp/1675566152548.png" alt=""loading="lazy">
</a>

<a href="#image-92233f82f4ce9b94ced8b645e84c4eb8">
<img src="../img/vpp/1675566185798.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-92233f82f4ce9b94ced8b645e84c4eb8">
<img src="../img/vpp/1675566185798.png" alt=""loading="lazy">
</a>

<a href="#image-06339619d4c06dcb5bf9be74a1ba4cfe">
<img src="../img/vpp/1675566298863.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-06339619d4c06dcb5bf9be74a1ba4cfe">
<img src="../img/vpp/1675566298863.png" alt=""loading="lazy">
</a>

<a href="#image-100fdf6aad599c4dad1e214990c360fe">
<img src="../img/vpp/1675566352430.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-100fdf6aad599c4dad1e214990c360fe">
<img src="../img/vpp/1675566352430.png" alt=""loading="lazy">
</a></p>
<p><a href="http://www.tnblog.net/hb/article/details/7885" target="_blank">http://www.tnblog.net/hb/article/details/7885</a>         <br>
<a href="https://tnblog.net/hb/article/details/7233" target="_blank">https://tnblog.net/hb/article/details/7233</a>              <br>
<a href="https://slideplayer.com/slide/17437344/" target="_blank">https://slideplayer.com/slide/17437344/</a></p>

            <footer class="footline">
            </footer>
          </article>

          <article class="default">
<h1 id="10-calico-vpp-qemu">10. calico vpp qemu</h1>

<p>date: 2023-02-05<br>
<strong>vm.xml</strong></p>
<pre tabindex="0"><code>...
		&lt;interface type=&#39;bridge&#39;&gt;
			&lt;source bridge=&#39;br0&#39;/&gt;
			&lt;model type=&#39;vmxnet3&#39;/&gt;
		&lt;/interface&gt;
		&lt;interface type=&#39;bridge&#39;&gt;
			&lt;source bridge=&#39;br0&#39;/&gt;
			&lt;model type=&#39;vmxnet3&#39;/&gt;
		&lt;/interface&gt;
...
</code></pre><pre tabindex="0"><code>## HOST : 10.1.1.12
/opt/startVMs/startvm2.sh calicovpp_5.117 jammy-server-cloudimg-amd64.20221210.pw1-10.1.5.3.img 4 8
/opt/startVMs/startvm2.sh calicovpp_5.118 jammy-server-cloudimg-amd64.20221210.pw1-10.1.5.3.img 4 8
/opt/startVMs/startvm2.sh calicovpp_5.119 jammy-server-cloudimg-amd64.20221210.pw1-10.1.5.3.img 4 8

root@junnan-gpu:~# kvm --version
QEMU emulator version 6.2.0 (Debian 1:6.2+dfsg-2ubuntu6.6)
Copyright (c) 2003-2021 Fabrice Bellard and the QEMU Project developers

kroot@junnan-gpu:~# kvm -device ?
...
Network devices:
name &#34;e1000&#34;, bus PCI, alias &#34;e1000-82540em&#34;, desc &#34;Intel Gigabit Ethernet&#34;
name &#34;e1000-82544gc&#34;, bus PCI, desc &#34;Intel Gigabit Ethernet&#34;
name &#34;e1000-82545em&#34;, bus PCI, desc &#34;Intel Gigabit Ethernet&#34;
...
name &#34;virtio-net-pci&#34;, bus PCI, alias &#34;virtio-net&#34;
name &#34;virtio-net-pci-non-transitional&#34;, bus PCI
name &#34;virtio-net-pci-transitional&#34;, bus PCI
name &#34;vmxnet3&#34;, bus PCI, desc &#34;VMWare Paravirtualized Ethernet v3&#34;
...

开始用e1000，不用DPDK，成功；采用DPDK，网络断掉，不成功。
采用vmxnet3，DPDK 成功！

## VM : 10.1.5.117 , 10.1.5.118 , 10.1.5.119

######### 启用rc.local
cat &lt;&lt; EOF &gt;&gt; /etc/rc.local
#!/bin/bash
echo 1 &gt; /sys/module/vfio/parameters/enable_unsafe_noiommu_mode
EOF

chmod +x /etc/rc.local

cat &lt;&lt; EOF &gt;&gt; /lib/systemd/system/rc-local.service

[Install]
WantedBy=multi-user.target
EOF

cat /lib/systemd/system/rc-local.service

# 启用服务
systemctl enable rc-local
systemctl start rc-local
systemctl status rc-local

cat /sys/module/vfio/parameters/enable_unsafe_noiommu_mode

hostnamectl set-hostname calvpp1
cat &lt;&lt; EOF &gt;&gt; /etc/hosts
10.1.5.111 calvpp1
10.1.5.112 calvpp2
10.1.5.113 calvpp3
EOF

ln -sf ../usr/share/zoneinfo/Asia/Shanghai /etc/localtime
apt update &amp;&amp; \
apt -y full-upgrade &amp;&amp; \
[ -f /var/run/reboot-required ] &amp;&amp; reboot -f

echo &#34;vfio-pci&#34; &gt; /etc/modules-load.d/95-vpp.conf

swapoff -a
sed -i &#39;/swap/ s/^\(.*\)$/#\1/g&#39; /etc/fstab
free -h

tee /etc/modules-load.d/containerd.conf &lt;&lt;EOF
overlay
br_netfilter
EOF

tee /etc/modules-load.d/ipvs.conf &lt;&lt;EOF
ip_vs
ip_vs_rr
ip_vs_wrr
ip_vs_sh
br_netfilter
bridge
nf_conntrack
EOF

cat &lt;&lt;EOF &gt;&gt; /etc/sysctl.conf 
net.ipv4.ip_forward = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
vm.swappiness=0
vm.nr_hugepages = 1024
EOF
sysctl -p

cat /proc/meminfo | grep Huge
apt install driverctl -y
lshw -businfo -c network
driverctl list-devices

export https_proxy=http://10.1.1.12:8118
wget https://github.com/containerd/containerd/releases/download/v1.6.14/cri-containerd-cni-1.6.14-linux-amd64.tar.gz
tar zxvf cri-containerd-cni-1.6.14-linux-amd64.tar.gz  -C /
mv /etc/cni/net.d/10-containerd-net.conflist /etc/cni/net.d/10-containerd-net.conflist.bak
mkdir -p /etc/containerd
containerd config default &gt; /etc/containerd/config.toml
sed -i &#39;s#registry.k8s.io#registry.aliyuncs.com/google_containers#g&#39; /etc/containerd/config.toml
# pause:3.6 ---&gt;&gt;&gt; pause:3.8
sed -i &#39;s/pause:3.6/pause:3.8/g&#39; /etc/containerd/config.toml
sed -i &#39;s/SystemdCgroup = false/SystemdCgroup = true/g&#39; /etc/containerd/config.toml
systemctl daemon-reload
systemctl enable  containerd.service
systemctl restart containerd.service
systemctl status containerd.service

apt install -y apt-transport-https socat
curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - 
cat &lt;&lt;EOF &gt;/etc/apt/sources.list.d/kubernetes.list
deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main
EOF
apt update
apt install -y kubeadm=1.25.5-00 kubelet=1.25.5-00 kubectl=1.25.5-00

systemctl enable kubelet.service --now

kubeadm config print init-defaults --component-configs KubeletConfiguration
# cgroupDriver: systemd # 
# kubeadm config images pull
kubeadm config images pull --kubernetes-version=v1.25.5 --image-repository registry.aliyuncs.com/google_containers

kubeadm init --kubernetes-version=v1.25.5 \
 --image-repository registry.aliyuncs.com/google_containers \
 --pod-network-cidr=10.244.0.0/16 \
 --apiserver-advertise-address=10.1.5.111

cat &lt;&lt;EOF &gt;&gt; ~/.bashrc
alias kp=&#39;kubectl get pod -o wide --all-namespaces&#39;
alias wkp=&#39;watch -n 1 kubectl get pod -o wide --all-namespaces&#39;
alias ks=&#39;kubectl get svc -o wide --all-namespaces&#39;
alias kn=&#39;kubectl get node -o wide --all-namespaces&#39;
alias k=&#39;kubectl&#39;
EOF

#kubectl taint nodes --all node-role.kubernetes.io/control-plane-
#kubectl taint nodes --all node-role.kubernetes.io/master-

wget https://raw.githubusercontent.com/projectcalico/calico/v3.24.5/manifests/tigera-operator.yaml

cat &lt;&lt; EOF &gt; calicovpp.img.pull.sh
time crictl pull docker.io/calico/apiserver:v3.24.5
time crictl pull docker.io/calico/cni:v3.24.5
time crictl pull docker.io/calico/kube-controllers:v3.24.5
time crictl pull docker.io/calico/node:v3.24.5
time crictl pull docker.io/calico/pod2daemon-flexvol:v3.24.5
time crictl pull docker.io/calico/typha:v3.24.5
time crictl pull docker.io/calicovpp/agent:v3.24.0
time crictl pull docker.io/calicovpp/vpp:v3.24.0
time crictl pull quay.io/tigera/operator:v1.28.5
EOF

bash -xv calicovpp.img.pull.sh

wget https://raw.githubusercontent.com/projectcalico/calico/v3.24.5/manifests/tigera-operator.yaml
wget https://raw.githubusercontent.com/projectcalico/vpp-dataplane/v3.24.0/yaml/calico/installation-default.yaml
wget https://raw.githubusercontent.com/projectcalico/vpp-dataplane/v3.24.0/yaml/generated/calico-vpp.yaml
curl https://raw.githubusercontent.com/projectcalico/vpp-dataplane/v3.24.0/test/scripts/vppdev.sh | tee /usr/local/bin/calivppctl
chmod +x /usr/local/bin/calivppctl

kubectl create -f tigera-operator.yaml

vi installation-default.yaml
apiVersion: operator.tigera.io/v1
kind: Installation
metadata:
  name: default
spec:
  # Configures Calico networking.
  calicoNetwork:
    linuxDataplane: VPP
    ipPools:
    - blockSize: 24
      cidr: 10.244.0.0/16
      encapsulation: None                ##### NO IPIP &amp;&amp; NO VXLAN
---

kubectl create -f installation-default.yaml

root@calvpp1:~/calico# lshw -businfo -c network
Bus info          Device      Class          Description
========================================================
pci@0000:00:03.0  ens3        network        VMXNET3 Ethernet Controller
pci@0000:00:04.0  ens4        network        VMXNET3 Ethernet Controller
root@calvpp1:~/calico# driverctl list-devices | grep vmxnet3
0000:00:03.0 vmxnet3
0000:00:04.0 vmxnet3

vi calico-vpp.yaml
...
    socksvr {
        socket-name /var/run/vpp/vpp-api.sock
    }
    dpdk {
        dev 0000:00:04.0 { num-rx-queues 1  num-tx-queues 1 }
    }
    plugins {
        plugin default { enable }
        plugin dpdk_plugin.so { enable }
        plugin calico_plugin.so { enable }
        plugin ping_plugin.so { enable }
        plugin dispatch_trace_plugin.so { enable }
    }
    buffers {
      buffers-per-numa 131072
    }
  vpp_dataplane_interface: ens3
  vpp_uplink_driver: &#34;none&#34;
...

kubectl create -f calico-vpp.yaml
### 采用e1000，DPDK不成功，网络丢失，换e1000为vmxnet3，成功！

### NO DPDK 成功！
root@calvpp1:~/calico# diff 2.installation-default.yaml installation-default.yaml.org 
11,14d10
&lt;     ipPools:
&lt;     - blockSize: 24
&lt;       cidr: 10.244.0.0/16
&lt;       encapsulation: None

root@calvpp1:~/calico# diff 3.calico-vpp.yaml calico-vpp.yaml.org 
164c164
&lt;   vpp_dataplane_interface: ens3
---
&gt;   vpp_dataplane_interface: eth1
### NO DPDK 成功！


##########
#export https_proxy=http://10.1.1.12:8118
wget https://raw.githubusercontent.com/projectcalico/calico/v3.24.5/manifests/tigera-operator.yaml
wget https://raw.githubusercontent.com/projectcalico/vpp-dataplane/v3.24.0/yaml/calico/installation-default.yaml
wget https://raw.githubusercontent.com/projectcalico/vpp-dataplane/v3.24.0/yaml/generated/calico-vpp.yaml
curl https://raw.githubusercontent.com/projectcalico/vpp-dataplane/v3.24.0/test/scripts/vppdev.sh | tee /usr/local/bin/calivppctl
chmod +x /usr/local/bin/calivppctl

kubectl create -f tigera-operator.yaml
kubectl create -f installation-default.yaml
kubectl create -f calico-vpp.yaml

lshw -businfo -c network
driverctl list-devices 

ip tuntap list

iperf -s -i 1
iperf -t 10 -i 1 -c 10.1.5.113
</code></pre>
            <footer class="footline">
            </footer>
          </article>

          <article class="default">
<h1 id="11-vpp_hoststack_ldp_nginx">11. VPP_HostStack_LDP_nginx</h1>

<p>date: 2023-02-09  <br>
<strong>VPP/HostStack/LDP/nginx</strong>    <br>
<a href="https://wiki.fd.io/view/VPP/HostStack/LDP/nginx" target="_blank">https://wiki.fd.io/view/VPP/HostStack/LDP/nginx</a></p>
<pre tabindex="0"><code># HOST 10.1.1.8, VM:10.1.5.151
### 1. kernel performance
# vpp151
ip a
...
2: ens3: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000
    link/ether 52:54:00:07:a0:7d brd ff:ff:ff:ff:ff:ff
    altname enp0s3
    inet 10.1.5.151/21 brd 10.1.7.255 scope global ens3
3: ens4: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000
    link/ether 52:54:00:2f:3b:13 brd ff:ff:ff:ff:ff:ff
    altname enp0s4
4: ens5: &lt;BROADCAST,MULTICAST&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000
    link/ether 52:54:00:0e:34:c7 brd ff:ff:ff:ff:ff:ff
    altname enp0s5

ip l s ens4 up
ip a a 10.65.65.151/24 dev ens4
nginx -c /etc/nginx/nginx.conf

# 10.1.1.8
wget https://github.com/scutse/wrk-rpm/releases/download/4.1.0/wrk-4.1.0-1.el7.centos.x86_64.rpm
yum install wrk-4.1.0-1.el7.centos.x86_64.rpm
wrk -c100 -d30s -t50 http://10.65.65.151/
ab -n 10000 -c 100 http://10.65.65.151/
</code></pre><p>
<a href="#image-a071576d22eef5d17ec846e0d8172f5f">
<img src="../img/vpp/1675932528751.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-a071576d22eef5d17ec846e0d8172f5f">
<img src="../img/vpp/1675932528751.png" alt=""loading="lazy">
</a>

<a href="#image-dd903dd9a7d2bea86ba6401f2e91e98b">
<img src="../img/vpp/1675932577163.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-dd903dd9a7d2bea86ba6401f2e91e98b">
<img src="../img/vpp/1675932577163.png" alt=""loading="lazy">
</a></p>
<pre tabindex="0"><code>### 2. vpp performance
# vpp151
killall nginx
ip a f dev ens4
ip l s ens4 down
systemctl start vpp

vppctl
set interface state ens4 up
set interface ip address ens4 10.65.65.151/24
quit

LD_PRELOAD=/usr/lib/x86_64-linux-gnu/libvcl_ldpreload.so VCL_CONFIG=/root/vcl.conf nginx -c /etc/nginx/nginx.conf

# 10.1.1.8
wrk -c100 -d30s -t50 http://10.65.65.151/
ab -n 10000 -c 100 http://10.65.65.151/
</code></pre><p>
<a href="#image-4d28b16912029c01e16f407c5f12ce17">
<img src="../img/vpp/1675932753289.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-4d28b16912029c01e16f407c5f12ce17">
<img src="../img/vpp/1675932753289.png" alt=""loading="lazy">
</a>

<a href="#image-3bbe58e98ac5688dc1396cb15da4b287">
<img src="../img/vpp/1675932788951.png" alt="" style="height: auto; width: auto;" loading="lazy">
</a>
<a href="javascript:history.back();" class="lightbox" id="image-3bbe58e98ac5688dc1396cb15da4b287">
<img src="../img/vpp/1675932788951.png" alt=""loading="lazy">
</a></p>
<p><strong>快3倍</strong></p>

            <footer class="footline">
            </footer>
          </article>

          </section>        </div>
      </main>
    </div>
    <script src="../js/clipboard.min.js?1675933203" defer></script>
    <script src="../js/perfect-scrollbar.min.js?1675933203" defer></script>
    <script src="../js/theme.js?1675933203" defer></script>
  </body>
</html>

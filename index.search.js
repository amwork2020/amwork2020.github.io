var relearn_search_index = [
  {
    "content": "date: 2023-02-04 https://wiki.fd.io/view/VPP/Progressive_VPP_Tutorial https://s3-docs.fd.io/vpp/22.10/\nvpp u2204 vmware : 网卡均为vmxnet3 https://packagecloud.io/fdio apt install driverctl driverctl list-devices | grep vmxnet3 0000:0b:00.0 vmxnet3 0000:13:00.0 vmxnet3 0000:1b:00.0 vmxnet3 driverctl set-override 0000:13:00.0 vfio-pci # 重启后仍然绑定 driverctl unset-override 0000:13:00.0 # 解除绑定 dpdk-devbind.py -b vfio-pci 0000:13:00.0 # 重启后解除绑定 apt install dpdk dpdk-dev -y ## 22.10 curl -s https://packagecloud.io/install/repositories/fdio/release/script.deb.sh | sudo bash -vx cat /etc/apt/sources.list.d/fdio_release.list apt-get update apt-get install vpp vpp-plugin-core vpp-plugin-dpdk # apt-get install vpp-api-python python3-vpp-api vpp-dbg vpp-dev # Uninstall the Packages # apt-get remove --purge vpp* # systemctl enable vpp systemctl status vpp vi /lib/systemd/system/vpp.service ExecStartPre=-/sbin/modprobe vfio-pci vi /etc/vpp/startup.conf dpdk { uio-driver vfio-pci dev 0000:13:00.0 dev 0000:1b:00.0 } systemctl daemon-reload \u0026\u0026 systemctl restart vpp \u0026\u0026 systemctl status vpp vppctl show ver vppctl 提示符： vpp# set interface state GigabitEthernet13/0/0 up set interface state GigabitEthernet1b/0/0 up set interface ip address GigabitEthernet13/0/0 1.1.1.1/24 set interface ip address GigabitEthernet1b/0/0 2.1.1.1/24 show interface ubuntu0: ubuntu1: ubuntu0 与 ubuntu1 可以互相ping通。 ubuntu0: iperf -s -i 1\nubuntu1: iperf -t 10 -i 1 -c 1.1.1.2 dpdk有2-3G，网卡是10G\n不用dpdk，重启后：\nip l s ens224 up ip l s ens256 up ip a a 1.1.1.1/24 dev ens224 ip a a 2.1.1.1/24 dev ens256 路由方式1G，这个对的。 clean:\nip a flush ens224 ip a flush ens256 ip l s ens224 down ip l s ens256 down ",
    "description": "",
    "tags": null,
    "title": "0. vpp-u2204",
    "uri": "/vpp/0.vpp-u2204/index.html"
  },
  {
    "content": "date: 2023-02-04\nVMWARE 加网卡 (192.168.68.56 8c 8G)\nhttps://www.dpdk.org/\nhttp://doc.dpdk.org/guides-22.11/\n编辑对应的 jammy.vmx，修改所有e1000为vmxnet3 ，多队列网卡 ethernet0.virtualDev = \"vmxnet3\" ethernet0.wakeOnPcktRcv = \"true\" ... ethernet1.virtualDev = \"vmxnet3\" ethernet1.wakeOnPcktRcv = \"true\" ethernet2.virtualDev = \"vmxnet3\" ethernet2.wakeOnPcktRcv = \"true\" lshw -c network -businfo root@jammy:~# ip a ... 2: ens192: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 00:0c:29:9f:04:74 brd ff:ff:ff:ff:ff:ff altname enp11s0 inet 192.168.68.56/24 brd 192.168.68.255 scope global ens192 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe9f:474/64 scope link valid_lft forever preferred_lft forever 3: ens224: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 00:0c:29:9f:04:7e brd ff:ff:ff:ff:ff:ff altname enp19s0 4: ens256: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 00:0c:29:9f:04:88 brd ff:ff:ff:ff:ff:ff altname enp27s0 apt install dpdk dpdk-dev -y vi /etc/default/grub GRUB_CMDLINE_LINUX=\"default_hugepagesz=1G hugepagesz=1G hugepages=4 iommu=pt intel_iommu=on\" #GRUB_CMDLINE_LINUX=\"default_hugepagesz=1G hugepagesz=1G hugepages=4 isolcpus=2-3 iommu=pt intel_iommu=on\" update-grub # grub2-mkconfig -o /boot/grub2/grub.cfg reboot ## dmesg | grep -e DMAR -e IOMMU cat /proc/cmdline | grep -e iommu=pt -e intel_iommu=on -e huge dmesg| grep -i iommu cat /proc/meminfo | grep Huge lscpu | grep NUMA dpdk-hugepages.py -s dpdk-devbind.py -s lshw -businfo -c network root@dpdk56:~# lshw -businfo -c network Bus info Device Class Description ==================================================== pci@0000:0b:00.0 ens192 network VMXNET3 Ethernet Controller pci@0000:13:00.0 ens224 network VMXNET3 Ethernet Controller pci@0000:1b:00.0 ens256 network VMXNET3 Ethernet Controller dmesg| grep -i iommu | grep -e 0000:0b:00.0 -e 0000:13:00.0 -e 0000:1b:00.0 root@dpdk56:~# dmesg| grep -i iommu | grep -e 0000:0b:00.0 -e 0000:13:00.0 -e 0000:1b:00.0 [ 2.723136] pci 0000:0b:00.0: Adding to iommu group 6 [ 2.723277] pci 0000:13:00.0: Adding to iommu group 7 [ 2.723432] pci 0000:1b:00.0: Adding to iommu group 8 root@jammy:~ # dpdk-devbind.py -s Network devices using kernel driver =================================== 0000:0b:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens192 drv=vmxnet3 unused=vfio-pci *Active* 0000:13:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens224 drv=vmxnet3 unused=vfio-pci 0000:1b:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens256 drv=vmxnet3 unused=vfio-pci dpdk-devbind.py -b vfio-pci 0000:13:00.0 0000:1b:00.0 # dpdk-devbind.py -b vfio-pci 0000:13:00.0 # dpdk-devbind.py -b vfio-pci 0000:1b:00.0 dpdk-devbind.py -s Network devices using DPDK-compatible driver ============================================ 0000:13:00.0 'VMXNET3 Ethernet Controller 07b0' drv=vfio-pci unused=vmxnet3 0000:1b:00.0 'VMXNET3 Ethernet Controller 07b0' drv=vfio-pci unused=vmxnet3 Network devices using kernel driver =================================== 0000:0b:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens192 drv=vmxnet3 unused=vfio-pci *Active* dpdk-hugepages.py -s root@jammy:~ # dpdk-hugepages.py -s Node Pages Size Total 0 4 1Gb 4Gb Hugepages mounted on /dev/hugepages ### build pkten export https_proxy=http://10.1.1.12:8118 wget https://github.com/pktgen/Pktgen-DPDK/archive/refs/tags/pktgen-22.07.1.tar.gz tar zxvf pktgen-22.07.1.tar.gz cd Pktgen-DPDK-pktgen-22.07.1 meson build cd build ninja 编译完毕后的pkten在[Pktgen dir]/build/app/pktgen ## 源码BUILD dpdk apt install -y build-essential ## pip3 install meson ninja apt install meson python3-pyelftools pkg-config libnuma-dev export http_proxy=http://10.1.1.12:8118 wget http://fast.dpdk.org/rel/dpdk-22.11.1.tar.xz tar Jxvf dpdk-22.11.1.tar.xz cd dpdk-stable-22.11.1 meson setup -Dexamples=all build cd build ninja # ninja install ### 因为 apt install dpdk dpdk-dev -y ./pktgen -l 1-3 -n 2 -- -T -P -m \"2.0,3.1\" -l: 使用CPU Cores 1、2、3 -n: 内存通道 --：此符号前是DPDK的配置参数，此符号后是DPDK Application的配置参数，此处即是Pktgen的参数 -T: 启用彩色文本输出 -P: Enable PROMISCUOUS mode on all ports -m string: 重点！指定cpu core与NIC的绑定关系，格式参照下图： VM1 (56) \u003c--------\u003e VM2 (57) root@dpdk56:~# ip a 2: ens192: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 00:0c:29:9f:04:74 brd ff:ff:ff:ff:ff:ff altname enp11s0 inet 192.168.68.56/24 brd 192.168.68.255 scope global ens192 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe9f:474/64 scope link valid_lft forever preferred_lft forever 3: ens224: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 00:0c:29:9f:04:7e brd ff:ff:ff:ff:ff:ff altname enp19s0 4: ens256: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 00:0c:29:9f:04:88 brd ff:ff:ff:ff:ff:ff altname enp27s0 root@dpdk57:~# ip a 2: ens192: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 00:0c:29:22:de:b7 brd ff:ff:ff:ff:ff:ff altname enp11s0 inet 192.168.68.57/24 brd 192.168.68.255 scope global ens192 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe22:deb7/64 scope link valid_lft forever preferred_lft forever 3: ens224: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 00:0c:29:22:de:c1 brd ff:ff:ff:ff:ff:ff altname enp19s0 4: ens256: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 00:0c:29:22:de:cb brd ff:ff:ff:ff:ff:ff altname enp27s0 dpdk-devbind.py -b vfio-pci 0000:13:00.0 dpdk-devbind.py -s 00:0c:29:9f:04:7e (vm1 ens224) --- 00:0c:29:22:de:c1 (vm2 ens224) vm1:\nroot@dpdk56:~# /root/Pktgen-DPDK-pktgen-22.07.1/build/app/pktgen -l 0-1 -n 3 -- -T -P -m \"1.0\" set 0 dst mac 00:0c:29:22:de:c1 set 0 count 100000 str vm2:\nroot@dpdk57:~# dpdk-testpmd -l 0-1 -n 1 -- -i set fwd rxonly show port stats all clear port stats all start vm1: vm2: dpdk-l2fwd vm2:\ndpdk-devbind.py -b vfio-pci 0000:13:00.0 dpdk-devbind.py -b vfio-pci 0000:1b:00.0 root@dpdk57:~/dpdk-stable-22.11.1/build/examples# ./dpdk-l2fwd -l 0-3 -n 4 -- -q 1 -p 0x3 vm1:\ndpdk-devbind.py -b vfio-pci 0000:13:00.0 root@dpdk56:~# /root/Pktgen-DPDK-pktgen-22.07.1/build/app/pktgen -l 0-1 -n 4 -- -T -P -m \"1.0\" set 0 dst mac 00:0c:29:22:de:c1 set 0 count 1000 start 0 vm2: vm1: ",
    "description": "",
    "tags": null,
    "title": "1. vmware-dpdk",
    "uri": "/dpdk/1.vmware-dpdk/index.html"
  },
  {
    "content": "date: 2023-02-04\n# vm.xml \u003cinterface type='bridge'\u003e \u003csource bridge='br0'/\u003e \u003cmodel type='vmxnet3'/\u003e \u003c/interface\u003e \u003cinterface type='bridge'\u003e \u003csource bridge='br0'/\u003e \u003cmodel type='vmxnet3'/\u003e \u003c/interface\u003e ## HOST : 10.1.1.8 ../startVMs/startvm.sh vpp_5.151 jammy-server-cloudimg-amd64.20221210.pw1-10.1.5.3.img 4 8 ######### 0.prepare apt update apt -y full-upgrade ln -sf ../usr/share/zoneinfo/Asia/Shanghai /etc/localtime [ -f /var/run/reboot-required ] \u0026\u0026 reboot -f ######### 1. 启用rc.local cat \u003c\u003c EOF \u003e\u003e /etc/rc.local #!/bin/bash echo 1 \u003e /sys/module/vfio/parameters/enable_unsafe_noiommu_mode EOF chmod +x /etc/rc.local cat \u003c\u003c EOF \u003e\u003e /lib/systemd/system/rc-local.service [Install] WantedBy=multi-user.target EOF cat /lib/systemd/system/rc-local.service # 启用服务 systemctl enable rc-local systemctl start rc-local systemctl status rc-local # 查看是否成功 cat /sys/module/vfio/parameters/enable_unsafe_noiommu_mode echo \"vfio-pci\" \u003e /etc/modules-load.d/95-vpp.conf ######### 2. hugepages cat \u003c\u003cEOF \u003e\u003e /etc/sysctl.conf vm.nr_hugepages = 2048 EOF sysctl -p ######### 3. vpp \u0026\u0026 dpdk lshw -businfo -c network apt install dpdk dpdk-dev -y ## https://packagecloud.io/fdio #curl -s https://packagecloud.io/install/repositories/fdio/release/script.deb.sh | sudo bash curl -s https://packagecloud.io/install/repositories/fdio/2210/script.deb.sh | sudo bash cat /etc/apt/sources.list.d/fdio_2210.list apt update apt install vpp vpp-plugin-core vpp-plugin-dpdk -y # systemctl enable vpp # systemctl disable vpp systemctl status vpp mkdir -p /var/log/vpp cat /etc/sysctl.d/80-vpp.conf vi /lib/systemd/system/vpp.service ExecStartPre=-/sbin/modprobe vfio-pci ExecStartPre=-/bin/bash -c 'echo 1 \u003e /sys/module/vfio/parameters/enable_unsafe_noiommu_mode \u0026\u0026 sleep 2' vi /etc/vpp/startup.conf dpdk { uio-driver vfio-pci dev 0000:00:04.0 } systemctl daemon-reload \u0026\u0026 systemctl restart vpp ## CPU0 100% vppctl show ver vppctl show interface apt install docker.io -y docker pull ubuntu:22.04 vi Dockerfile FROM ubuntu:22.04 RUN sed -i 's/archive.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.list \u0026\u0026 \\ sed -i 's/security.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.list RUN apt-get update -y \u0026\u0026 apt-get install -y tzdata \u0026\u0026 \\ ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \u0026\u0026 \\ dpkg-reconfigure -f noninteractive tzdata RUN apt-get install dpdk kmod curl vim -y RUN curl -s https://packagecloud.io/install/repositories/fdio/release/script.deb.sh | bash \u0026\u0026 \\ apt-get update -y \u0026\u0026 apt-get install vpp vpp-plugin-core vpp-plugin-dpdk -y RUN mkdir -p /var/log/vpp ### vpp v22.10-release ### dpdk 21.11 docker build -t amwork2010/vppdpdk:22.10 . docker run --privileged \\ -v /sys/bus/pci/devices:/sys/bus/pci/devices \\ -v /sys/kernel/mm/hugepages:/sys/kernel/mm/hugepages \\ -v /sys/devices/system/node:/sys/devices/system/node \\ -v /lib/modules:/lib/modules \\ -v /dev:/dev \\ -it amwork2010/vppdpdk:22.10 bash docker run --privileged \\ -v /sys/bus/pci/devices:/sys/bus/pci/devices \\ -v /sys/kernel/mm/hugepages:/sys/kernel/mm/hugepages \\ -v /sys/devices/system/node:/sys/devices/system/node \\ -v /lib/modules:/lib/modules \\ -v /dev:/dev \\ -d amwork2010/vppdpdk:22.10 sleep infinity vi /etc/vpp/startup.conf dpdk { uio-driver vfio-pci dev 0000:00:04.0 } vpp -c /etc/vpp/startup.conf \u0026 #### docker run --privileged \\ -v /lib/modules:/lib/modules \\ -d amwork2010/vppdpdk:22.10 sleep infinity # 也可以banding , kmod ： the kmod package would provide modinfo, modprobe and other related tools. # root@714c8d6fc89e:/# modprobe vfio-pci # modprobe: FATAL: Module vfio-pci not found in directory /lib/modules/5.15.0-58-generic # so: -v /lib/modules:/lib/modules # vpp 启动 带不到dpdk网卡，docker stop ， 再start，再 vpp -c /etc/vpp/startup.conf \u0026 才可以banding ",
    "description": "",
    "tags": null,
    "title": "0. vpp-qemu",
    "uri": "/vpp/0.vpp-qemu/index.html"
  },
  {
    "content": "date: 2023-02-04 HOST: 10.1.1.12, guest: j1210 10.1.5.110\nroot@junnan-gpu:/u01/vms/j1210# more vm.xml \u003cdomain type='kvm'\u003e \u003cname\u003ej1210\u003c/name\u003e \u003cvcpu current='8'\u003e24\u003c/vcpu\u003e \u003cmemory\u003e8388608\u003c/memory\u003e \u003cos\u003e \u003ctype arch='x86_64' machine='pc'\u003ehvm\u003c/type\u003e \u003cbootmenu enable='yes'/\u003e \u003c/os\u003e \u003cfeatures\u003e \u003cacpi/\u003e \u003capic/\u003e \u003cpae/\u003e \u003c/features\u003e \u003ccpu mode=\"host-passthrough\" check=\"none\" migratable=\"on\"/\u003e \u003cclock offset='utc'/\u003e \u003con_poweroff\u003edestroy\u003c/on_poweroff\u003e \u003con_reboot\u003erestart\u003c/on_reboot\u003e \u003con_crash\u003edestroy\u003c/on_crash\u003e \u003cdevices\u003e \u003cemulator\u003e/usr/bin/kvm\u003c/emulator\u003e \u003cdisk type='file' device='disk'\u003e \u003cdriver name='qemu' type='qcow2'/\u003e \u003csource file='/u01/vms/j1210/disk'/\u003e \u003ctarget dev='vda' bus='virtio'/\u003e \u003cboot order='1'/\u003e \u003c/disk\u003e \u003cinterface type='bridge'\u003e \u003csource bridge='br0'/\u003e \u003cmodel type='virtio'/\u003e \u003c/interface\u003e \u003cinterface type='network'\u003e \u003csource network='default'/\u003e \u003cmodel type='e1000'/\u003e \u003c/interface\u003e \u003cinterface type='network'\u003e \u003csource network='default'/\u003e \u003cmodel type='e1000'/\u003e \u003c/interface\u003e \u003cserial type='pty'\u003e \u003ctarget port='0'/\u003e \u003c/serial\u003e \u003cconsole type='pty'\u003e \u003ctarget type='serial' port='0'/\u003e \u003c/console\u003e \u003cgraphics type='vnc' port='-1' autoport='yes' listen='0.0.0.0'/\u003e \u003cvideo\u003e \u003cmodel type='cirrus' vram='65536' heads='1'/\u003e \u003c/video\u003e \u003cinput type='tablet' bus='usb'/\u003e \u003cinput type='mouse' bus='ps2'/\u003e \u003c/devices\u003e \u003c/domain\u003e \u003cinterface type='bridge'\u003e \u003csource bridge='br0'/\u003e \u003cmodel type='virtio'/\u003e \u003c/interface\u003e \u003cinterface type='network'\u003e \u003csource network='default'/\u003e \u003cmodel type='e1000'/\u003e \u003c/interface\u003e \u003cinterface type='network'\u003e \u003csource network='default'/\u003e \u003cmodel type='e1000'/\u003e \u003c/interface\u003e vi /etc/default/grub GRUB_CMDLINE_LINUX=\"default_hugepagesz=1G hugepagesz=1G hugepages=4\" #GRUB_CMDLINE_LINUX=\"default_hugepagesz=1G hugepagesz=1G hugepages=4 isolcpus=2-3 iommu=pt intel_iommu=on\" update-grub apt install dpdk dpdk-dev -y root@ubuntu:~# dpdk-devbind.py -s Network devices using kernel driver =================================== 0000:00:03.0 'Virtio network device 1000' if=ens3 drv=virtio-pci unused=vfio-pci *Active* 0000:00:04.0 '82540EM Gigabit Ethernet Controller 100e' if=ens4 drv=e1000 unused=vfio-pci 0000:00:05.0 '82540EM Gigabit Ethernet Controller 100e' if=ens5 drv=e1000 unused=vfio-pci root@ubuntu:~# dpdk-devbind.py -b vfio-pci 0000:00:04.0 Error: bind failed for 0000:00:04.0 - Cannot bind to driver vfio-pci: [Errno 22] Invalid argument Error: unbind failed for 0000:00:04.0 - Cannot open /sys/bus/pci/drivers//unbind: [Errno 13] Permission denied: '/sys/bus/pci/drivers//unbind' root@ubuntu:~# cat /sys/module/vfio/parameters/enable_unsafe_noiommu_mode N root@ubuntu:~# echo 1 \u003e /sys/module/vfio/parameters/enable_unsafe_noiommu_mode echo 1 \u003e /sys/module/vfio/parameters/enable_unsafe_noiommu_mode root@ubuntu:~# cat /sys/module/vfio/parameters/enable_unsafe_noiommu_mode Y root@ubuntu:~# dpdk-devbind.py -b vfio-pci 0000:00:04.0 root@ubuntu:~# dpdk-devbind.py -b vfio-pci 0000:00:05.0 root@ubuntu:~# dpdk-devbind.py -s Network devices using DPDK-compatible driver ============================================ 0000:00:04.0 '82540EM Gigabit Ethernet Controller 100e' drv=vfio-pci unused=e1000 0000:00:05.0 '82540EM Gigabit Ethernet Controller 100e' drv=vfio-pci unused=e1000 Network devices using kernel driver =================================== 0000:00:03.0 'Virtio network device 1000' if=ens3 drv=virtio-pci unused=vfio-pci *Active* root@ubuntu:~# dpdk-testpmd -l0-3 -- -i --nb-cores=2 --nb-ports=2 --total-num-mbufs=2048 ... start ... stop # top 会看到 2cpu 100%us 或者：\nroot@ubuntu:~# dpdk-devbind.py -s Network devices using kernel driver =================================== 0000:00:03.0 'Virtio network device 1000' if=ens3 drv=virtio-pci unused=vfio-pci *Active* 0000:00:04.0 '82540EM Gigabit Ethernet Controller 100e' if=ens4 drv=e1000 unused=vfio-pci 0000:00:05.0 '82540EM Gigabit Ethernet Controller 100e' if=ens5 drv=e1000 unused=vfio-pci apt install dpdk-kmods-dkms # 安装igb_uio modprobe igb_uio root@ubuntu:~# dpdk-devbind.py -s Network devices using kernel driver =================================== 0000:00:03.0 'Virtio network device 1000' if=ens3 drv=virtio-pci unused=igb_uio,vfio-pci *Active* 0000:00:04.0 '82540EM Gigabit Ethernet Controller 100e' if=ens4 drv=e1000 unused=igb_uio,vfio-pci 0000:00:05.0 '82540EM Gigabit Ethernet Controller 100e' if=ens5 drv=e1000 unused=igb_uio,vfio-pci root@ubuntu:~# dpdk-devbind.py -b igb_uio 0000:00:04.0 root@ubuntu:~# dpdk-devbind.py -b igb_uio 0000:00:05.0 root@ubuntu:~# dpdk-devbind.py -s Network devices using DPDK-compatible driver ============================================ 0000:00:04.0 '82540EM Gigabit Ethernet Controller 100e' drv=igb_uio unused=e1000,vfio-pci 0000:00:05.0 '82540EM Gigabit Ethernet Controller 100e' drv=igb_uio unused=e1000,vfio-pci Network devices using kernel driver =================================== 0000:00:03.0 'Virtio network device 1000' if=ens3 drv=virtio-pci unused=igb_uio,vfio-pci *Active* root@ubuntu:~# dpdk-testpmd -l0-3 -- -i --nb-cores=2 --nb-ports=2 --total-num-mbufs=2048 ... start ... stop # top 会看到 2cpu 100%us # build apt install build-essential pip3 install meson ninja apt install meson python3-pyelftools pkg-config libnuma-dev wget http://fast.dpdk.org/rel/dpdk-22.11.1.tar.xz tar Jxvf dpdk-22.11.1.tar.xz cd dpdk-stable-22.11.1 meson setup -Dexamples=all build cd build ninja ninja install root@junnan-gpu:~# ll /usr/bin/kvm lrwxrwxrwx 1 root root 18 12月 8 17:17 /usr/bin/kvm -\u003e qemu-system-x86_64* root@junnan-gpu:~# qemu-system-x86_64 --version QEMU emulator version 6.2.0 (Debian 1:6.2+dfsg-2ubuntu6.6) Copyright (c) 2003-2021 Fabrice Bellard and the QEMU Project developers root@junnan-gpu:~# qemu-system-x86_64 -device ? ... Network devices: name \"e1000\", bus PCI, alias \"e1000-82540em\", desc \"Intel Gigabit Ethernet\" name \"e1000-82544gc\", bus PCI, desc \"Intel Gigabit Ethernet\" name \"e1000-82545em\", bus PCI, desc \"Intel Gigabit Ethernet\" name \"e1000e\", bus PCI, desc \"Intel 82574L GbE Controller\" name \"i82550\", bus PCI, desc \"Intel i82550 Ethernet\" name \"i82551\", bus PCI, desc \"Intel i82551 Ethernet\" name \"i82557a\", bus PCI, desc \"Intel i82557A Ethernet\" name \"i82557b\", bus PCI, desc \"Intel i82557B Ethernet\" name \"i82557c\", bus PCI, desc \"Intel i82557C Ethernet\" name \"i82558a\", bus PCI, desc \"Intel i82558A Ethernet\" name \"i82558b\", bus PCI, desc \"Intel i82558B Ethernet\" name \"i82559a\", bus PCI, desc \"Intel i82559A Ethernet\" name \"i82559b\", bus PCI, desc \"Intel i82559B Ethernet\" name \"i82559c\", bus PCI, desc \"Intel i82559C Ethernet\" name \"i82559er\", bus PCI, desc \"Intel i82559ER Ethernet\" name \"i82562\", bus PCI, desc \"Intel i82562 Ethernet\" name \"i82801\", bus PCI, desc \"Intel i82801 Ethernet\" name \"ne2k_isa\", bus ISA name \"ne2k_pci\", bus PCI name \"pcnet\", bus PCI name \"pvrdma\", bus PCI, desc \"RDMA Device\" name \"rocker\", bus PCI, desc \"Rocker Switch\" name \"rtl8139\", bus PCI name \"tulip\", bus PCI name \"usb-net\", bus usb-bus name \"virtio-net-device\", bus virtio-bus name \"virtio-net-pci\", bus PCI, alias \"virtio-net\" name \"virtio-net-pci-non-transitional\", bus PCI name \"virtio-net-pci-transitional\", bus PCI name \"vmxnet3\", bus PCI, desc \"VMWare Paravirtualized Ethernet v3\" ... 所以可以修改 e1000 --\u003e vmxnet3 \u003cinterface type='bridge'\u003e \u003csource bridge='br0'/\u003e \u003cmodel type='vmxnet3'/\u003e \u003c/interface\u003e \u003cinterface type='bridge'\u003e \u003csource bridge='br0'/\u003e \u003cmodel type='vmxnet3'/\u003e \u003c/interface\u003e \u003cinterface type='bridge'\u003e \u003csource bridge='br0'/\u003e \u003cmodel type='vmxnet3'/\u003e \u003c/interface\u003e # u22.04 10.1.5.161 ######### 0.prepare apt update apt -y full-upgrade ln -sf ../usr/share/zoneinfo/Asia/Shanghai /etc/localtime [ -f /var/run/reboot-required ] \u0026\u0026 reboot -f ######### 1. 启用rc.local cat \u003c\u003c EOF \u003e\u003e /etc/rc.local #!/bin/bash echo 1 \u003e /sys/module/vfio/parameters/enable_unsafe_noiommu_mode EOF chmod +x /etc/rc.local cat \u003c\u003c EOF \u003e\u003e /lib/systemd/system/rc-local.service [Install] WantedBy=multi-user.target EOF cat /lib/systemd/system/rc-local.service # 启用服务 systemctl enable rc-local systemctl start rc-local systemctl status rc-local # 查看是否成功 cat /sys/module/vfio/parameters/enable_unsafe_noiommu_mode echo \"vfio-pci\" \u003e /etc/modules-load.d/95-vpp.conf ######### 2. hugepages cat \u003c\u003cEOF \u003e\u003e /etc/sysctl.conf vm.nr_hugepages = 2048 EOF sysctl -p ######### 3. dpdk apt install dpdk dpdk-dev -y dpdk-devbind.py -s dpdk-devbind.py -b vfio-pci 0000:00:04.0 dpdk-devbind.py -b vfio-pci 0000:00:05.0 root@ubuntu:~# dpdk-devbind.py -s Network devices using kernel driver =================================== 0000:00:03.0 'VMXNET3 Ethernet Controller 07b0' if=ens3 drv=vmxnet3 unused=vfio-pci *Active* 0000:00:04.0 'VMXNET3 Ethernet Controller 07b0' if=ens4 drv=vmxnet3 unused=vfio-pci 0000:00:05.0 'VMXNET3 Ethernet Controller 07b0' if=ens5 drv=vmxnet3 unused=vfio-pci root@ubuntu:~# dpdk-devbind.py -s Network devices using DPDK-compatible driver ============================================ 0000:00:04.0 'VMXNET3 Ethernet Controller 07b0' drv=vfio-pci unused=vmxnet3 0000:00:05.0 'VMXNET3 Ethernet Controller 07b0' drv=vfio-pci unused=vmxnet3 Network devices using kernel driver =================================== 0000:00:03.0 'VMXNET3 Ethernet Controller 07b0' if=ens3 drv=vmxnet3 unused=vfio-pci *Active* apt install docker.io -y docker pull ubuntu:22.04 cat \u003c\u003c EOF \u003e Dockerfile FROM ubuntu:22.04 RUN sed -i 's/archive.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.list \u0026\u0026 \\ sed -i 's/security.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.list RUN apt-get update -y \u0026\u0026 \\ apt-get install dpdk kmod -y EOF docker build -t amwork2010/dpdk:1 . docker run --privileged \\ -v /sys/bus/pci/devices:/sys/bus/pci/devices \\ -v /sys/kernel/mm/hugepages:/sys/kernel/mm/hugepages \\ -v /sys/devices/system/node:/sys/devices/system/node \\ -v /lib/modules:/lib/modules \\ -v /dev:/dev \\ -it amwork2010/dpdk:1 bash docker run --privileged \\ -v /lib/modules:/lib/modules \\ -it amwork2010/dpdk:1 bash ## 也可以banding , kmod ： the kmod package would provide modinfo, modprobe and other related tools. root@714c8d6fc89e:/# modprobe vfio-pci modprobe: FATAL: Module vfio-pci not found in directory /lib/modules/5.15.0-58-generic so need: -v /lib/modules:/lib/modules ",
    "description": "",
    "tags": null,
    "title": "2. qemu-dpdk",
    "uri": "/dpdk/2.qemu-dpdk/index.html"
  },
  {
    "content": "date: 2023-02-04 https://wiki.fd.io/view/VPP/Progressive_VPP_Tutorial https://s3-docs.fd.io/vpp/22.10/\nRocky Linux 8.7\n# rocky87 vi /etc/default/grub GRUB_CMDLINE_LINUX=\"........... default_hugepagesz=1G hugepagesz=1G hugepages=4 iommu=pt intel_iommu=on\" grub2-mkconfig -o /boot/grub2/grub.cfg reboot [root@rocky87 ~]# lshw -c network -businfo Bus info Device Class Description ======================================================== pci@0000:04:00.0 ens161 network VMXNET3 Ethernet Controller pci@0000:0b:00.0 ens192 network VMXNET3 Ethernet Controller pci@0000:13:00.0 ens224 network VMXNET3 Ethernet Controller pci@0000:1b:00.0 ens256 network VMXNET3 Ethernet Controller yum install -y driverctl driverctl list-devices | grep -i vmxnet3 [root@rocky87 ~]# driverctl list-devices | grep -i vmxnet3 0000:04:00.0 vmxnet3 0000:0b:00.0 vmxnet3 0000:13:00.0 vmxnet3 0000:1b:00.0 vmxnet3 yum install -y epel-release sed -e 's|^metalink=|#metalink=|g' \\ -e 's|^#baseurl=https\\?://download.fedoraproject.org/pub/epel/|baseurl=https://mirrors.ustc.edu.cn/epel/|g' \\ -e 's|^#baseurl=https\\?://download.example/pub/epel/|baseurl=https://mirrors.ustc.edu.cn/epel/|g' \\ -i.bak \\ /etc/yum.repos.d/epel.repo yum install -y mbedtls # https://packagecloud.io/fdio # curl -s https://packagecloud.io/install/repositories/fdio/release/script.rpm.sh | sudo bash curl -s https://packagecloud.io/install/repositories/fdio/2106/script.rpm.sh | sudo bash yum install -y vpp vpp-plugins vpp-devel vpp-debuginfo vpp-api-python3 vpp-api-lua vpp-ext-deps #yum install -y vpp vpp-plugins vpp-devel vpp-debuginfo vpp-api-python3 vpp-api-lua vpp-ext-deps ### auto install vpp-lib vpp-selinux-policy [root@rocky87 yum.repos.d]# yum install vpp vpp-plugins vpp-devel vpp-debuginfo vpp-api-python3 vpp-api-lua vpp-ext-deps Last metadata expiration check: 0:02:59 ago on Sun 01 Jan 2023 10:30:25 AM CST. Error: Problem: cannot install the best candidate for the job - nothing provides libmbedcrypto.so.3()(64bit) needed by vpp-plugins-21.06-release.x86_64 - nothing provides libmbedtls.so.12()(64bit) needed by vpp-plugins-21.06-release.x86_64 - nothing provides libmbedx509.so.0()(64bit) needed by vpp-plugins-21.06-release.x86_64 (try to add '--skip-broken' to skip uninstallable packages or '--nobest' to use not only best candidate packages) [root@rocky87 yum.repos.d]# rpm -ql mbedtls ... /usr/lib64/libmbedcrypto.so.2.28.1 /usr/lib64/libmbedcrypto.so.7 /usr/lib64/libmbedtls.so.14 /usr/lib64/libmbedtls.so.2.28.1 /usr/lib64/libmbedx509.so.1 /usr/lib64/libmbedx509.so.2.28.1 ... cd /usr/lib64/ ln -s libmbedcrypto.so.7 libmbedcrypto.so.3 ln -s libmbedtls.so.14 libmbedtls.so.12 ln -s libmbedx509.so.1 libmbedx509.so.0 yum install -y vpp vpp-devel vpp-debuginfo vpp-api-python3 vpp-api-lua vpp-ext-deps wget --content-disposition https://packagecloud.io/fdio/2106/packages/el/8/vpp-plugins-21.06.0-3~gbb25fbf28~b50.x86_64.rpm/download.rpm?distro_version_id=205 rpm -ivh vpp-plugins-21.06.0-3~gbb25fbf28~b50.x86_64.rpm --nodeps 修改配置同ubuntu ip l s ens224 down ip l s ens256 down systemctl restart vpp systemctl status vpp 自己build 失败！make install-dep 依赖包安装不全，名字也对不上，比如：python36-ply实际能安装python3-ply，python-virtualenv 实际 python3-virtualenv devtoolset-9 devtoolset-9-libasan-devel 根本没有 gcc9的，安装的是gcc version 8.5.0 20210514 (Red Hat 8.5.0-15) (GCC) # build OK, 见下篇文档 ",
    "description": "",
    "tags": null,
    "title": "0. vpp-rocky87",
    "uri": "/vpp/0.vpp-rocky87/index.html"
  },
  {
    "content": "date: 2023-02-04 192.168.68.56、 192.168.68.57\napt install -y openvswitch-switch-dpdk update-alternatives --set ovs-vswitchd /usr/lib/openvswitch-switch-dpdk/ovs-vswitchd-dpdk ovs-vswitchd --version systemctl restart openvswitch-switch.service root@dpdk56:~# ovs-vswitchd --version ovs-vswitchd (Open vSwitch) 2.17.3 DPDK 21.11.2 root@dpdk56:~# dpdk-devbind.py -s Network devices using kernel driver =================================== 0000:0b:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens192 drv=vmxnet3 unused=vfio-pci *Active* 0000:13:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens224 drv=vmxnet3 unused=vfio-pci 0000:1b:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens256 drv=vmxnet3 unused=vfio-pci dpdk-devbind.py -b vfio-pci 0000:13:00.0 0000:1b:00.0 root@dpdk56:~# dpdk-devbind.py -b vfio-pci 0000:13:00.0 0000:1b:00.0 root@dpdk56:~# dpdk-devbind.py -s Network devices using DPDK-compatible driver ============================================ 0000:13:00.0 'VMXNET3 Ethernet Controller 07b0' drv=vfio-pci unused=vmxnet3 0000:1b:00.0 'VMXNET3 Ethernet Controller 07b0' drv=vfio-pci unused=vmxnet3 ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-init=true ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-socket-mem=\"1024,0\" ### 只有一个numa node0 ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-lcore-mask=0x2 ### 0b0010 --\u003e Cpu1 ovs-vsctl set Open_vSwitch . other_config:pmd-cpu-mask=0x4 ### 0b0100 --\u003e Cpu2 ovs-vsctl get Open_vSwitch . dpdk_initialized ovs-vsctl get Open_vSwitch . dpdk_version # dpdk-init 指定 OVS 是否应该初始化并支持 DPDK 端口。该字段可以是true或try。值true将导致 ovs-vswitchd 进程在初始化失败时中止。值try表示即使 EAL 初始化失败，ovs-vswitchd 进程也应该继续运行。 # dpdk-lcore-mask 指定应该生成 dpdk lcore 线程的 CPU 核心，并需要十六进制字符串（例如“0x123”）。 # dpdk-socket-mem 逗号分隔的内存列表，用于从特定套接字上的大页面中预分配。如果未指定，则默认情况下不会设置此选项。将使用 DPDK 默认值。 # dpdk-hugepage-dir hugetlbfs挂载目录 # vhost-sock-dir 设置虚拟主机用户 unix 套接字文件路径的选项。 root@dpdk56:~# ovs-vsctl list open_vswitch _uuid : 6b97fe90-77a5-4a61-a8b6-0b74ed9f803d bridges : [db9afd87-ad4d-4d1a-842a-3e2eb21abf9e] cur_cfg : 12 datapath_types : [netdev, system] datapaths : {} db_version : \"8.3.0\" dpdk_initialized : true dpdk_version : \"DPDK 21.11.2\" external_ids : {hostname=dpdk56, rundir=\"/var/run/openvswitch\", system-id=\"921fb59b-cbab-4609-929b-875d5dedb844\"} iface_types : [bareudp, dpdk, dpdkvhostuser, dpdkvhostuserclient, erspan, geneve, gre, gtpu, internal, ip6erspan, ip6gre, lisp, patch, stt, system, tap, vxlan] manager_options : [] next_cfg : 12 other_config : {dpdk-init=\"true\", dpdk-lcore-mask=\"0x2\", dpdk-socket-mem=\"1024,0\", pmd-cpu-mask=\"0x4\"} ovs_version : \"2.17.3\" ssl : [] statistics : {} system_type : ubuntu system_version : \"22.04\" ovs-vsctl add-br br0 -- set bridge br0 datapath_type=netdev ovs-vsctl add-port br0 dpdk-p0 -- set Interface dpdk-p0 type=dpdk options:dpdk-devargs=0000:13:00.0 ovs-vsctl add-port br0 dpdk-p1 -- set Interface dpdk-p1 type=dpdk options:dpdk-devargs=0000:1b:00.0 root@dpdk56:~# ovs-vsctl show 6b97fe90-77a5-4a61-a8b6-0b74ed9f803d Bridge br0 datapath_type: netdev Port dpdk-p1 Interface dpdk-p1 type: dpdk options: {dpdk-devargs=\"0000:1b:00.0\"} Port dpdk-p0 Interface dpdk-p0 type: dpdk options: {dpdk-devargs=\"0000:13:00.0\"} Port br0 Interface br0 type: internal ovs_version: \"2.17.3\" 借助 pmd 多线程支持，OVS 默认为每个 NUMA 节点创建一个 pmd 线程，前提是该 NUMA 节点至少有一个 DPDK 接口添加到 OVS。 但是，在有多个端口/rxq 产生流量的情况下，可以通过创建在不同内核上运行的多个 pmd 线程来提高性能。 这些 pmd 线程可以通过各自负责不同的端口/rxq 来分担工作量。将端口/rxq 分配给 pmd 线程是自动完成的。 掩码中的设置位意味着创建了一个 pmd 线程并将其固定到相应的 CPU 内核。例如，要在核心 1 和 2 上运行 pmd 线程：0x110 = 0x6 ovs-vsctl set Open_vSwitch . other_config:pmd-cpu-mask=0x6 在 DPDK 端口添加到交换机后，轮询线程不断轮询 DPDK 设备并消耗 100% 的核心，可以从top和 ps命令中检查： $ top -H $ ps -eLo pid,psr,comm | grep pmd # To stop ovs-vswitchd \u0026 delete bridge, run: $ ovs-appctl -t ovs-vswitchd exit $ ovs-appctl -t ovsdb-server exit $ ovs-vsctl del-br br0 # 在笔记本T480上，跑一个还勉强，跑2个基本夯住了，跑不动。 ",
    "description": "",
    "tags": null,
    "title": "3. ovs-dpdk vmware",
    "uri": "/dpdk/3.ovs-dpdk.vmware/index.html"
  },
  {
    "content": "date: 2023-02-04 HOST: 10.1.1.12 VM : ovs1:10.1.5.131 ovs2:10.1.5.132\n# ovs1:10.1.5.131 ovs2:10.1.5.132 apt update apt -y full-upgrade ln -sf ../usr/share/zoneinfo/Asia/Shanghai /etc/localtime [ -f /var/run/reboot-required ] \u0026\u0026 reboot -f ######### 1. 启用rc.local cat \u003c\u003c EOF \u003e\u003e /etc/rc.local #!/bin/bash echo 1 \u003e /sys/module/vfio/parameters/enable_unsafe_noiommu_mode EOF chmod +x /etc/rc.local cat \u003c\u003c EOF \u003e\u003e /lib/systemd/system/rc-local.service [Install] WantedBy=multi-user.target EOF cat /lib/systemd/system/rc-local.service # 启用服务 systemctl enable rc-local systemctl start rc-local systemctl status rc-local # 查看是否成功 cat /sys/module/vfio/parameters/enable_unsafe_noiommu_mode echo \"vfio-pci\" \u003e /etc/modules-load.d/95-vpp.conf ######### 2. hugepages cat \u003c\u003cEOF \u003e\u003e /etc/sysctl.conf vm.nr_hugepages = 1024 EOF sysctl -p ######### 3. check apt install driverctl -y cat /proc/meminfo | grep Huge lshw -businfo -c network driverctl list-devices ######### 4. ovs+dpdk apt install -y openvswitch-switch-dpdk update-alternatives --set ovs-vswitchd /usr/lib/openvswitch-switch-dpdk/ovs-vswitchd-dpdk ovs-vswitchd --version systemctl restart openvswitch-switch.service root@ovs1:~# ip a 2: ens3: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 52:54:00:3a:2e:71 brd ff:ff:ff:ff:ff:ff altname enp0s3 inet 10.1.5.131/21 brd 10.1.7.255 scope global ens3 3: ens4: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 52:54:00:c0:e1:75 brd ff:ff:ff:ff:ff:ff altname enp0s4 4: ens5: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 52:54:00:33:8e:0b brd ff:ff:ff:ff:ff:ff altname enp0s5 root@ovs2:~# ip a 2: ens3: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 52:54:00:fd:02:a5 brd ff:ff:ff:ff:ff:ff altname enp0s3 inet 10.1.5.132/21 brd 10.1.7.255 scope global ens3 3: ens4: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 52:54:00:9e:bc:de brd ff:ff:ff:ff:ff:ff altname enp0s4 4: ens5: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 52:54:00:16:ea:af brd ff:ff:ff:ff:ff:ff altname enp0s5 root@ovs1:~# dpdk-devbind.py -s 0000:00:03.0 'VMXNET3 Ethernet Controller 07b0' if=ens3 drv=vmxnet3 unused=vfio-pci *Active* 0000:00:04.0 'VMXNET3 Ethernet Controller 07b0' if=ens4 drv=vmxnet3 unused=vfio-pci 0000:00:05.0 'VMXNET3 Ethernet Controller 07b0' if=ens5 drv=vmxnet3 unused=vfio-pci dpdk-devbind.py -b vfio-pci 0000:00:04.0 root@ovs1:~# dpdk-devbind.py -s 0000:00:04.0 'VMXNET3 Ethernet Controller 07b0' drv=vfio-pci unused=vmxnet3 ######### 5. configure ovs dpdk ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-init=true ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-socket-mem=\"1024,0\" ### 只有一个numa node0 ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-lcore-mask=0x2 ### 0b0010 --\u003e Cpu1 ovs-vsctl set Open_vSwitch . other_config:pmd-cpu-mask=0x4 ### 0b0100 --\u003e Cpu2 ovs-vsctl get Open_vSwitch . dpdk_initialized ovs-vsctl get Open_vSwitch . dpdk_version ovs-vsctl list open_vswitch # 10.1.5.131 # hwaddr=52:54:00:c0:e1:75 == ens4 ovs-vsctl add-br br-phy -- set Bridge br-phy datapath_type=netdev -- br-set-external-id br-phy bridge-id br-phy -- set bridge br-phy fail-mode=standalone \\ other_config:hwaddr=52:54:00:c0:e1:75 ovs-vsctl show ovs-ofctl show br-phy ovs-vsctl add-port br-phy dpdk0 -- set Interface dpdk0 type=dpdk options:dpdk-devargs=0000:00:04.0 ip addr add 1.1.1.1/24 dev br-phy ip link set br-phy up iperf -s -i 1 # 10.1.5.132 ovs-vsctl add-br br-phy -- set Bridge br-phy datapath_type=netdev -- br-set-external-id br-phy bridge-id br-phy -- set bridge br-phy fail-mode=standalone \\ other_config:hwaddr=52:54:00:9e:bc:de ovs-vsctl show ovs-ofctl show br-phy ovs-vsctl add-port br-phy dpdk0 -- set Interface dpdk0 type=dpdk options:dpdk-devargs=0000:00:04.0 ip addr add 1.1.1.2/24 dev br-phy ip link set br-phy up iperf -t 10 -i 1 -c 1.1.1.1 #### iperf测试性能只有700-800M，而不用DPDK，iperf测试性能2G左右，why? root@ovs1:~# ovs-ofctl dump-ports br-phy dpdk0 OFPST_PORT reply (xid=0x4): 1 ports port dpdk0: rx pkts=532994, bytes=395231512, drop=0, errs=0, frame=?, over=?, crc=? tx pkts=136677, bytes=9034000, drop=0, errs=0, coll=? root@ovs1:~# root@ovs1:~# ovs-ofctl dump-ports br-phy dpdk0 OFPST_PORT reply (xid=0x4): 1 ports port dpdk0: rx pkts=781948, bytes=771677632, drop=0, errs=0, frame=?, over=?, crc=? tx pkts=228977, bytes=15129284, drop=0, errs=0, coll=? root@ovs1:~# root@ovs1:~# ovs-ofctl dump-ports br-phy dpdk0 OFPST_PORT reply (xid=0x4): 1 ports port dpdk0: rx pkts=953469, bytes=1031276094, drop=0, errs=0, frame=?, over=?, crc=? tx pkts=314671, bytes=20785064, drop=0, errs=0, coll=? root@ovs1:~# root@ovs1:~# ovs-ofctl dump-ports br-phy dpdk0 OFPST_PORT reply (xid=0x4): 1 ports port dpdk0: rx pkts=1058565, bytes=1190342002, drop=0, errs=0, frame=?, over=?, crc=? tx pkts=366688, bytes=24220318, drop=0, errs=0, coll=? root@ovs1:~# root@ovs1:~# ovs-ofctl dump-ports br-phy dpdk0 OFPST_PORT reply (xid=0x4): 1 ports port dpdk0: rx pkts=1088174, bytes=1235080612, drop=0, errs=0, frame=?, over=?, crc=? tx pkts=381556, bytes=25201606, drop=0, errs=0, coll=? ###### 有数据呀，说明从dpdk走呀，why? # 1 docker run --name ng1-1 --net=none -p 5001:5001 --privileged=true -d nginx:alpine1 ovs-docker add-port br-phy eth0 ng1-1 docker exec -it ng1-1 ip addr add 1.1.1.11/24 dev eth0 # 2 docker run --name ng2-1 --net=none -p 5001:5001 --privileged=true -d nginx:alpine1 ovs-docker add-port br-phy eth0 ng2-1 docker exec -it ng2-1 ip addr add 1.1.1.12/24 dev eth0 # ovs-docker del-port br0 eth0 ng2-1 可以ping通，但iperf不通 ovs-vsctl add-br br-int -- set Bridge br-int datapath_type=netdev -- br-set-external-id br-int bridge-id br-int -- set bridge br-int fail-mode=standalone # 1 ovs-vsctl add-port br-int vxlan0 -- set interface vxlan0 type=vxlan options:remote_ip=1.1.1.2 # 2 ovs-vsctl add-port br-int vxlan0 -- set interface vxlan0 type=vxlan options:remote_ip=1.1.1.1 # 1 docker run --name ng1-3 --net=none -p 5001:5001 --privileged=true -d nginx:alpine1 ovs-docker add-port br-int eth0 ng1-3 docker exec -it ng1-3 ip addr add 3.3.3.31/24 dev eth0 # 2 docker run --name ng2-3 --net=none --privileged=true -d nginx:alpine1 ovs-docker add-port br-int eth0 ng2-3 docker exec -it ng2-3 ip addr add 3.3.3.32/24 dev eth0 可以ping通，但iperf不通 ovs-vsctl add-br br1 ovs-vsctl add-port br1 ens5 # 1 ip l s ens5 up ifconfig br1 2.2.2.1/24 up # 2 ip l s ens5 up ifconfig br1 2.2.2.2/24 up # 1 docker run --name ng1-2 --net=none --privileged=true -d nginx:alpine1 ovs-docker add-port br1 eth0 ng1-2 docker exec -it ng1-2 ip addr add 2.2.2.11/24 dev eth0 # 2 docker run --name ng2-2 --net=none --privileged=true -d nginx:alpine1 ovs-docker add-port br1 eth0 ng2-2 docker exec -it ng2-2 ip addr add 2.2.2.12/24 dev eth0 可以ping通，iperf通，2G左右！ cat \u003c\u003c EOF \u003e Dockerfile FROM nginx:alpine RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories; \\\\ apk add --no-cache bash iperf tcpdump; \\\\ rm -rf /var/cache/apk/*; EOF docker build -t nginx:alpine1 . +--------------+ | vm0 | 3.3.3.31/24 +--------------+ (vm_port0) | | | +--------------+ | br-int | 3.3.3.32/24 +--------------+ +--------------+ | vxlan0 | | vxlan0 | +--------------+ +--------------+ | | | | | | 1.1.1.1/24 | +--------------+ | | br-phy | 1.1.1.2/24 +--------------+ +---------------+ | dpdk0/eth1 |----------------------------------| eth1 | +--------------+ +---------------+ Host A with OVS. Remote host. https://docs.openvswitch.org/en/latest/howto/userspace-tunneling/ https://github.com/bytedance/ovs-dpdk/blob/open-source/Documentation/howto/userspace-tunneling.rst https://community.arm.com/arm-community-blogs/b/tools-software-ides-blog/posts/open-vswitch-with-dpdk-on-arm-setup-for-phy-phy-test ",
    "description": "",
    "tags": null,
    "title": "4. ovs-dpdk qemu",
    "uri": "/dpdk/4.ovs-dpdk.qemu/index.html"
  },
  {
    "content": "date: 2023-02-04 https://wiki.fd.io/view/VPP/Progressive_VPP_Tutorial https://s3-docs.fd.io/vpp/22.10/\nRocky Linux 8.7\nvi /etc/default/grub GRUB_CMDLINE_LINUX=\"..... default_hugepagesz=1G hugepagesz=1G hugepages=4 iommu=pt intel_iommu=on\" grub2-mkconfig -o /boot/grub2/grub.cfg reboot cp /etc/os-release /etc/os-release.bak vi /etc/os-release ID=\"rocky\" --\u003e centos VERSION_ID=\"8.7\" --\u003e VERSION_ID=\"8\" yum -y groupinstall \"Development Tools\" yum -y install git export https_proxy=http://10.1.1.12:8118 export http_proxy=http://10.1.1.12:8118 git clone https://github.com/FDio/vpp.git cd vpp git branch -a git checkout -b 2210 origin/stable/2210 git branch -a make install-dep make install-ext-dep make pkg-rpm #make build #make build-release cd /root mkdir -p rpm cd rpm/ mv /root/vpp/build/external/vpp-ext-deps-22.10-9.x86_64.rpm ./ mv /root/vpp/build-root/vpp-*rpm ./ [root@rocky87 rpm]# ll total 201588 -rw-r--r-- 1 root root 255504 Jan 27 08:39 vpp-22.10.0-3~gb89dcf824.x86_64.rpm -rw-r--r-- 1 root root 31608 Jan 27 08:39 vpp-api-lua-22.10.0-3~gb89dcf824.x86_64.rpm -rw-r--r-- 1 root root 71388 Jan 27 08:39 vpp-api-python3-22.10.0-3~gb89dcf824.x86_64.rpm -rw-r--r-- 1 root root 1212796 Jan 27 08:39 vpp-debuginfo-22.10.0-3~gb89dcf824.x86_64.rpm -rw-r--r-- 1 root root 5591264 Jan 27 08:39 vpp-debugsource-22.10.0-3~gb89dcf824.x86_64.rpm -rw-r--r-- 1 root root 1886688 Jan 27 08:39 vpp-devel-22.10.0-3~gb89dcf824.x86_64.rpm -rw-r--r-- 1 root root 84047680 Jan 27 08:03 vpp-ext-deps-22.10-9.x86_64.rpm -rw-r--r-- 1 root root 6461424 Jan 27 08:39 vpp-lib-22.10.0-3~gb89dcf824.x86_64.rpm -rw-r--r-- 1 root root 50819564 Jan 27 08:39 vpp-lib-debuginfo-22.10.0-3~gb89dcf824.x86_64.rpm -rw-r--r-- 1 root root 11108604 Jan 27 08:39 vpp-plugins-22.10.0-3~gb89dcf824.x86_64.rpm -rw-r--r-- 1 root root 44888316 Jan 27 08:40 vpp-plugins-debuginfo-22.10.0-3~gb89dcf824.x86_64.rpm -rw-r--r-- 1 root root 18520 Jan 27 08:39 vpp-selinux-policy-22.10.0-3~gb89dcf824.x86_64.rpm yum install *.rpm # 报错，conflicts with files，有冲突 yum install *.rpm --downloadonly # 先把需要的依赖包安装 cd /var/cache/dnf/..... yum install .... # 用rpm强制安装 cd /root/rpm rpm -ivh --force *.rpm systemctl status vpp vi /lib/systemd/system/vpp.service ExecStartPre=-/sbin/modprobe vfio-pci vi /etc/vpp/startup.conf dpdk { uio-driver vfio-pci dev 0000:13:00.0 dev 0000:1b:00.0 } plugins { path /usr/lib/vpp_plugins } systemctl daemon-reload \u0026\u0026 systemctl restart vpp \u0026\u0026 systemctl status vpp /opt/vpp/external/x86_64/bin/dpdk-devbind.py -s vppctl show ver vppctl show int vppctl show plugins export PATH=$PATH:/opt/vpp/external/x86_64/bin/ vmware:\n### vmware modprobe vfio-pci ip l s ens224 down ip l s ens256 down dpdk-devbind.py -b vfio-pci 0000:13:00.0 0000:1b:00.0 vi /lib/systemd/system/vpp.service ExecStartPre=-/usr/sbin/ip l s ens224 down ExecStartPre=-/usr/sbin/ip l s ens256 down ExecStartPre=-/sbin/modprobe vfio-pci ## 查看网卡信息 /opt/vpp/external/x86_64/bin/dpdk-devbind.py -s vi /etc/vpp/startup.conf dpdk { uio-driver vfio-pci dev 0000:13:00.0 dev 0000:1b:00.0 } plugins { path /usr/lib/vpp_plugins } systemctl daemon-reload \u0026\u0026 systemctl restart vpp \u0026\u0026 systemctl status vpp /opt/vpp/external/x86_64/bin/dpdk-devbind.py -s qemu:\n### qemu ### 以下可以不用做 BEGIN echo \"vfio-pci\" \u003e /etc/modules-load.d/95-vpp.conf cat \u003c\u003c EOF \u003e\u003e /etc/rc.local echo 1 \u003e /sys/module/vfio/parameters/enable_unsafe_noiommu_mode EOF chmod +x /etc/rc.local ### 可以不用做 END vi /lib/systemd/system/vpp.service After=syslog.target network.target auditd.service NetworkManager-wait-online.service ExecStartPre=-/usr/sbin/ip l s eth1 down ExecStartPre=-/usr/sbin/ip l s eth2 down ExecStartPre=-/sbin/modprobe vfio-pci ExecStartPre=-/bin/bash -c 'echo 1 \u003e /sys/module/vfio/parameters/enable_unsafe_noiommu_mode \u0026\u0026 sleep 2' ## 查看网卡信息 export PATH=$PATH:/opt/vpp/external/x86_64/bin/ /opt/vpp/external/x86_64/bin/dpdk-devbind.py -s vi /etc/vpp/startup.conf dpdk { uio-driver vfio-pci dev 0000:00:04.0 dev 0000:00:05.0 } plugins { path /usr/lib/vpp_plugins } systemctl daemon-reload \u0026\u0026 systemctl restart vpp \u0026\u0026 systemctl status vpp /opt/vpp/external/x86_64/bin/dpdk-devbind.py -s vppctl show ver vppctl show int vppctl show plugins top -H # CPU0 100% ",
    "description": "",
    "tags": null,
    "title": "0. vpp-rocky87-build",
    "uri": "/vpp/1.vpp-rocky87-build/index.html"
  },
  {
    "content": "date: 2023-02-04\nCreate a veth interface https://s3-docs.fd.io/vpp/22.10/gettingstarted/progressivevpp/index.html ip link add name ns1host type veth peer name ns1vpp ip netns add vns1 ip link set ns1host netns vns1 ip netns exec vns1 ifconfig ns1host 1.1.1.2/24 up ip netns exec vns1 route add -net 2.2.2.0/24 gw 1.1.1.1 ip link add name ns2host type veth peer name ns2vpp ip netns add vns2 ip link set ns2host netns vns2 ip netns exec vns2 ifconfig ns2host 2.2.2.2/24 up ip netns exec vns2 route add -net 1.1.1.0/24 gw 2.2.2.1 vppctl\ncreate host-interface name ns1vpp create host-interface name ns2vpp show interface set int state host-ns1vpp up set int state host-ns2vpp up set int ip address host-ns1vpp 1.1.1.1/24 set int ip address host-ns2vpp 2.2.2.1/24 show ip fib show int addr trace add af-packet-input 10 show trace clear trace ip netns exec vns1 ip a ip netns exec vns1 ping -c 1 2.2.2.2\nshow ip neighbors show ip fib show hardware-interfaces host-ns1vpp --\u003e 02:fe:f8:9f:d7:c3 host-ns2vpp --\u003e 02:fe:5b:e3:0b:b4 ip netns exec vns1 ip a ns1host 32:52:66:6e:76:44 ip netns exec vns2 ip a ns2host f6:b5:34:dc:25:c5 ICMP: 1.1.1.2 -\u003e 2.2.2.2\n32:52:66:6e:76:44 -\u003e 02:fe:f8:9f:d7:c3\n02:fe:5b:e3:0b:b4 -\u003e f6:b5:34:dc:25:c5\nhttps://wiki.fd.io/view/VPP/Configure_VPP_As_A_Router_Between_Namespaces\n# 1.Setup #!/bin/bash if [ $USER != \"root\" ] ; then echo \"Restarting script with sudo...\" sudo $0 ${*} exit fi # delete previous incarnations if they exist ip link del dev veth_vpp1 ip link del dev veth_vpp2 ip netns del vpp1 ip netns del vpp2 #create namespaces ip netns add vpp1 ip netns add vpp2 # create and configure 1st veth pair ip link add name veth_vpp1 type veth peer name vpp1 ip link set dev vpp1 up ip link set dev veth_vpp1 up netns vpp1 ip netns exec vpp1 \\ bash -c \" ip link set dev lo up ip addr add 172.16.1.2/24 dev veth_vpp1 ip route add 172.16.2.0/24 via 172.16.1.1 \" # create and configure 2st veth pair ip link add name veth_vpp2 type veth peer name vpp2 ip link set dev vpp2 up ip link set dev veth_vpp2 up netns vpp2 ip netns exec vpp2 \\ bash -c \" ip link set dev lo up ip addr add 172.16.2.2/24 dev veth_vpp2 ip route add 172.16.1.0/24 via 172.16.2.1 \" # 2.Configure Interfaces sudo vppctl create host-interface name vpp1 sudo vppctl create host-interface name vpp2 sudo vppctl set int state host-vpp1 up sudo vppctl set int state host-vpp2 up sudo vppctl set int ip address host-vpp1 172.16.1.1/24 sudo vppctl set int ip address host-vpp2 172.16.2.1/24 # 3.Test $ sudo ip netns exec vpp1 ping 172.16.2.1 -c 1 PING 172.16.2.2 (172.16.2.2) 56(84) bytes of data. 64 bytes from 172.16.2.2: icmp_seq=1 ttl=63 time=0.135 ms --- 172.16.2.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.135/0.135/0.135/0.000 ms vpp# show ip arp Time FIB IP4 Stat Ethernet Interface 1050.5729 0 172.16.1.2 5a:df:31:28:dc:5c host-vpp1 1050.5768 0 172.16.2.2 12:fa:19:cb:39:e3 host-vpp2 vpp# show interface vpp# show ip fib host1 : 10.1.5.151 host2 : 10.1.5.152 # 151 set interface state GigabitEthernet0/4/0 up set interface ip address GigabitEthernet0/4/0 1.1.1.1/24 # 152 set interface state GigabitEthernet0/4/0 up set interface ip address GigabitEthernet0/4/0 1.1.1.2/24 ping ok! memif https://s3-docs.fd.io/vpp/22.10/gettingstarted/progressivevpp/twovppinstances.html routing https://s3-docs.fd.io/vpp/22.10/gettingstarted/progressivevpp/routing.html switching https://s3-docs.fd.io/vpp/22.10/gettingstarted/progressivevpp/switching.html ",
    "description": "",
    "tags": null,
    "title": "2. VPP Tutorial",
    "uri": "/vpp/2.vpp-tutorial/index.html"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "amwork2010 blog",
    "uri": "/index.html"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "dpdk",
    "uri": "/categories/dpdk/index.html"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "DPDKs",
    "uri": "/dpdk/index.html"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "vpp",
    "uri": "/categories/vpp/index.html"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "VPPs",
    "uri": "/vpp/index.html"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tags",
    "uri": "/tags/index.html"
  }
]

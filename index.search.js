var relearn_search_index = [
  {
    "content": " blogs of amwork2010 日期: 2023-02-04\n",
    "description": "",
    "tags": null,
    "title": "DPDK",
    "uri": "/dpdk/index.html"
  },
  {
    "content": "blogs of amwork2010 日期: 2023-02-04\n",
    "description": "",
    "tags": null,
    "title": "VPP",
    "uri": "/vpp/index.html"
  },
  {
    "content": "blogs of amwork2010 日期: 2023-02-04\n",
    "description": "",
    "tags": null,
    "title": "Ubuntu",
    "uri": "/ubuntu/index.html"
  },
  {
    "content": "date: 2023-02-04\nVMWARE 加网卡 (192.168.68.56 8c 8G)\nhttps://www.dpdk.org/\nhttp://doc.dpdk.org/guides-22.11/\n编辑对应的 jammy.vmx，修改所有e1000为vmxnet3 ，多队列网卡 ethernet0.virtualDev = \"vmxnet3\" ethernet0.wakeOnPcktRcv = \"true\" ... ethernet1.virtualDev = \"vmxnet3\" ethernet1.wakeOnPcktRcv = \"true\" ethernet2.virtualDev = \"vmxnet3\" ethernet2.wakeOnPcktRcv = \"true\" lshw -c network -businfo root@jammy:~# ip a ... 2: ens192: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 00:0c:29:9f:04:74 brd ff:ff:ff:ff:ff:ff altname enp11s0 inet 192.168.68.56/24 brd 192.168.68.255 scope global ens192 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe9f:474/64 scope link valid_lft forever preferred_lft forever 3: ens224: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 00:0c:29:9f:04:7e brd ff:ff:ff:ff:ff:ff altname enp19s0 4: ens256: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 00:0c:29:9f:04:88 brd ff:ff:ff:ff:ff:ff altname enp27s0 apt install dpdk dpdk-dev -y vi /etc/default/grub GRUB_CMDLINE_LINUX=\"default_hugepagesz=1G hugepagesz=1G hugepages=4 iommu=pt intel_iommu=on\" #GRUB_CMDLINE_LINUX=\"default_hugepagesz=1G hugepagesz=1G hugepages=4 isolcpus=2-3 iommu=pt intel_iommu=on\" update-grub # grub2-mkconfig -o /boot/grub2/grub.cfg reboot ## dmesg | grep -e DMAR -e IOMMU cat /proc/cmdline | grep -e iommu=pt -e intel_iommu=on -e huge dmesg| grep -i iommu cat /proc/meminfo | grep Huge lscpu | grep NUMA dpdk-hugepages.py -s dpdk-devbind.py -s lshw -businfo -c network root@dpdk56:~# lshw -businfo -c network Bus info Device Class Description ==================================================== pci@0000:0b:00.0 ens192 network VMXNET3 Ethernet Controller pci@0000:13:00.0 ens224 network VMXNET3 Ethernet Controller pci@0000:1b:00.0 ens256 network VMXNET3 Ethernet Controller dmesg| grep -i iommu | grep -e 0000:0b:00.0 -e 0000:13:00.0 -e 0000:1b:00.0 root@dpdk56:~# dmesg| grep -i iommu | grep -e 0000:0b:00.0 -e 0000:13:00.0 -e 0000:1b:00.0 [ 2.723136] pci 0000:0b:00.0: Adding to iommu group 6 [ 2.723277] pci 0000:13:00.0: Adding to iommu group 7 [ 2.723432] pci 0000:1b:00.0: Adding to iommu group 8 root@jammy:~ # dpdk-devbind.py -s Network devices using kernel driver =================================== 0000:0b:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens192 drv=vmxnet3 unused=vfio-pci *Active* 0000:13:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens224 drv=vmxnet3 unused=vfio-pci 0000:1b:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens256 drv=vmxnet3 unused=vfio-pci dpdk-devbind.py -b vfio-pci 0000:13:00.0 0000:1b:00.0 # dpdk-devbind.py -b vfio-pci 0000:13:00.0 # dpdk-devbind.py -b vfio-pci 0000:1b:00.0 dpdk-devbind.py -s Network devices using DPDK-compatible driver ============================================ 0000:13:00.0 'VMXNET3 Ethernet Controller 07b0' drv=vfio-pci unused=vmxnet3 0000:1b:00.0 'VMXNET3 Ethernet Controller 07b0' drv=vfio-pci unused=vmxnet3 Network devices using kernel driver =================================== 0000:0b:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens192 drv=vmxnet3 unused=vfio-pci *Active* dpdk-hugepages.py -s root@jammy:~ # dpdk-hugepages.py -s Node Pages Size Total 0 4 1Gb 4Gb Hugepages mounted on /dev/hugepages ### build pkten export https_proxy=http://10.1.1.12:8118 wget https://github.com/pktgen/Pktgen-DPDK/archive/refs/tags/pktgen-22.07.1.tar.gz tar zxvf pktgen-22.07.1.tar.gz cd Pktgen-DPDK-pktgen-22.07.1 meson build cd build ninja 编译完毕后的pkten在[Pktgen dir]/build/app/pktgen ## 源码BUILD dpdk apt install -y build-essential ## pip3 install meson ninja apt install meson python3-pyelftools pkg-config libnuma-dev export http_proxy=http://10.1.1.12:8118 wget http://fast.dpdk.org/rel/dpdk-22.11.1.tar.xz tar Jxvf dpdk-22.11.1.tar.xz cd dpdk-stable-22.11.1 meson setup -Dexamples=all build cd build ninja # ninja install ### 因为 apt install dpdk dpdk-dev -y ./pktgen -l 1-3 -n 2 -- -T -P -m \"2.0,3.1\" -l: 使用CPU Cores 1、2、3 -n: 内存通道 --：此符号前是DPDK的配置参数，此符号后是DPDK Application的配置参数，此处即是Pktgen的参数 -T: 启用彩色文本输出 -P: Enable PROMISCUOUS mode on all ports -m string: 重点！指定cpu core与NIC的绑定关系，格式参照下图： VM1 (56) \u003c--------\u003e VM2 (57) root@dpdk56:~# ip a 2: ens192: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 00:0c:29:9f:04:74 brd ff:ff:ff:ff:ff:ff altname enp11s0 inet 192.168.68.56/24 brd 192.168.68.255 scope global ens192 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe9f:474/64 scope link valid_lft forever preferred_lft forever 3: ens224: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 00:0c:29:9f:04:7e brd ff:ff:ff:ff:ff:ff altname enp19s0 4: ens256: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 00:0c:29:9f:04:88 brd ff:ff:ff:ff:ff:ff altname enp27s0 root@dpdk57:~# ip a 2: ens192: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 00:0c:29:22:de:b7 brd ff:ff:ff:ff:ff:ff altname enp11s0 inet 192.168.68.57/24 brd 192.168.68.255 scope global ens192 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe22:deb7/64 scope link valid_lft forever preferred_lft forever 3: ens224: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 00:0c:29:22:de:c1 brd ff:ff:ff:ff:ff:ff altname enp19s0 4: ens256: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 00:0c:29:22:de:cb brd ff:ff:ff:ff:ff:ff altname enp27s0 dpdk-devbind.py -b vfio-pci 0000:13:00.0 dpdk-devbind.py -s 00:0c:29:9f:04:7e (vm1 ens224) --- 00:0c:29:22:de:c1 (vm2 ens224) vm1:\nroot@dpdk56:~# /root/Pktgen-DPDK-pktgen-22.07.1/build/app/pktgen -l 0-1 -n 3 -- -T -P -m \"1.0\" set 0 dst mac 00:0c:29:22:de:c1 set 0 count 100000 str vm2:\nroot@dpdk57:~# dpdk-testpmd -l 0-1 -n 1 -- -i set fwd rxonly show port stats all clear port stats all start vm1: vm2: dpdk-l2fwd vm2:\ndpdk-devbind.py -b vfio-pci 0000:13:00.0 dpdk-devbind.py -b vfio-pci 0000:1b:00.0 root@dpdk57:~/dpdk-stable-22.11.1/build/examples# ./dpdk-l2fwd -l 0-3 -n 4 -- -q 1 -p 0x3 vm1:\ndpdk-devbind.py -b vfio-pci 0000:13:00.0 root@dpdk56:~# /root/Pktgen-DPDK-pktgen-22.07.1/build/app/pktgen -l 0-1 -n 4 -- -T -P -m \"1.0\" set 0 dst mac 00:0c:29:22:de:c1 set 0 count 1000 start 0 vm2: vm1: ",
    "description": "",
    "tags": null,
    "title": "1. vmware-dpdk",
    "uri": "/dpdk/1.vmware-dpdk/index.html"
  },
  {
    "content": "date: 2023-02-04 HOST: 10.1.1.12, guest: j1210 10.1.5.110\nroot@junnan-gpu:/u01/vms/j1210# more vm.xml \u003cdomain type='kvm'\u003e \u003cname\u003ej1210\u003c/name\u003e \u003cvcpu current='8'\u003e24\u003c/vcpu\u003e \u003cmemory\u003e8388608\u003c/memory\u003e \u003cos\u003e \u003ctype arch='x86_64' machine='pc'\u003ehvm\u003c/type\u003e \u003cbootmenu enable='yes'/\u003e \u003c/os\u003e \u003cfeatures\u003e \u003cacpi/\u003e \u003capic/\u003e \u003cpae/\u003e \u003c/features\u003e \u003ccpu mode=\"host-passthrough\" check=\"none\" migratable=\"on\"/\u003e \u003cclock offset='utc'/\u003e \u003con_poweroff\u003edestroy\u003c/on_poweroff\u003e \u003con_reboot\u003erestart\u003c/on_reboot\u003e \u003con_crash\u003edestroy\u003c/on_crash\u003e \u003cdevices\u003e \u003cemulator\u003e/usr/bin/kvm\u003c/emulator\u003e \u003cdisk type='file' device='disk'\u003e \u003cdriver name='qemu' type='qcow2'/\u003e \u003csource file='/u01/vms/j1210/disk'/\u003e \u003ctarget dev='vda' bus='virtio'/\u003e \u003cboot order='1'/\u003e \u003c/disk\u003e \u003cinterface type='bridge'\u003e \u003csource bridge='br0'/\u003e \u003cmodel type='virtio'/\u003e \u003c/interface\u003e \u003cinterface type='network'\u003e \u003csource network='default'/\u003e \u003cmodel type='e1000'/\u003e \u003c/interface\u003e \u003cinterface type='network'\u003e \u003csource network='default'/\u003e \u003cmodel type='e1000'/\u003e \u003c/interface\u003e \u003cserial type='pty'\u003e \u003ctarget port='0'/\u003e \u003c/serial\u003e \u003cconsole type='pty'\u003e \u003ctarget type='serial' port='0'/\u003e \u003c/console\u003e \u003cgraphics type='vnc' port='-1' autoport='yes' listen='0.0.0.0'/\u003e \u003cvideo\u003e \u003cmodel type='cirrus' vram='65536' heads='1'/\u003e \u003c/video\u003e \u003cinput type='tablet' bus='usb'/\u003e \u003cinput type='mouse' bus='ps2'/\u003e \u003c/devices\u003e \u003c/domain\u003e \u003cinterface type='bridge'\u003e \u003csource bridge='br0'/\u003e \u003cmodel type='virtio'/\u003e \u003c/interface\u003e \u003cinterface type='network'\u003e \u003csource network='default'/\u003e \u003cmodel type='e1000'/\u003e \u003c/interface\u003e \u003cinterface type='network'\u003e \u003csource network='default'/\u003e \u003cmodel type='e1000'/\u003e \u003c/interface\u003e vi /etc/default/grub GRUB_CMDLINE_LINUX=\"default_hugepagesz=1G hugepagesz=1G hugepages=4\" #GRUB_CMDLINE_LINUX=\"default_hugepagesz=1G hugepagesz=1G hugepages=4 isolcpus=2-3 iommu=pt intel_iommu=on\" update-grub apt install dpdk dpdk-dev -y root@ubuntu:~# dpdk-devbind.py -s Network devices using kernel driver =================================== 0000:00:03.0 'Virtio network device 1000' if=ens3 drv=virtio-pci unused=vfio-pci *Active* 0000:00:04.0 '82540EM Gigabit Ethernet Controller 100e' if=ens4 drv=e1000 unused=vfio-pci 0000:00:05.0 '82540EM Gigabit Ethernet Controller 100e' if=ens5 drv=e1000 unused=vfio-pci root@ubuntu:~# dpdk-devbind.py -b vfio-pci 0000:00:04.0 Error: bind failed for 0000:00:04.0 - Cannot bind to driver vfio-pci: [Errno 22] Invalid argument Error: unbind failed for 0000:00:04.0 - Cannot open /sys/bus/pci/drivers//unbind: [Errno 13] Permission denied: '/sys/bus/pci/drivers//unbind' root@ubuntu:~# cat /sys/module/vfio/parameters/enable_unsafe_noiommu_mode N root@ubuntu:~# echo 1 \u003e /sys/module/vfio/parameters/enable_unsafe_noiommu_mode echo 1 \u003e /sys/module/vfio/parameters/enable_unsafe_noiommu_mode root@ubuntu:~# cat /sys/module/vfio/parameters/enable_unsafe_noiommu_mode Y root@ubuntu:~# dpdk-devbind.py -b vfio-pci 0000:00:04.0 root@ubuntu:~# dpdk-devbind.py -b vfio-pci 0000:00:05.0 root@ubuntu:~# dpdk-devbind.py -s Network devices using DPDK-compatible driver ============================================ 0000:00:04.0 '82540EM Gigabit Ethernet Controller 100e' drv=vfio-pci unused=e1000 0000:00:05.0 '82540EM Gigabit Ethernet Controller 100e' drv=vfio-pci unused=e1000 Network devices using kernel driver =================================== 0000:00:03.0 'Virtio network device 1000' if=ens3 drv=virtio-pci unused=vfio-pci *Active* root@ubuntu:~# dpdk-testpmd -l0-3 -- -i --nb-cores=2 --nb-ports=2 --total-num-mbufs=2048 ... start ... stop # top 会看到 2cpu 100%us 或者：\nroot@ubuntu:~# dpdk-devbind.py -s Network devices using kernel driver =================================== 0000:00:03.0 'Virtio network device 1000' if=ens3 drv=virtio-pci unused=vfio-pci *Active* 0000:00:04.0 '82540EM Gigabit Ethernet Controller 100e' if=ens4 drv=e1000 unused=vfio-pci 0000:00:05.0 '82540EM Gigabit Ethernet Controller 100e' if=ens5 drv=e1000 unused=vfio-pci apt install dpdk-kmods-dkms # 安装igb_uio modprobe igb_uio root@ubuntu:~# dpdk-devbind.py -s Network devices using kernel driver =================================== 0000:00:03.0 'Virtio network device 1000' if=ens3 drv=virtio-pci unused=igb_uio,vfio-pci *Active* 0000:00:04.0 '82540EM Gigabit Ethernet Controller 100e' if=ens4 drv=e1000 unused=igb_uio,vfio-pci 0000:00:05.0 '82540EM Gigabit Ethernet Controller 100e' if=ens5 drv=e1000 unused=igb_uio,vfio-pci root@ubuntu:~# dpdk-devbind.py -b igb_uio 0000:00:04.0 root@ubuntu:~# dpdk-devbind.py -b igb_uio 0000:00:05.0 root@ubuntu:~# dpdk-devbind.py -s Network devices using DPDK-compatible driver ============================================ 0000:00:04.0 '82540EM Gigabit Ethernet Controller 100e' drv=igb_uio unused=e1000,vfio-pci 0000:00:05.0 '82540EM Gigabit Ethernet Controller 100e' drv=igb_uio unused=e1000,vfio-pci Network devices using kernel driver =================================== 0000:00:03.0 'Virtio network device 1000' if=ens3 drv=virtio-pci unused=igb_uio,vfio-pci *Active* root@ubuntu:~# dpdk-testpmd -l0-3 -- -i --nb-cores=2 --nb-ports=2 --total-num-mbufs=2048 ... start ... stop # top 会看到 2cpu 100%us # build apt install build-essential pip3 install meson ninja apt install meson python3-pyelftools pkg-config libnuma-dev wget http://fast.dpdk.org/rel/dpdk-22.11.1.tar.xz tar Jxvf dpdk-22.11.1.tar.xz cd dpdk-stable-22.11.1 meson setup -Dexamples=all build cd build ninja ninja install root@junnan-gpu:~# ll /usr/bin/kvm lrwxrwxrwx 1 root root 18 12月 8 17:17 /usr/bin/kvm -\u003e qemu-system-x86_64* root@junnan-gpu:~# qemu-system-x86_64 --version QEMU emulator version 6.2.0 (Debian 1:6.2+dfsg-2ubuntu6.6) Copyright (c) 2003-2021 Fabrice Bellard and the QEMU Project developers root@junnan-gpu:~# qemu-system-x86_64 -device ? ... Network devices: name \"e1000\", bus PCI, alias \"e1000-82540em\", desc \"Intel Gigabit Ethernet\" name \"e1000-82544gc\", bus PCI, desc \"Intel Gigabit Ethernet\" name \"e1000-82545em\", bus PCI, desc \"Intel Gigabit Ethernet\" name \"e1000e\", bus PCI, desc \"Intel 82574L GbE Controller\" name \"i82550\", bus PCI, desc \"Intel i82550 Ethernet\" name \"i82551\", bus PCI, desc \"Intel i82551 Ethernet\" name \"i82557a\", bus PCI, desc \"Intel i82557A Ethernet\" name \"i82557b\", bus PCI, desc \"Intel i82557B Ethernet\" name \"i82557c\", bus PCI, desc \"Intel i82557C Ethernet\" name \"i82558a\", bus PCI, desc \"Intel i82558A Ethernet\" name \"i82558b\", bus PCI, desc \"Intel i82558B Ethernet\" name \"i82559a\", bus PCI, desc \"Intel i82559A Ethernet\" name \"i82559b\", bus PCI, desc \"Intel i82559B Ethernet\" name \"i82559c\", bus PCI, desc \"Intel i82559C Ethernet\" name \"i82559er\", bus PCI, desc \"Intel i82559ER Ethernet\" name \"i82562\", bus PCI, desc \"Intel i82562 Ethernet\" name \"i82801\", bus PCI, desc \"Intel i82801 Ethernet\" name \"ne2k_isa\", bus ISA name \"ne2k_pci\", bus PCI name \"pcnet\", bus PCI name \"pvrdma\", bus PCI, desc \"RDMA Device\" name \"rocker\", bus PCI, desc \"Rocker Switch\" name \"rtl8139\", bus PCI name \"tulip\", bus PCI name \"usb-net\", bus usb-bus name \"virtio-net-device\", bus virtio-bus name \"virtio-net-pci\", bus PCI, alias \"virtio-net\" name \"virtio-net-pci-non-transitional\", bus PCI name \"virtio-net-pci-transitional\", bus PCI name \"vmxnet3\", bus PCI, desc \"VMWare Paravirtualized Ethernet v3\" ... 所以可以修改 e1000 --\u003e vmxnet3 \u003cinterface type='bridge'\u003e \u003csource bridge='br0'/\u003e \u003cmodel type='vmxnet3'/\u003e \u003c/interface\u003e \u003cinterface type='bridge'\u003e \u003csource bridge='br0'/\u003e \u003cmodel type='vmxnet3'/\u003e \u003c/interface\u003e \u003cinterface type='bridge'\u003e \u003csource bridge='br0'/\u003e \u003cmodel type='vmxnet3'/\u003e \u003c/interface\u003e # u22.04 10.1.5.161 ######### 0.prepare apt update apt -y full-upgrade ln -sf ../usr/share/zoneinfo/Asia/Shanghai /etc/localtime [ -f /var/run/reboot-required ] \u0026\u0026 reboot -f ######### 1. 启用rc.local cat \u003c\u003c EOF \u003e\u003e /etc/rc.local #!/bin/bash echo 1 \u003e /sys/module/vfio/parameters/enable_unsafe_noiommu_mode EOF chmod +x /etc/rc.local cat \u003c\u003c EOF \u003e\u003e /lib/systemd/system/rc-local.service [Install] WantedBy=multi-user.target EOF cat /lib/systemd/system/rc-local.service # 启用服务 systemctl enable rc-local systemctl start rc-local systemctl status rc-local # 查看是否成功 cat /sys/module/vfio/parameters/enable_unsafe_noiommu_mode echo \"vfio-pci\" \u003e /etc/modules-load.d/95-vpp.conf ######### 2. hugepages cat \u003c\u003cEOF \u003e\u003e /etc/sysctl.conf vm.nr_hugepages = 2048 EOF sysctl -p ######### 3. dpdk apt install dpdk dpdk-dev -y dpdk-devbind.py -s dpdk-devbind.py -b vfio-pci 0000:00:04.0 dpdk-devbind.py -b vfio-pci 0000:00:05.0 root@ubuntu:~# dpdk-devbind.py -s Network devices using kernel driver =================================== 0000:00:03.0 'VMXNET3 Ethernet Controller 07b0' if=ens3 drv=vmxnet3 unused=vfio-pci *Active* 0000:00:04.0 'VMXNET3 Ethernet Controller 07b0' if=ens4 drv=vmxnet3 unused=vfio-pci 0000:00:05.0 'VMXNET3 Ethernet Controller 07b0' if=ens5 drv=vmxnet3 unused=vfio-pci root@ubuntu:~# dpdk-devbind.py -s Network devices using DPDK-compatible driver ============================================ 0000:00:04.0 'VMXNET3 Ethernet Controller 07b0' drv=vfio-pci unused=vmxnet3 0000:00:05.0 'VMXNET3 Ethernet Controller 07b0' drv=vfio-pci unused=vmxnet3 Network devices using kernel driver =================================== 0000:00:03.0 'VMXNET3 Ethernet Controller 07b0' if=ens3 drv=vmxnet3 unused=vfio-pci *Active* apt install docker.io -y docker pull ubuntu:22.04 cat \u003c\u003c EOF \u003e Dockerfile FROM ubuntu:22.04 RUN sed -i 's/archive.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.list \u0026\u0026 \\ sed -i 's/security.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.list RUN apt-get update -y \u0026\u0026 \\ apt-get install dpdk kmod -y EOF docker build -t amwork2010/dpdk:1 . docker run --privileged \\ -v /sys/bus/pci/devices:/sys/bus/pci/devices \\ -v /sys/kernel/mm/hugepages:/sys/kernel/mm/hugepages \\ -v /sys/devices/system/node:/sys/devices/system/node \\ -v /lib/modules:/lib/modules \\ -v /dev:/dev \\ -it amwork2010/dpdk:1 bash docker run --privileged \\ -v /lib/modules:/lib/modules \\ -it amwork2010/dpdk:1 bash ## 也可以banding , kmod ： the kmod package would provide modinfo, modprobe and other related tools. root@714c8d6fc89e:/# modprobe vfio-pci modprobe: FATAL: Module vfio-pci not found in directory /lib/modules/5.15.0-58-generic so need: -v /lib/modules:/lib/modules ",
    "description": "",
    "tags": null,
    "title": "2. qemu-dpdk",
    "uri": "/dpdk/2.qemu-dpdk/index.html"
  },
  {
    "content": "date: 2023-02-04 192.168.68.56、 192.168.68.57\napt install -y openvswitch-switch-dpdk update-alternatives --set ovs-vswitchd /usr/lib/openvswitch-switch-dpdk/ovs-vswitchd-dpdk ovs-vswitchd --version systemctl restart openvswitch-switch.service root@dpdk56:~# ovs-vswitchd --version ovs-vswitchd (Open vSwitch) 2.17.3 DPDK 21.11.2 root@dpdk56:~# dpdk-devbind.py -s Network devices using kernel driver =================================== 0000:0b:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens192 drv=vmxnet3 unused=vfio-pci *Active* 0000:13:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens224 drv=vmxnet3 unused=vfio-pci 0000:1b:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens256 drv=vmxnet3 unused=vfio-pci dpdk-devbind.py -b vfio-pci 0000:13:00.0 0000:1b:00.0 root@dpdk56:~# dpdk-devbind.py -b vfio-pci 0000:13:00.0 0000:1b:00.0 root@dpdk56:~# dpdk-devbind.py -s Network devices using DPDK-compatible driver ============================================ 0000:13:00.0 'VMXNET3 Ethernet Controller 07b0' drv=vfio-pci unused=vmxnet3 0000:1b:00.0 'VMXNET3 Ethernet Controller 07b0' drv=vfio-pci unused=vmxnet3 ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-init=true ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-socket-mem=\"1024,0\" ### 只有一个numa node0 ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-lcore-mask=0x2 ### 0b0010 --\u003e Cpu1 ovs-vsctl set Open_vSwitch . other_config:pmd-cpu-mask=0x4 ### 0b0100 --\u003e Cpu2 ovs-vsctl get Open_vSwitch . dpdk_initialized ovs-vsctl get Open_vSwitch . dpdk_version # dpdk-init 指定 OVS 是否应该初始化并支持 DPDK 端口。该字段可以是true或try。值true将导致 ovs-vswitchd 进程在初始化失败时中止。值try表示即使 EAL 初始化失败，ovs-vswitchd 进程也应该继续运行。 # dpdk-lcore-mask 指定应该生成 dpdk lcore 线程的 CPU 核心，并需要十六进制字符串（例如“0x123”）。 # dpdk-socket-mem 逗号分隔的内存列表，用于从特定套接字上的大页面中预分配。如果未指定，则默认情况下不会设置此选项。将使用 DPDK 默认值。 # dpdk-hugepage-dir hugetlbfs挂载目录 # vhost-sock-dir 设置虚拟主机用户 unix 套接字文件路径的选项。 root@dpdk56:~# ovs-vsctl list open_vswitch _uuid : 6b97fe90-77a5-4a61-a8b6-0b74ed9f803d bridges : [db9afd87-ad4d-4d1a-842a-3e2eb21abf9e] cur_cfg : 12 datapath_types : [netdev, system] datapaths : {} db_version : \"8.3.0\" dpdk_initialized : true dpdk_version : \"DPDK 21.11.2\" external_ids : {hostname=dpdk56, rundir=\"/var/run/openvswitch\", system-id=\"921fb59b-cbab-4609-929b-875d5dedb844\"} iface_types : [bareudp, dpdk, dpdkvhostuser, dpdkvhostuserclient, erspan, geneve, gre, gtpu, internal, ip6erspan, ip6gre, lisp, patch, stt, system, tap, vxlan] manager_options : [] next_cfg : 12 other_config : {dpdk-init=\"true\", dpdk-lcore-mask=\"0x2\", dpdk-socket-mem=\"1024,0\", pmd-cpu-mask=\"0x4\"} ovs_version : \"2.17.3\" ssl : [] statistics : {} system_type : ubuntu system_version : \"22.04\" ovs-vsctl add-br br0 -- set bridge br0 datapath_type=netdev ovs-vsctl add-port br0 dpdk-p0 -- set Interface dpdk-p0 type=dpdk options:dpdk-devargs=0000:13:00.0 ovs-vsctl add-port br0 dpdk-p1 -- set Interface dpdk-p1 type=dpdk options:dpdk-devargs=0000:1b:00.0 root@dpdk56:~# ovs-vsctl show 6b97fe90-77a5-4a61-a8b6-0b74ed9f803d Bridge br0 datapath_type: netdev Port dpdk-p1 Interface dpdk-p1 type: dpdk options: {dpdk-devargs=\"0000:1b:00.0\"} Port dpdk-p0 Interface dpdk-p0 type: dpdk options: {dpdk-devargs=\"0000:13:00.0\"} Port br0 Interface br0 type: internal ovs_version: \"2.17.3\" 借助 pmd 多线程支持，OVS 默认为每个 NUMA 节点创建一个 pmd 线程，前提是该 NUMA 节点至少有一个 DPDK 接口添加到 OVS。 但是，在有多个端口/rxq 产生流量的情况下，可以通过创建在不同内核上运行的多个 pmd 线程来提高性能。 这些 pmd 线程可以通过各自负责不同的端口/rxq 来分担工作量。将端口/rxq 分配给 pmd 线程是自动完成的。 掩码中的设置位意味着创建了一个 pmd 线程并将其固定到相应的 CPU 内核。例如，要在核心 1 和 2 上运行 pmd 线程：0x110 = 0x6 ovs-vsctl set Open_vSwitch . other_config:pmd-cpu-mask=0x6 在 DPDK 端口添加到交换机后，轮询线程不断轮询 DPDK 设备并消耗 100% 的核心，可以从top和 ps命令中检查： $ top -H $ ps -eLo pid,psr,comm | grep pmd # To stop ovs-vswitchd \u0026 delete bridge, run: $ ovs-appctl -t ovs-vswitchd exit $ ovs-appctl -t ovsdb-server exit $ ovs-vsctl del-br br0 # 在笔记本T480上，跑一个还勉强，跑2个基本夯住了，跑不动。 ",
    "description": "",
    "tags": null,
    "title": "3. ovs-dpdk vmware",
    "uri": "/dpdk/3.ovs-dpdk.vmware/index.html"
  },
  {
    "content": "date: 2023-02-04 HOST: 10.1.1.12 VM : ovs1:10.1.5.131 ovs2:10.1.5.132\n# ovs1:10.1.5.131 ovs2:10.1.5.132 apt update apt -y full-upgrade ln -sf ../usr/share/zoneinfo/Asia/Shanghai /etc/localtime [ -f /var/run/reboot-required ] \u0026\u0026 reboot -f ######### 1. 启用rc.local cat \u003c\u003c EOF \u003e\u003e /etc/rc.local #!/bin/bash echo 1 \u003e /sys/module/vfio/parameters/enable_unsafe_noiommu_mode EOF chmod +x /etc/rc.local cat \u003c\u003c EOF \u003e\u003e /lib/systemd/system/rc-local.service [Install] WantedBy=multi-user.target EOF cat /lib/systemd/system/rc-local.service # 启用服务 systemctl enable rc-local systemctl start rc-local systemctl status rc-local # 查看是否成功 cat /sys/module/vfio/parameters/enable_unsafe_noiommu_mode echo \"vfio-pci\" \u003e /etc/modules-load.d/95-vpp.conf ######### 2. hugepages cat \u003c\u003cEOF \u003e\u003e /etc/sysctl.conf vm.nr_hugepages = 1024 EOF sysctl -p ######### 3. check apt install driverctl -y cat /proc/meminfo | grep Huge lshw -businfo -c network driverctl list-devices ######### 4. ovs+dpdk apt install -y openvswitch-switch-dpdk update-alternatives --set ovs-vswitchd /usr/lib/openvswitch-switch-dpdk/ovs-vswitchd-dpdk ovs-vswitchd --version systemctl restart openvswitch-switch.service root@ovs1:~# ip a 2: ens3: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 52:54:00:3a:2e:71 brd ff:ff:ff:ff:ff:ff altname enp0s3 inet 10.1.5.131/21 brd 10.1.7.255 scope global ens3 3: ens4: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 52:54:00:c0:e1:75 brd ff:ff:ff:ff:ff:ff altname enp0s4 4: ens5: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 52:54:00:33:8e:0b brd ff:ff:ff:ff:ff:ff altname enp0s5 root@ovs2:~# ip a 2: ens3: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 52:54:00:fd:02:a5 brd ff:ff:ff:ff:ff:ff altname enp0s3 inet 10.1.5.132/21 brd 10.1.7.255 scope global ens3 3: ens4: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 52:54:00:9e:bc:de brd ff:ff:ff:ff:ff:ff altname enp0s4 4: ens5: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 52:54:00:16:ea:af brd ff:ff:ff:ff:ff:ff altname enp0s5 root@ovs1:~# dpdk-devbind.py -s 0000:00:03.0 'VMXNET3 Ethernet Controller 07b0' if=ens3 drv=vmxnet3 unused=vfio-pci *Active* 0000:00:04.0 'VMXNET3 Ethernet Controller 07b0' if=ens4 drv=vmxnet3 unused=vfio-pci 0000:00:05.0 'VMXNET3 Ethernet Controller 07b0' if=ens5 drv=vmxnet3 unused=vfio-pci dpdk-devbind.py -b vfio-pci 0000:00:04.0 root@ovs1:~# dpdk-devbind.py -s 0000:00:04.0 'VMXNET3 Ethernet Controller 07b0' drv=vfio-pci unused=vmxnet3 ######### 5. configure ovs dpdk ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-init=true ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-socket-mem=\"1024,0\" ### 只有一个numa node0 ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-lcore-mask=0x2 ### 0b0010 --\u003e Cpu1 ovs-vsctl set Open_vSwitch . other_config:pmd-cpu-mask=0x4 ### 0b0100 --\u003e Cpu2 ovs-vsctl get Open_vSwitch . dpdk_initialized ovs-vsctl get Open_vSwitch . dpdk_version ovs-vsctl list open_vswitch # 10.1.5.131 # hwaddr=52:54:00:c0:e1:75 == ens4 ovs-vsctl add-br br-phy -- set Bridge br-phy datapath_type=netdev -- br-set-external-id br-phy bridge-id br-phy -- set bridge br-phy fail-mode=standalone \\ other_config:hwaddr=52:54:00:c0:e1:75 ovs-vsctl show ovs-ofctl show br-phy ovs-vsctl add-port br-phy dpdk0 -- set Interface dpdk0 type=dpdk options:dpdk-devargs=0000:00:04.0 ip addr add 1.1.1.1/24 dev br-phy ip link set br-phy up iperf -s -i 1 # 10.1.5.132 ovs-vsctl add-br br-phy -- set Bridge br-phy datapath_type=netdev -- br-set-external-id br-phy bridge-id br-phy -- set bridge br-phy fail-mode=standalone \\ other_config:hwaddr=52:54:00:9e:bc:de ovs-vsctl show ovs-ofctl show br-phy ovs-vsctl add-port br-phy dpdk0 -- set Interface dpdk0 type=dpdk options:dpdk-devargs=0000:00:04.0 ip addr add 1.1.1.2/24 dev br-phy ip link set br-phy up iperf -t 10 -i 1 -c 1.1.1.1 #### iperf测试性能只有700-800M，而不用DPDK，iperf测试性能2G左右，why? root@ovs1:~# ovs-ofctl dump-ports br-phy dpdk0 OFPST_PORT reply (xid=0x4): 1 ports port dpdk0: rx pkts=532994, bytes=395231512, drop=0, errs=0, frame=?, over=?, crc=? tx pkts=136677, bytes=9034000, drop=0, errs=0, coll=? root@ovs1:~# root@ovs1:~# ovs-ofctl dump-ports br-phy dpdk0 OFPST_PORT reply (xid=0x4): 1 ports port dpdk0: rx pkts=781948, bytes=771677632, drop=0, errs=0, frame=?, over=?, crc=? tx pkts=228977, bytes=15129284, drop=0, errs=0, coll=? root@ovs1:~# root@ovs1:~# ovs-ofctl dump-ports br-phy dpdk0 OFPST_PORT reply (xid=0x4): 1 ports port dpdk0: rx pkts=953469, bytes=1031276094, drop=0, errs=0, frame=?, over=?, crc=? tx pkts=314671, bytes=20785064, drop=0, errs=0, coll=? root@ovs1:~# root@ovs1:~# ovs-ofctl dump-ports br-phy dpdk0 OFPST_PORT reply (xid=0x4): 1 ports port dpdk0: rx pkts=1058565, bytes=1190342002, drop=0, errs=0, frame=?, over=?, crc=? tx pkts=366688, bytes=24220318, drop=0, errs=0, coll=? root@ovs1:~# root@ovs1:~# ovs-ofctl dump-ports br-phy dpdk0 OFPST_PORT reply (xid=0x4): 1 ports port dpdk0: rx pkts=1088174, bytes=1235080612, drop=0, errs=0, frame=?, over=?, crc=? tx pkts=381556, bytes=25201606, drop=0, errs=0, coll=? ###### 有数据呀，说明从dpdk走呀，why? # 1 docker run --name ng1-1 --net=none -p 5001:5001 --privileged=true -d nginx:alpine1 ovs-docker add-port br-phy eth0 ng1-1 docker exec -it ng1-1 ip addr add 1.1.1.11/24 dev eth0 # 2 docker run --name ng2-1 --net=none -p 5001:5001 --privileged=true -d nginx:alpine1 ovs-docker add-port br-phy eth0 ng2-1 docker exec -it ng2-1 ip addr add 1.1.1.12/24 dev eth0 # ovs-docker del-port br0 eth0 ng2-1 可以ping通，但iperf不通 ovs-vsctl add-br br-int -- set Bridge br-int datapath_type=netdev -- br-set-external-id br-int bridge-id br-int -- set bridge br-int fail-mode=standalone # 1 ovs-vsctl add-port br-int vxlan0 -- set interface vxlan0 type=vxlan options:remote_ip=1.1.1.2 # 2 ovs-vsctl add-port br-int vxlan0 -- set interface vxlan0 type=vxlan options:remote_ip=1.1.1.1 # 1 docker run --name ng1-3 --net=none -p 5001:5001 --privileged=true -d nginx:alpine1 ovs-docker add-port br-int eth0 ng1-3 docker exec -it ng1-3 ip addr add 3.3.3.31/24 dev eth0 # 2 docker run --name ng2-3 --net=none --privileged=true -d nginx:alpine1 ovs-docker add-port br-int eth0 ng2-3 docker exec -it ng2-3 ip addr add 3.3.3.32/24 dev eth0 可以ping通，但iperf不通 ovs-vsctl add-br br1 ovs-vsctl add-port br1 ens5 # 1 ip l s ens5 up ifconfig br1 2.2.2.1/24 up # 2 ip l s ens5 up ifconfig br1 2.2.2.2/24 up # 1 docker run --name ng1-2 --net=none --privileged=true -d nginx:alpine1 ovs-docker add-port br1 eth0 ng1-2 docker exec -it ng1-2 ip addr add 2.2.2.11/24 dev eth0 # 2 docker run --name ng2-2 --net=none --privileged=true -d nginx:alpine1 ovs-docker add-port br1 eth0 ng2-2 docker exec -it ng2-2 ip addr add 2.2.2.12/24 dev eth0 可以ping通，iperf通，2G左右！ cat \u003c\u003c EOF \u003e Dockerfile FROM nginx:alpine RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories; \\\\ apk add --no-cache bash iperf tcpdump; \\\\ rm -rf /var/cache/apk/*; EOF docker build -t nginx:alpine1 . +--------------+ | vm0 | 3.3.3.31/24 +--------------+ (vm_port0) | | | +--------------+ | br-int | 3.3.3.32/24 +--------------+ +--------------+ | vxlan0 | | vxlan0 | +--------------+ +--------------+ | | | | | | 1.1.1.1/24 | +--------------+ | | br-phy | 1.1.1.2/24 +--------------+ +---------------+ | dpdk0/eth1 |----------------------------------| eth1 | +--------------+ +---------------+ Host A with OVS. Remote host. https://docs.openvswitch.org/en/latest/howto/userspace-tunneling/ https://github.com/bytedance/ovs-dpdk/blob/open-source/Documentation/howto/userspace-tunneling.rst https://community.arm.com/arm-community-blogs/b/tools-software-ides-blog/posts/open-vswitch-with-dpdk-on-arm-setup-for-phy-phy-test ",
    "description": "",
    "tags": null,
    "title": "4. ovs-dpdk qemu",
    "uri": "/dpdk/4.ovs-dpdk.qemu/index.html"
  },
  {
    "content": "date: 2023-02-04\ndpdk-devbind.py -b vfio-pci 0000:00:04.0 systemctl restart openvswitch-switch.service ovs-vsctl show cat \u003e\u003e /etc/libvirt/qemu.conf \u003c\u003c EOF user = \"root\" group = \"root\" EOF systemctl restart libvirtd.service # 131 ovs-vsctl add-port br-phy vhost-user1 -- set Interface vhost-user1 type=dpdkvhostuser ovs-vsctl add-port br-phy vhost-user2 -- set Interface vhost-user2 type=dpdkvhostuser qemu-system-x86_64 -m 1024 -smp 2 -cpu host -hda /root/vms/alpine1/alpine1-virt-3.16.1-x86_64.qcow2 -boot c -enable-kvm -no-reboot -net none -nographic \\ -chardev socket,id=char1,path=/run/openvswitch/vhost-user1 \\ -netdev type=vhost-user,id=mynet1,chardev=char1,vhostforce=on \\ -device virtio-net-pci,mac=00:00:00:00:00:01,netdev=mynet1 \\ -object memory-backend-file,id=mem,size=1G,mem-path=/dev/hugepages,share=on \\ -numa node,memdev=mem -mem-prealloc \\ -vnc :01 qemu-system-x86_64 -m 1024 -smp 2 -cpu host -hda /root/vms/alpine2/alpine2-virt-3.16.1-x86_64.qcow2 -boot c -enable-kvm -no-reboot -net none -nographic \\ -chardev socket,id=char2,path=/run/openvswitch/vhost-user2 \\ -netdev type=vhost-user,id=mynet2,chardev=char2,vhostforce=on \\ -device virtio-net-pci,mac=00:00:00:00:00:02,netdev=mynet2 \\ -object memory-backend-file,id=mem,size=1G,mem-path=/dev/hugepages,share=on \\ -numa node,memdev=mem -mem-prealloc \\ -vnc :02 # 131 vm1 login: ip a flush eth0 ip addr add 3.3.3.31/24 dev eth0 ip link set eth0 up iperf3 -s -i 1 # 131 vm2 login: ip a flush eth0 ip addr add 3.3.3.32/24 dev eth0 ip link set eth0 up ping -c 5 3.3.3.31 # ping ---\u003e OK iperf3 -t 10 -i 1 -c 3.3.3.31 alpine:~# iperf3 -t 10 -i 1 -c 3.3.3.31 Connecting to host 3.3.3.31, port 5201 [ 5] local 3.3.3.32 port 58624 connected to 3.3.3.31 port 5201 [ ID] Interval Transfer Bitrate Retr Cwnd [ 5] 0.00-1.00 sec 604 MBytes 5.07 Gbits/sec 289 386 KBytes [ 5] 1.00-2.00 sec 768 MBytes 6.44 Gbits/sec 31 354 KBytes [ 5] 2.00-3.00 sec 811 MBytes 6.80 Gbits/sec 26 359 KBytes [ 5] 3.00-4.00 sec 774 MBytes 6.49 Gbits/sec 76 358 KBytes [ 5] 4.00-5.00 sec 808 MBytes 6.78 Gbits/sec 5 380 KBytes [ 5] 5.00-6.00 sec 805 MBytes 6.75 Gbits/sec 44 290 KBytes [ 5] 6.00-7.00 sec 810 MBytes 6.80 Gbits/sec 20 438 KBytes [ 5] 7.00-8.00 sec 814 MBytes 6.83 Gbits/sec 20 389 KBytes [ 5] 8.00-9.00 sec 791 MBytes 6.64 Gbits/sec 19 361 KBytes [ 5] 9.00-10.00 sec 803 MBytes 6.74 Gbits/sec 13 334 KBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate Retr [ 5] 0.00-10.00 sec 7.61 GBytes 6.53 Gbits/sec 543 sender [ 5] 0.00-10.01 sec 7.61 GBytes 6.53 Gbits/sec receiver ## 结论：同一台主机内vm 6G # 132 ovs-vsctl add-port br-phy vhost-user3 -- set Interface vhost-user3 type=dpdkvhostuser qemu-system-x86_64 -m 1024 -smp 2 -cpu host -hda /root/alpine-virt-3.16.1-x86_64.qcow2 -boot c -enable-kvm -no-reboot -net none -nographic \\ -chardev socket,id=char3,path=/run/openvswitch/vhost-user3 \\ -netdev type=vhost-user,id=mynet3,chardev=char3,vhostforce=on \\ -device virtio-net-pci,mac=00:00:00:00:00:03,netdev=mynet3 \\ -object memory-backend-file,id=mem,size=1G,mem-path=/dev/hugepages,share=on \\ -numa node,memdev=mem -mem-prealloc \\ -vnc :03 # 132 vm1 login: ip a flush eth0 ip addr add 3.3.3.33/24 dev eth0 ip link set eth0 up ping -c 5 3.3.3.31 # ping ---\u003e OK iperf3 -t 10 -i 1 -c 3.3.3.31 alpine:~# iperf3 -t 10 -i 1 -c 3.3.3.31 Connecting to host 3.3.3.31, port 5201 [ 5] local 3.3.3.33 port 40190 connected to 3.3.3.31 port 5201 [ ID] Interval Transfer Bitrate Retr Cwnd [ 5] 0.00-1.00 sec 151 MBytes 1.26 Gbits/sec 0 508 KBytes [ 5] 1.00-2.00 sec 153 MBytes 1.29 Gbits/sec 0 592 KBytes [ 5] 2.00-3.00 sec 154 MBytes 1.29 Gbits/sec 0 625 KBytes [ 5] 3.00-4.00 sec 144 MBytes 1.21 Gbits/sec 0 656 KBytes [ 5] 4.00-5.00 sec 144 MBytes 1.21 Gbits/sec 0 687 KBytes [ 5] 5.00-6.00 sec 140 MBytes 1.17 Gbits/sec 0 735 KBytes [ 5] 6.00-7.00 sec 142 MBytes 1.19 Gbits/sec 0 772 KBytes [ 5] 7.00-8.00 sec 139 MBytes 1.16 Gbits/sec 310 639 KBytes [ 5] 8.00-9.00 sec 143 MBytes 1.20 Gbits/sec 0 735 KBytes [ 5] 9.00-10.00 sec 144 MBytes 1.21 Gbits/sec 111 543 KBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate Retr [ 5] 0.00-10.00 sec 1.42 GBytes 1.22 Gbits/sec 421 sender [ 5] 0.00-10.01 sec 1.42 GBytes 1.22 Gbits/sec receiver ## 结论：不同主机之间 vm 1.2G # 131 vm1 alpine:~# ip a 1: lo: \u003cLOOPBACK,UP,LOWER_UP\u003e mtu 65536 qdisc noqueue state UNKNOWN qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc pfifo_fast state UP qlen 1000 link/ether 00:00:00:00:00:01 brd ff:ff:ff:ff:ff:ff inet 3.3.3.31/24 scope global eth0 valid_lft forever preferred_lft forever alpine:~# iperf3 -s -i 1 ----------------------------------------------------------- Server listening on 5201 (test #1) ----------------------------------------------------------- Accepted connection from 3.3.3.32, port 36032 [ 5] local 3.3.3.31 port 5201 connected to 3.3.3.32 port 36034 [ ID] Interval Transfer Bitrate [ 5] 0.00-1.00 sec 480 MBytes 4.03 Gbits/sec [ 5] 1.00-2.00 sec 514 MBytes 4.31 Gbits/sec [ 5] 2.00-3.00 sec 533 MBytes 4.47 Gbits/sec [ 5] 3.00-4.00 sec 520 MBytes 4.36 Gbits/sec [ 5] 4.00-5.00 sec 517 MBytes 4.34 Gbits/sec [ 5] 5.00-6.00 sec 533 MBytes 4.47 Gbits/sec [ 5] 6.00-7.00 sec 538 MBytes 4.51 Gbits/sec [ 5] 7.00-8.00 sec 531 MBytes 4.45 Gbits/sec [ 5] 8.00-9.00 sec 542 MBytes 4.55 Gbits/sec [ 5] 9.00-10.00 sec 553 MBytes 4.64 Gbits/sec - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate [ 5] 0.00-10.00 sec 5.14 GBytes 4.41 Gbits/sec receiver ----------------------------------------------------------- Server listening on 5201 (test #2) ----------------------------------------------------------- Accepted connection from 3.3.3.33, port 50596 [ 5] local 3.3.3.31 port 5201 connected to 3.3.3.33 port 50610 [ ID] Interval Transfer Bitrate [ 5] 0.00-1.00 sec 143 MBytes 1.20 Gbits/sec [ 5] 1.00-2.00 sec 151 MBytes 1.27 Gbits/sec [ 5] 2.00-3.00 sec 154 MBytes 1.29 Gbits/sec [ 5] 3.00-4.00 sec 155 MBytes 1.30 Gbits/sec [ 5] 4.00-5.00 sec 156 MBytes 1.31 Gbits/sec [ 5] 5.00-6.00 sec 151 MBytes 1.27 Gbits/sec [ 5] 6.00-7.00 sec 154 MBytes 1.29 Gbits/sec [ 5] 7.00-8.00 sec 155 MBytes 1.30 Gbits/sec [ 5] 8.00-9.00 sec 156 MBytes 1.30 Gbits/sec [ 5] 9.00-10.00 sec 154 MBytes 1.29 Gbits/sec - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate [ 5] 0.00-10.01 sec 1.49 GBytes 1.28 Gbits/sec receiver ----------------------------------------------------------- Server listening on 5201 (test #3) ----------------------------------------------------------- ### 131 ovs-vsctl add-br br0 ovs-vsctl add-port br0 ens5 qemu-system-x86_64 -m 1024 -net nic,macaddr=00:00:00:00:01:01 -net tap -drive file=/root/alpine-virt-3.16.1-x86_64.qcow2 -boot c -enable-kvm -no-reboot -nographic ovs-vsctl add-port br0 tap0 ip a flush eth0 ip addr add 2.2.2.21/24 dev eth0 ip link set eth0 up iperf3 -s -i 1 qemu-system-x86_64 -m 1024 -net nic,macaddr=00:00:00:00:01:02 -net tap -drive file=/root/alpine2-virt-3.16.1-x86_64.qcow2 -boot c -enable-kvm -no-reboot -nographic ovs-vsctl add-port br0 tap1 ip a flush eth0 ip addr add 2.2.2.22/24 dev eth0 ip link set eth0 up iperf3 -t 10 -i 1 -c 2.2.2.21 alpine:~# iperf3 -t 10 -i 1 -c 2.2.2.21 Connecting to host 2.2.2.21, port 5201 [ 5] local 2.2.2.22 port 34292 connected to 2.2.2.21 port 5201 [ ID] Interval Transfer Bitrate Retr Cwnd [ 5] 0.00-1.00 sec 91.9 MBytes 770 Mbits/sec 0 1.33 MBytes [ 5] 1.00-2.01 sec 73.8 MBytes 612 Mbits/sec 0 1.57 MBytes [ 5] 2.01-3.00 sec 82.5 MBytes 699 Mbits/sec 0 1.99 MBytes [ 5] 3.00-4.02 sec 62.5 MBytes 516 Mbits/sec 0 1.99 MBytes [ 5] 4.02-5.00 sec 85.0 MBytes 723 Mbits/sec 0 2.20 MBytes [ 5] 5.00-6.01 sec 72.5 MBytes 605 Mbits/sec 0 2.20 MBytes [ 5] 6.01-7.01 sec 71.2 MBytes 595 Mbits/sec 0 2.20 MBytes [ 5] 7.01-8.00 sec 144 MBytes 1.22 Gbits/sec 265 1.43 MBytes [ 5] 8.00-9.00 sec 149 MBytes 1.25 Gbits/sec 0 1.52 MBytes [ 5] 9.00-10.00 sec 150 MBytes 1.26 Gbits/sec 0 1.59 MBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate Retr [ 5] 0.00-10.00 sec 982 MBytes 824 Mbits/sec 265 sender [ 5] 0.00-10.02 sec 979 MBytes 820 Mbits/sec receiver ## 结论：同一台主机内vm 1G ip l s ens5 up ip l s br0 up ip addr add 2.2.2.1/24 dev br0 ### 132 qemu-system-x86_64 -m 1024 -net nic,macaddr=00:00:00:00:01:03 -net tap -drive file=/root/alpine-virt-3.16.1-x86_64.qcow2 -boot c -enable-kvm -no-reboot -nographic ovs-vsctl add-port br0 tap0 ip a flush eth0 ip addr add 2.2.2.23/24 dev eth0 ip link set eth0 up ping 2.2.2.21 不通 ### ens5 br0 up ip l s ens5 up ip l s br0 up ip addr add 2.2.2.2/24 dev br0 alpine:~# iperf3 -t 10 -i 1 -c 2.2.2.21 Connecting to host 2.2.2.21, port 5201 [ 5] local 2.2.2.23 port 40028 connected to 2.2.2.21 port 5201 [ ID] Interval Transfer Bitrate Retr Cwnd [ 5] 0.00-1.04 sec 23.8 MBytes 191 Mbits/sec 0 1.08 MBytes [ 5] 1.04-2.01 sec 22.5 MBytes 195 Mbits/sec 0 1.08 MBytes [ 5] 2.01-3.01 sec 22.5 MBytes 189 Mbits/sec 0 1.08 MBytes [ 5] 3.01-4.00 sec 22.5 MBytes 191 Mbits/sec 0 1.08 MBytes [ 5] 4.00-5.01 sec 22.5 MBytes 188 Mbits/sec 0 1.08 MBytes [ 5] 5.01-6.01 sec 23.8 MBytes 199 Mbits/sec 0 1.08 MBytes [ 5] 6.01-7.04 sec 25.0 MBytes 203 Mbits/sec 0 1.08 MBytes [ 5] 7.04-8.04 sec 22.5 MBytes 188 Mbits/sec 0 1.08 MBytes [ 5] 8.04-9.04 sec 23.1 MBytes 195 Mbits/sec 0 1.13 MBytes [ 5] 9.04-10.05 sec 22.5 MBytes 187 Mbits/sec 0 1.13 MBytes - - - - - - - - - - - - - - - - - - - - - - - - - [ ID] Interval Transfer Bitrate Retr [ 5] 0.00-10.05 sec 231 MBytes 193 Mbits/sec 0 sender [ 5] 0.00-10.05 sec 231 MBytes 192 Mbits/sec receiver ## 结论：不同主机之间 vm 190M root@ovs1:~/vms/alpine1# cat vm.xml \u003cdomain type='kvm'\u003e \u003cname\u003ealpine1\u003c/name\u003e \u003cvcpu placement='static'\u003e2\u003c/vcpu\u003e \u003cmemory\u003e1048576\u003c/memory\u003e \u003cmemoryBacking\u003e \u003chugepages\u003e \u003cpage size='2048' unit='KiB'/\u003e \u003c/hugepages\u003e \u003c/memoryBacking\u003e \u003cos\u003e \u003ctype arch='x86_64' machine='pc'\u003ehvm\u003c/type\u003e \u003cbootmenu enable='yes'/\u003e \u003c/os\u003e \u003cfeatures\u003e \u003cacpi/\u003e \u003capic/\u003e \u003cpae/\u003e \u003c/features\u003e \u003ccpu mode=\"host-passthrough\"\u003e \u003cnuma\u003e \u003ccell id='0' cpus='0-1' memory='1048576' unit='KiB' memAccess='shared'/\u003e \u003c/numa\u003e \u003c/cpu\u003e \u003cclock offset='utc'/\u003e \u003con_poweroff\u003edestroy\u003c/on_poweroff\u003e \u003con_reboot\u003erestart\u003c/on_reboot\u003e \u003con_crash\u003edestroy\u003c/on_crash\u003e \u003cdevices\u003e \u003cemulator\u003e/usr/bin/kvm\u003c/emulator\u003e \u003cdisk type='file' device='disk'\u003e \u003cdriver name='qemu' type='qcow2'/\u003e \u003csource file='/root/vms/alpine1/alpine1-virt-3.16.1-x86_64.qcow2'/\u003e \u003ctarget dev='vda' bus='virtio'/\u003e \u003cboot order='1'/\u003e \u003c/disk\u003e \u003cinterface type='vhostuser'\u003e \u003cmac address='00:00:00:00:00:01'/\u003e \u003csource type='unix' path='/run/openvswitch/vhost-user1' mode='client'/\u003e \u003ctarget dev='vnet1'/\u003e \u003cmodel type='virtio'/\u003e \u003cdriver queues='2'\u003e \u003chost mrg_rxbuf='off'/\u003e \u003c/driver\u003e \u003c/interface\u003e \u003cserial type='pty'\u003e \u003ctarget port='0'/\u003e \u003c/serial\u003e \u003cconsole type='pty'\u003e \u003ctarget type='serial' port='0'/\u003e \u003c/console\u003e \u003cgraphics type='vnc' port='-1' autoport='yes' listen='0.0.0.0'/\u003e \u003cvideo\u003e \u003cmodel type='cirrus' vram='65536' heads='1'/\u003e \u003c/video\u003e \u003cinput type='tablet' bus='usb'/\u003e \u003cinput type='mouse' bus='ps2'/\u003e \u003c/devices\u003e \u003c/domain\u003e root@ovs1:~/vms/alpine1# cat /var/log/libvirt/qemu/alpine1.log 2023-01-12 22:27:56.091+0000: starting up libvirt version: 8.0.0, package: 1ubuntu7.4 (Christian Ehrhardt \u003cchristian.ehrhardt@canonical.com\u003e Tue, 22 Nov 2022 15:59:28 +0100), qemu version: 6.2.0Debian 1:6.2+dfsg-2ubuntu6.6, kernel: 5.15.0-57-generic, hostname: ovs1 LC_ALL=C \\ PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin \\ HOME=/var/lib/libvirt/qemu/domain-7-alpine1 \\ XDG_DATA_HOME=/var/lib/libvirt/qemu/domain-7-alpine1/.local/share \\ XDG_CACHE_HOME=/var/lib/libvirt/qemu/domain-7-alpine1/.cache \\ XDG_CONFIG_HOME=/var/lib/libvirt/qemu/domain-7-alpine1/.config \\ /usr/bin/kvm \\ -name guest=alpine1,debug-threads=on \\ -S \\ -object '{\"qom-type\":\"secret\",\"id\":\"masterKey0\",\"format\":\"raw\",\"file\":\"/var/lib/libvirt/qemu/domain-7-alpine1/master-key.aes\"}' \\ -machine pc-i440fx-6.2,usb=off,dump-guest-core=off \\ -accel kvm \\ -cpu host,migratable=on \\ -m 1024 \\ -overcommit mem-lock=off \\ -smp 2,sockets=2,cores=1,threads=1 \\ -object '{\"qom-type\":\"memory-backend-file\",\"id\":\"ram-node0\",\"mem-path\":\"/dev/hugepages/libvirt/qemu/7-alpine1\",\"share\":true,\"prealloc\":true,\"size\":1073741824}' \\ -numa node,nodeid=0,cpus=0-1,memdev=ram-node0 \\ -uuid fd61bba1-7b26-4f1d-a612-5fca2a6358e6 \\ -no-user-config \\ -nodefaults \\ -chardev socket,id=charmonitor,fd=32,server=on,wait=off \\ -mon chardev=charmonitor,id=monitor,mode=control \\ -rtc base=utc \\ -no-shutdown \\ -boot menu=on,strict=on \\ -device piix3-usb-uhci,id=usb,bus=pci.0,addr=0x1.0x2 \\ -blockdev '{\"driver\":\"file\",\"filename\":\"/root/vms/alpine1/alpine1-virt-3.16.1-x86_64.qcow2\",\"node-name\":\"libvirt-1-storage\",\"auto-read-only\":true,\"discard\":\"unmap\"}' \\ -blockdev '{\"node-name\":\"libvirt-1-format\",\"read-only\":false,\"driver\":\"qcow2\",\"file\":\"libvirt-1-storage\",\"backing\":null}' \\ -device virtio-blk-pci,bus=pci.0,addr=0x4,drive=libvirt-1-format,id=virtio-disk0,bootindex=1 \\ -chardev socket,id=charnet0,path=/run/openvswitch/vhost-user1 \\ -netdev vhost-user,chardev=charnet0,queues=2,id=hostnet0 \\ -device virtio-net-pci,mrg_rxbuf=off,mq=on,vectors=6,netdev=hostnet0,id=net0,mac=00:00:00:00:00:01,bus=pci.0,addr=0x3 \\ -chardev pty,id=charserial0 \\ -device isa-serial,chardev=charserial0,id=serial0 \\ -device usb-tablet,id=input0,bus=usb.0,port=1 \\ -audiodev '{\"id\":\"audio1\",\"driver\":\"none\"}' \\ -vnc 0.0.0.0:0,audiodev=audio1 \\ -device cirrus-vga,id=video0,bus=pci.0,addr=0x2 \\ -device virtio-balloon-pci,id=balloon0,bus=pci.0,addr=0x5 \\ -sandbox on,obsolete=deny,elevateprivileges=deny,spawn=deny,resourcecontrol=deny \\ -msg timestamp=on char device redirected to /dev/pts/1 (label charserial0) 2023-01-12T22:50:38.845268Z kvm: terminating on signal 15 from pid 1032 (/usr/sbin/libvirtd) 2023-01-12 22:50:39.047+0000: shutting down, reason=destroyed -chardev socket,id=charnet0,path=/run/openvswitch/vhost-user1 \\ -netdev vhost-user,chardev=charnet0,queues=2,id=hostnet0 \\ -device virtio-net-pci,mrg_rxbuf=off,mq=on,vectors=6,netdev=hostnet0,id=net0,mac=00:00:00:00:00:01,bus=pci.0,addr=0x3 \\ ### xml 生成的 id 都是一样的，无法修改，所以vhost-user模式下，启动2台虚拟机会冲突，用qemu-system-x86_64 指定不同的id等启动才行。 id=charnet0 netdev=hostnet0 ",
    "description": "",
    "tags": null,
    "title": "5. vhostuser ovs-dpdk",
    "uri": "/dpdk/5.vhostuser.ovs-dpdk/index.html"
  },
  {
    "content": "date: 2023-02-04 https://libvirt.org/formatdomain.html https://docs.openvswitch.org/en/latest/topics/dpdk/vhost-user/\n# Open vSwitch provides two types of vHost User ports: . vhost-user (dpdkvhostuser) ---\u003e deprecated! . vhost-user-client (dpdkvhostuserclient) # Ports of type vhost-user are currently deprecated and will be removed in a future release. # https://libvirt.org/formatdomain.html # 133 hostnamectl set-hostname ovs3 ip a 2: ens3: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 52:54:00:55:b6:f4 brd ff:ff:ff:ff:ff:ff altname enp0s3 inet 10.1.5.133/21 brd 10.1.7.255 scope global ens3 3: ens4: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 52:54:00:56:79:6c brd ff:ff:ff:ff:ff:ff altname enp0s4 4: ens5: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 52:54:00:f7:61:4d brd ff:ff:ff:ff:ff:ff altname enp0s5 # 134 hostnamectl set-hostname ovs4 ip a 2: ens3: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 52:54:00:9b:e5:3d brd ff:ff:ff:ff:ff:ff altname enp0s3 inet 10.1.5.134/21 brd 10.1.7.255 scope global ens3 3: ens4: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 52:54:00:5a:1e:4b brd ff:ff:ff:ff:ff:ff altname enp0s4 4: ens5: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 52:54:00:54:30:07 brd ff:ff:ff:ff:ff:ff altname enp0s5 cat \u003c\u003cEOF \u003e\u003e /etc/sysctl.conf vm.nr_hugepages = 2048 EOF sysctl -p dpdk-devbind.py -s Network devices using kernel driver =================================== 0000:00:03.0 'VMXNET3 Ethernet Controller 07b0' if=ens3 drv=vmxnet3 unused=vfio-pci *Active* 0000:00:04.0 'VMXNET3 Ethernet Controller 07b0' if=ens4 drv=vmxnet3 unused=vfio-pci 0000:00:05.0 'VMXNET3 Ethernet Controller 07b0' if=ens5 drv=vmxnet3 unused=vfio-pci dpdk-devbind.py -b vfio-pci 0000:00:04.0 ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-init=true ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-socket-mem=\"1024,0\" ### 只有一个numa node0 ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-lcore-mask=0x2 ### 0b0010 --\u003e Cpu1 ovs-vsctl set Open_vSwitch . other_config:pmd-cpu-mask=0x4 ### 0b0100 --\u003e Cpu2 ovs-vsctl get Open_vSwitch . dpdk_initialized ovs-vsctl get Open_vSwitch . dpdk_version ovs-vsctl list open_vswitch # 10.1.5.133 ovs-vsctl add-br br-phy -- set Bridge br-phy datapath_type=netdev -- br-set-external-id br-phy bridge-id br-phy -- set bridge br-phy fail-mode=standalone \\ other_config:hwaddr=52:54:00:56:79:6c ovs-vsctl show ovs-ofctl show br-phy ovs-vsctl add-port br-phy dpdk0 -- set Interface dpdk0 type=dpdk options:dpdk-devargs=0000:00:04.0 ovs-vsctl show # 10.1.5.134 ovs-vsctl add-br br-phy -- set Bridge br-phy datapath_type=netdev -- br-set-external-id br-phy bridge-id br-phy -- set bridge br-phy fail-mode=standalone \\ other_config:hwaddr=52:54:00:5a:1e:4b ovs-vsctl show ovs-ofctl show br-phy ovs-vsctl add-port br-phy dpdk0 -- set Interface dpdk0 type=dpdk options:dpdk-devargs=0000:00:04.0 ovs-vsctl show # both apt update \u0026\u0026 apt install -y qemu-kvm virt-manager libvirt-daemon-system virtinst libvirt-clients qemu-utils bridge-utils iperf tcpdump uml-utilities cat \u003e\u003e /etc/libvirt/qemu.conf \u003c\u003c EOF user = \"root\" group = \"root\" EOF systemctl restart libvirtd.service # 133 ovs-vsctl add-port br-phy dpdkvhostclient0 -- set Interface dpdkvhostclient0 type=dpdkvhostuserclient options:vhost-server-path=/run/openvswitch/dpdkvhostclient0 ovs-vsctl add-port br-phy dpdkvhostclient1 -- set Interface dpdkvhostclient1 type=dpdkvhostuserclient options:vhost-server-path=/run/openvswitch/dpdkvhostclient1 qemu-system-x86_64 -m 1024 -smp 2 -cpu host -hda /root/vms/alpine1/alpine1-virt-3.16.1-x86_64.qcow2 -boot c -enable-kvm -no-reboot -net none -nographic \\ -chardev socket,id=char1,path=/run/openvswitch/dpdkvhostclient0,server=on \\ -netdev type=vhost-user,id=mynet1,chardev=char1,vhostforce=on \\ -device virtio-net-pci,mac=00:00:00:00:33:01,netdev=mynet1 \\ -object memory-backend-file,id=mem,size=1G,mem-path=/dev/hugepages,share=on \\ -numa node,memdev=mem -mem-prealloc \\ -vnc :01 qemu-system-x86_64 -m 1024 -smp 2 -cpu host -hda /root/vms/alpine2/alpine2-virt-3.16.1-x86_64.qcow2 -boot c -enable-kvm -no-reboot -net none -nographic \\ -chardev socket,id=char2,path=/run/openvswitch/dpdkvhostclient1,server=on \\ -netdev type=vhost-user,id=mynet2,chardev=char2,vhostforce=on \\ -device virtio-net-pci,mac=00:00:00:00:33:02,netdev=mynet2 \\ -object memory-backend-file,id=mem,size=1G,mem-path=/dev/hugepages,share=on \\ -numa node,memdev=mem -mem-prealloc \\ -vnc :02 # 133 vm1 login: ip a flush eth0 ip addr add 3.3.3.31/24 dev eth0 ip link set eth0 up iperf3 -s -i 1 # 133 vm2 login: ip a flush eth0 ip addr add 3.3.3.32/24 dev eth0 ip link set eth0 up ping -c 5 3.3.3.31 # ping ---\u003e OK iperf3 -t 10 -i 1 -c 3.3.3.31 ## 结论：同一台主机内vm 6G # 134 ovs-vsctl add-port br-phy dpdkvhostclient3 -- set Interface dpdkvhostclient3 type=dpdkvhostuserclient options:vhost-server-path=/run/openvswitch/dpdkvhostclient3 qemu-system-x86_64 -m 1024 -smp 2 -cpu host -hda /root/vms/alpine3/alpine3-virt-3.16.1-x86_64.qcow2 -boot c -enable-kvm -no-reboot -net none -nographic \\ -chardev socket,id=char3,path=/run/openvswitch/dpdkvhostclient3,server=on \\ -netdev type=vhost-user,id=mynet3,chardev=char3,vhostforce=on \\ -device virtio-net-pci,mac=00:00:00:00:00:03,netdev=mynet3 \\ -object memory-backend-file,id=mem,size=1G,mem-path=/dev/hugepages,share=on \\ -numa node,memdev=mem -mem-prealloc \\ -vnc :03 # 134 vm1 login: ip a flush eth0 ip addr add 3.3.3.33/24 dev eth0 ip link set eth0 up ping -c 5 3.3.3.31 # ping ---\u003e OK iperf3 -t 10 -i 1 -c 3.3.3.31 ## 结论：不同主机之间 vm 1.2G root@ovs3:~/vms/alpine1# cat vm.xml \u003cdomain type='kvm'\u003e \u003cname\u003ealpine1\u003c/name\u003e \u003cvcpu placement='static'\u003e2\u003c/vcpu\u003e \u003cmemory\u003e1048576\u003c/memory\u003e \u003cmemoryBacking\u003e \u003chugepages\u003e \u003cpage size='2048' unit='KiB'/\u003e \u003c/hugepages\u003e \u003c/memoryBacking\u003e \u003cos\u003e \u003ctype arch='x86_64' machine='pc'\u003ehvm\u003c/type\u003e \u003cbootmenu enable='yes'/\u003e \u003c/os\u003e \u003cfeatures\u003e \u003cacpi/\u003e \u003capic/\u003e \u003cpae/\u003e \u003c/features\u003e \u003ccpu mode=\"host-passthrough\"\u003e \u003cnuma\u003e \u003ccell id='0' cpus='0-1' memory='1048576' unit='KiB' memAccess='shared'/\u003e \u003c/numa\u003e \u003c/cpu\u003e \u003cclock offset='utc'/\u003e \u003con_poweroff\u003edestroy\u003c/on_poweroff\u003e \u003con_reboot\u003erestart\u003c/on_reboot\u003e \u003con_crash\u003edestroy\u003c/on_crash\u003e \u003cdevices\u003e \u003cemulator\u003e/usr/bin/kvm\u003c/emulator\u003e \u003cdisk type='file' device='disk'\u003e \u003cdriver name='qemu' type='qcow2'/\u003e \u003csource file='/root/vms/alpine1/alpine1-virt-3.16.1-x86_64.qcow2'/\u003e \u003ctarget dev='vda' bus='virtio'/\u003e \u003cboot order='1'/\u003e \u003c/disk\u003e \u003cinterface type='vhostuser'\u003e \u003cmac address='00:00:00:00:33:01'/\u003e \u003csource type='unix' path='/run/openvswitch/dpdkvhostclient0' mode='server'/\u003e \u003cmodel type='virtio'/\u003e \u003cdriver queues='2'\u003e \u003chost mrg_rxbuf='off'/\u003e \u003c/driver\u003e \u003c/interface\u003e \u003cserial type='pty'\u003e \u003ctarget port='0'/\u003e \u003c/serial\u003e \u003cconsole type='pty'\u003e \u003ctarget type='serial' port='0'/\u003e \u003c/console\u003e \u003cgraphics type='vnc' port='-1' autoport='yes' listen='0.0.0.0'/\u003e \u003cvideo\u003e \u003cmodel type='cirrus' vram='65536' heads='1'/\u003e \u003c/video\u003e \u003cinput type='tablet' bus='usb'/\u003e \u003cinput type='mouse' bus='ps2'/\u003e \u003c/devices\u003e \u003c/domain\u003e ### xml 生成的 id 都是一样的，无法修改，所以vhost-user模式下，启动2台虚拟机会冲突，用qemu-system-x86_64 指定不同的id等启动才行。 ### dpdkvhostuserclient模式下，2台虚拟机可以互相ping通，但iperf3测试，同一台主机内vm 3G，比指定id，性能差了一倍，说明还是有冲突。 ### reboot dpdk-devbind.py -b vfio-pci 0000:00:04.0 systemctl restart openvswitch-switch.service ovs-vsctl show ### vi /etc/rc.local #!/bin/bash echo 1 \u003e /sys/module/vfio/parameters/enable_unsafe_noiommu_mode dpdk-devbind.py -b vfio-pci 0000:00:04.0 systemctl restart openvswitch-switch.service ### 下面方式不好用！why？ vi /lib/systemd/system/openvswitch-switch.service ExecStartPre=-/usr/bin/echo 1 \u003e /sys/module/vfio/parameters/enable_unsafe_noiommu_mode ExecStartPre=-/sbin/modprobe vfio-pci ExecStartPre=-/usr/bin/dpdk-devbind.py -b vfio-pci 0000:00:04.0 ExecStart=/bin/true systemctl daemon-reload \u0026\u0026 systemctl restart openvswitch-switch systemctl status openvswitch-switch ",
    "description": "",
    "tags": null,
    "title": "6. vhostuserclient ovs-dpdk",
    "uri": "/dpdk/6.vhostuserclient.ovs-dpdk/index.html"
  },
  {
    "content": "date: 2023-02-04\nDPDK_in_Containers_Hands-on_Lab https://github.com/intel/SDN-NFV-Hands-on-Samples/tree/master/DPDK_in_Containers_Hands-on_Lab vmware : 192.168.68.56 u22.04 vi /etc/default/grub GRUB_CMDLINE_LINUX=\"default_hugepagesz=1G hugepagesz=1G hugepages=8 iommu=pt intel_iommu=on\" update-grub reboot # hugepages=4 同时运行pktgen, dpdk-testpmd报错！ root@dpdk56:~# dpdk-hugepages.py -s Node Pages Size Total 0 5 1Gb 5Gb Hugepages mounted on /dev/hugepages ## no 8G -\u003e 5G 实际 5G ## vm内存修改为16G apt install -y openvswitch-switch-dpdk update-alternatives --set ovs-vswitchd /usr/lib/openvswitch-switch-dpdk/ovs-vswitchd-dpdk ovs-vswitchd --version systemctl restart openvswitch-switch.service ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-init=true ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-socket-mem=\"1024,0\" ### 只有一个numa node0 ovs-vsctl --no-wait set Open_vSwitch . other_config:dpdk-lcore-mask=0x2 ### 0b0010 --\u003e Cpu1 ovs-vsctl set Open_vSwitch . other_config:pmd-cpu-mask=0x4 ### 0b0100 --\u003e Cpu2 ovs-vsctl add-br br0 -- set bridge br0 datapath_type=netdev ovs-vsctl add-port br0 vhost-user1 -- set Interface vhost-user1 type=dpdkvhostuser ovs-vsctl add-port br0 vhost-user2 -- set Interface vhost-user2 type=dpdkvhostuser ovs-vsctl add-port br0 vhost-user3 -- set Interface vhost-user3 type=dpdkvhostuser ovs-vsctl add-port br0 vhost-user4 -- set Interface vhost-user4 type=dpdkvhostuser ovs-vsctl show ovs-ofctl show br0 # 不加也可以 # echo never \u003e /sys/kernel/mm/transparent_hugepage/enabled # echo never \u003e /sys/kernel/mm/transparent_hugepage/defrag ls -l /var/run/openvswitch/|grep vhost-user srwxr-xr-x 1 root root 0 Dec 30 16:13 vhost-user1 srwxr-xr-x 1 root root 0 Dec 30 16:13 vhost-user2 srwxr-xr-x 1 root root 0 Dec 30 16:13 vhost-user3 srwxr-xr-x 1 root root 0 Dec 30 16:13 vhost-user4 ovs-ofctl del-flows br0 echo \"(Add bi-directional flow vhost-user2 and vhost-user3)\" ovs-ofctl add-flow br0 in_port=2,dl_type=0x800,idle_timeout=0,action=output:3 ovs-ofctl add-flow br0 in_port=3,dl_type=0x800,idle_timeout=0,action=output:2 echo \"(Add bi-directional flow between vhost-user1 and vhost-user4)\" ovs-ofctl add-flow br0 in_port=1,dl_type=0x800,idle_timeout=0,action=output:4 ovs-ofctl add-flow br0 in_port=4,dl_type=0x800,idle_timeout=0,action=output:1 ovs-ofctl dump-flows br0 echo \"Showing OpenFlow to Open vSwitch port mapping:\" ovs-ofctl show br0 ovs-ofctl dump-ports br0 build dpdk-docker, (包含dpdk-testpmd，没有dpdk-l2fwd)\ncat \u003c\u003c EOF \u003e Dockerfile FROM ubuntu:22.04 RUN sed -i 's/archive.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.list RUN sed -i 's/security.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.list RUN ln -sf ../usr/share/zoneinfo/Asia/Shanghai /etc/localtime RUN apt-get update \u0026\u0026 apt-get install -y dpdk dpdk-dev iperf tcpdump WORKDIR /root RUN apt-get -qq clean \u0026\u0026 rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* CMD [\"/bin/bash\"] EOF docker build -t amwork2010/dpdk:u2204 . ##### 源码build dpdk-docker, (包含dpdk-testpmd，包含dpdk-l2fwd)\ncat \u003c\u003c EOF \u003e Dockerfile FROM ubuntu:22.04 RUN sed -i 's/archive.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.list RUN sed -i 's/security.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.list RUN ln -sf ../usr/share/zoneinfo/Asia/Shanghai /etc/localtime RUN apt update \u0026\u0026 apt install -y build-essential RUN apt install -y meson python3-pyelftools pkg-config libnuma-dev wget WORKDIR /root RUN wget http://fast.dpdk.org/rel/dpdk-22.11.1.tar.xz RUN tar Jxvf dpdk-22.11.1.tar.xz RUN cd dpdk-stable-22.11.1 \u0026\u0026 meson setup -Dexamples=all build \u0026\u0026 cd build \u0026\u0026 ninja \u0026\u0026 ninja install RUN apt-get -qq clean \u0026\u0026 rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* CMD [\"/bin/bash\"] EOF docker build -t amwork2010/dpdk:22.11 . build dpdk-pktgen\ncat \u003c\u003c EOF \u003e Dockerfile FROM ubuntu:22.04 RUN sed -i 's/archive.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.list RUN sed -i 's/security.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.list RUN ln -sf ../usr/share/zoneinfo/Asia/Shanghai /etc/localtime RUN apt update \u0026\u0026 apt install -y build-essential meson python3-pyelftools pkg-config libnuma-dev wget dpdk dpdk-dev WORKDIR /root RUN wget https://github.com/pktgen/Pktgen-DPDK/archive/refs/tags/pktgen-22.07.1.tar.gz RUN tar zxvf pktgen-22.07.1.tar.gz RUN cd Pktgen-DPDK-pktgen-22.07.1 \u0026\u0026 meson build \u0026\u0026 cd build \u0026\u0026 ninja RUN cp /root/Pktgen-DPDK-pktgen-22.07.1/build/app/pktgen /usr/bin RUN apt-get -qq clean \u0026\u0026 rm -rf /var/lib/apt/lists/* /tmp/* /var/tmp/* CMD [\"/bin/bash\"] EOF docker build -t amwork2010/pktgen:22.07 . run dpdk-testpmd\ndocker run -it --rm --privileged \\ -v /dev/hugepages:/dev/hugepages -v /var/run/openvswitch:/var/run/openvswitch amwork2010/dpdk:u2204 #-c 0xE0: DPDK can run on core 5-7: (0b1110 0000)--\u003e Cpu6 Cpu7 #--main-lcore 5: make the make the master testpmd thread run on core 5 (0b0010 0000) #-n 1: we only have one memory bank in this VM #--file-prefix testpmd: \"testpmd\" will be appended to hugepage memory files used by this process #--no-pci don't look for any PCI devices #--vdev=net_virtio_user3,mac=00:00:00:00:00:03,path=/var/run/openvswitch/vhost-user3 #--vdev=net_virtio_user4,mac=00:00:00:00:00:04,path=/var/run/openvswitch/vhost-user4: #\tuse a virtual #\tdevice using the net_virtio_user driver, MAC address 00:00:00:00:00:03, and the path to the #\tunix socket is /var/run/openvswitch/vhost-user3 dpdk-testpmd -c 0xE0 -n 1 --socket-mem 1024 --file-prefix testpmd --no-pci \\ --vdev 'net_virtio_user3,mac=00:00:00:00:00:03,path=/var/run/openvswitch/vhost-user3' \\ --vdev 'net_virtio_user4,mac=00:00:00:00:00:04,path=/var/run/openvswitch/vhost-user4' \\ -- -i --burst=64 --txd=2048 --rxd=2048 --auto-start --coremask=0xc0 提示符： testpmd\u003e show port stats all run pktgen\ndocker run -it --rm --privileged \\ -v /dev/hugepages:/dev/hugepages -v /var/run/openvswitch:/var/run/openvswitch amwork2010/pktgen:22.07 #-c 0x19: DPDK can run on core 0,3-4: (0b0001 1001) #--main-lcore 3: make the pktgen dpdk thread run on core 3 (0b1000) #-n 1: we only have one memory bank in this VM #--file-prefix pktgen: \"pktgen\" will be appended to hugepage memory files used by this process #--no-pci don't look for any PCI devices #--vdev 'virtio_user1,mac=00:00:00:00:00:01,path=/var/run/openvswitch/vhost-user1' #--vdev 'virtio_user2,mac=00:00:00:00:00:02,path=/var/run/openvswitch/vhost-user2' #-P: Promiscuous mode #-T: Color terminal output #-m \"0.0,4.1\" (core.port): core 0: port 0 rx/tx; core 4: port 1 rx/tx #注：-m选项一定要和前面dpdk的-c选项符合 pktgen -c 0x19 --main-lcore 3 -n 1 --socket-mem 1024 --file-prefix pktgen --no-pci \\ --vdev 'net_virtio_user1,mac=00:00:00:00:00:01,path=/var/run/openvswitch/vhost-user1' \\ --vdev 'net_virtio_user2,mac=00:00:00:00:00:02,path=/var/run/openvswitch/vhost-user2' \\ -- -T -P -m \"0.0,4.1\" 提示符： \u003e start all ### str ### stp # 在pktgen中设置速率为10%，更具体的速率设置可以通过tx_cycles设置 # 端口0共发送100个包，端口1发送200个 Pktgen:/\u003eset all rate 10 Pktgen:/\u003eset 0 count 100 Pktgen:/\u003eset 1 count 200 Pktgen:/\u003estr watch -n 1 ovs-ofctl dump-flows br0 run pktgen\ndocker run -it --rm --privileged \\ -v /dev/hugepages:/dev/hugepages -v /var/run/openvswitch:/var/run/openvswitch amwork2010/pktgen:22.07 pktgen -c 0x19 --main-lcore 3 -n 1 --socket-mem 1024 --file-prefix pktgen --no-pci \\ --vdev 'net_virtio_user1,mac=00:00:00:00:00:01,path=/var/run/openvswitch/vhost-user1' \\ --vdev 'net_virtio_user2,mac=00:00:00:00:00:02,path=/var/run/openvswitch/vhost-user2' \\ -- -T -P -m \"0.0,4.1\" 发包验证 在pktgen端执行： # 在pktgen中设置速率为10%，更具体的速率设置可以通过tx_cycles设置 # 端口0共发送100个包，端口1发送200个 Pktgen:/\u003eset all rate 10 Pktgen:/\u003eset 0 count 100 Pktgen:/\u003eset 1 count 200 Pktgen:/\u003estr set all rate 10 set 0 count 100 set 1 count 200 str 总结 本次实践主要还是集中在OVS上面的container App的互通以及container内部对dpdk的支持，分别验证了在container内部运行testpmd和l2fwd来进行报文转发。其中，dpdk app的运行模式可以为后续cneos平台server docker化提供一定的技术指导作用。 如果从更系统化的层面来考虑docker结合ovs以及dpdk的使用，更通用的使用场景应该是这样的：在ovs的南向通过dpdk pmd和硬件平台上物理nic的PF或VF绑定，高速收发报文；在ovs的北向，通过virtual device和docker container来共享收发报文，进行上层业务的处理。 南北向之间的流量需要配置flow table来指导转发。流量示意如下图所示：\n流量从物理port流入，到达OVS查找流表送入到串联检测类container1(如NF，IPS)中，container1处理完后再送回流表，再次查找流表找到物理口发送出去 流量从物理port流入，到达OVS查找流表送入到检测类container2中(如WAF) 考虑到有多个安全container app，流量串行通过containerN,container2; 实际上，container之间的数据交互还有别的实现方式，如docker天然支持容器互联技术，这块还有待进一步确定实际方案\nhttps://github.com/intel/SDN-NFV-Hands-on-Samples/tree/master/DPDK_in_Containers_Hands-on_Lab ovs+dpdk-docker实践\nhttps://huaweicloud.csdn.net/635669a1d3efff3090b5e5af.html?spm=1001.2101.3001.6661.1\u0026utm_medium=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromBaidu~activity-1-78589592-blog-77887910.pc_relevant_vip_default\u0026depth_1-utm_source=distribute.pc_relevant_t0.none-task-blog-2~default~BlogCommendFromBaidu~activity-1-78589592-blog-77887910.pc_relevant_vip_default\u0026utm_relevant_index=1 https://datawine.github.io/docker-ovs-dpdk-vnf-exp.html https://blog.csdn.net/me_blue/article/details/78589592 https://www.youtube.com/watch?v=hEmvd7ZjkFw\u0026list=PLg-UKERBljNx44Q68QfQcYsza-fV0ARbp https://www.slideshare.net/MichelleHolley1/dpdk-in-containers-handson-lab\nhttps://doc.dpdk.org/guides/howto/virtio_user_for_container_networking.html 上图(1) 方案中需要NIC支持SR-IOV功能，物理NIC支持的VF个数也依赖于硬件资源；每个container的接口独占VF，多个VF共享下面的一个PF。基于这种方案实现的container，无论对硬件的依赖和绑定，还是container的迁移，支持性都做得不够好。 上图(2) 方案中需要在host中运行vswitch或者vRouter来将上层的containers和底层的物理NIC解耦，只要vswitch（当前比较流行的OVS+DPDK，将OVS放在用户态来实现）的性能足够，一样可以实现高性能的container app了。 基于以上比较，本次预研主要选取第二种方案来实现，方案中使用virtual device(包括virtio-user和vhost-user backend）来实现高性能的container App 或者IPC。Virtio使用共享内存的方式来收发报文，传统的VM可以通过qemu来共享vhost后端的物理地址，但对container而言，作为系统的一个进程，使用这种方式则比较难。目前的思路是只能使用DPDK初始化的hugepages来进行内存共享。所以，要在container中使用dpdk，必须要分配足够的大页内存，且不同container在使用共享内存时要能够分区使用，避免地址重复。\n",
    "description": "",
    "tags": null,
    "title": "7. ovs-dpdk-docker",
    "uri": "/dpdk/7.ovs-dpdk-docker/index.html"
  },
  {
    "content": "date: 2023-02-04 VMWARE 加网卡 编辑对应的 dpdk1.vmx，修改所有e1000为vmxnet3 ，多队列网卡 ethernet0.virtualDev = \"vmxnet3\" ethernet0.wakeOnPcktRcv = \"true\" ... ethernet1.virtualDev = \"vmxnet3\" ethernet1.wakeOnPcktRcv = \"true\" ethernet2.virtualDev = \"vmxnet3\" ethernet2.wakeOnPcktRcv = \"true\" ethernet3.virtualDev = \"vmxnet3\" ethernet3.wakeOnPcktRcv = \"true\" [root@rocky91 ~]# dmesg |grep ens [ 0.794713] ACPI: Added _OSI(3.0 _SCP Extensions) [ 5.571790] vmxnet3 0000:1b:00.0 ens256: renamed from eth2 [ 5.590331] vmxnet3 0000:0b:00.0 ens192: renamed from eth0 [ 5.615548] vmxnet3 0000:13:00.0 ens224: renamed from eth1 [ 5.632393] vmxnet3 0000:1b:00.0 ens256: intr type 3, mode 0, 5 vectors allocated [ 5.632872] vmxnet3 0000:1b:00.0 ens256: NIC Link is Up 10000 Mbps [ 5.716301] vmxnet3 0000:0b:00.0 ens192: intr type 3, mode 0, 5 vectors allocated [ 5.716640] vmxnet3 0000:0b:00.0 ens192: NIC Link is Up 10000 Mbps [ 5.774846] vmxnet3 0000:13:00.0 ens224: intr type 3, mode 0, 5 vectors allocated [ 5.775629] vmxnet3 0000:13:00.0 ens224: NIC Link is Up 10000 Mbps [ 6.497363] IPv6: ADDRCONF(NETDEV_CHANGE): ens224: link becomes ready [ 6.497983] IPv6: ADDRCONF(NETDEV_CHANGE): ens256: link becomes ready [root@rocky91 ~]# cat /proc/interrupts |grep ens 75: 7 67 0 185 IR-PCI-MSI 5767168-edge ens192-rxtx-0 76: 3 1 45 0 IR-PCI-MSI 5767169-edge ens192-rxtx-1 77: 0 11 0 9 IR-PCI-MSI 5767170-edge ens192-rxtx-2 78: 1 42 0 105 IR-PCI-MSI 5767171-edge ens192-rxtx-3 79: 0 0 0 0 IR-PCI-MSI 5767172-edge ens192-event-4 [root@dpdk1 ~]# cat /proc/interrupts |grep ens vmxnet3 (4CPU) 1G大页\nvi /etc/default/grub #GRUB_CMDLINE_LINUX=\"default_hugepagesz=1G hugepagesz=1G hugepages=2 isolcpus=2-3 iommu=pt intel_iommu=on\" GRUB_CMDLINE_LINUX=\"resume=/dev/mapper/rl_rocky91-swap rd.lvm.lv=rl_rocky91/root rd.lvm.lv=rl_rocky91/swap default_hugepagesz=1G hugepagesz=1G hugepages=4 isolcpus=2-3 iommu=pt intel_iommu=on\" grub2-mkconfig -o /boot/grub2/grub.cfg reboot dmesg | grep -e DMAR -e IOMMU cat /proc/cmdline | grep iommu=pt cat /proc/cmdline | grep intel_iommu=on [root@rocky91 ~]# cat /proc/meminfo |grep Huge AnonHugePages: 4096 kB ShmemHugePages: 0 kB FileHugePages: 0 kB HugePages_Total: 2 HugePages_Free: 2 HugePages_Rsvd: 0 HugePages_Surp: 0 Hugepagesize: 1048576 kB Hugetlb: 2097152 kB [root@rocky91 ~]# cat /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages 4 [root@alma91 ~]# mount | grep huge hugetlbfs on /dev/hugepages type hugetlbfs (rw,relatime,pagesize=1024M) 使用大页 mkdir /mnt/huge mount -t hugetlbfs pagesize=1GB /mnt/huge 通过在/etc/fstab文件中添加以下行，可以使挂载点在重新启动后永久存在： nodev /mnt/huge hugetlbfs pagesize=1GB 0 0 reboot [root@dpdk1 ~]# mount|grep hugetlbfs hugetlbfs on /dev/hugepages type hugetlbfs (rw,relatime) nodev on /mnt/huge type hugetlbfs (rw,relatime,pagesize=1GB) yum install -y dpdk dpdk-devel dpdk-tools [root@rocky91 ~]# dpdk-devbind.py -s Network devices using kernel driver =================================== 0000:0b:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens192 drv=vmxnet3 unused= *Active* 0000:13:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens224 drv=vmxnet3 unused= *Active* 0000:1b:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens256 drv=vmxnet3 unused= *Active* [root@rocky91 ~]# ip link set ens224 down [root@rocky91 ~]# ip link set ens256 down [root@rocky91 ~]# modprobe vfio-pci [root@rocky91 ~]# dpdk-devbind.py -s Network devices using kernel driver =================================== 0000:0b:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens192 drv=vmxnet3 unused=vfio-pci *Active* 0000:13:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens224 drv=vmxnet3 unused=vfio-pci 0000:1b:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens256 drv=vmxnet3 unused=vfio-pci [root@rocky91 ~]# dpdk-devbind.py -b vfio-pci 0000:13:00.0 [root@rocky91 ~]# dpdk-devbind.py -b vfio-pci 0000:1b:00.0 [root@rocky91 ~]# dpdk-devbind.py -s Network devices using DPDK-compatible driver ============================================ 0000:13:00.0 'VMXNET3 Ethernet Controller 07b0' drv=vfio-pci unused=vmxnet3 0000:1b:00.0 'VMXNET3 Ethernet Controller 07b0' drv=vfio-pci unused=vmxnet3 Network devices using kernel driver =================================== 0000:0b:00.0 'VMXNET3 Ethernet Controller 07b0' if=ens192 drv=vmxnet3 unused=vfio-pci *Active* 取消绑定: dpdk-devbind.py -u 0000:1b:00.0 dpdk-devbind.py -b vmxnet3 0000:1b:00.0 ##绑回vmxnet3 dpdk-testpmd 测试失败！ dpdk-testpmd 测试失败！未查明原因！ dpdk-testpmd 测试失败！\n# top 会看到1cpu 100%us show port stats all ./build/examples/dpdk-helloworld -l 0-3 -n 4 ./build/app/dpdk-testpmd -l 2-3 -m 4096 -n 4 -- -i -a --forward-mode=rxonly --rxd=4096 --txd=4096 --rss-ip ## 选到3 ./build/app/dpdk-testpmd -l 1-2 -m 4096 -n 4 -- -i -a --forward-mode=rxonly --rxd=4096 --txd=4096 --rss-ip ## 选到2 ./build/app/dpdk-testpmd -l 0-3 -m 4096 -n 4 -- -i -a --forward-mode=rxonly --rxd=4096 --txd=4096 --rss-ip ## 选到1 ./build/app/dpdk-testpmd -l0-3 -- -i --nb-cores=2 --nb-ports=2 --total-num-mbufs=2048 start stop 自己build好用！\nyum -y groupinstall \"Development Tools\" yum install -y python3 python3-pip pip3 install meson ninja pyelftools -i https://pypi.tuna.tsinghua.edu.cn/simple yum install -y numactl numactl-devel export http_proxy=http://10.1.1.12:8118 export https_proxy=http://10.1.1.12:8118 wget http://fast.dpdk.org/rel/dpdk-22.11.1.tar.xz tar Jxvf dpdk-22.11.1.tar.xz cd dpdk-stable-22.11.1 meson setup -Dexamples=all build cd build ninja ninja install 自己build好用！ ",
    "description": "",
    "tags": null,
    "title": "8. rocky 9.1 DPDK",
    "uri": "/dpdk/8.rocky9.1-dpdk/index.html"
  },
  {
    "content": "date: 2023-02-04 https://www.qemu.org/download/\n## centos7 升级 qemu ## https://www.qemu.org/download/ ## qemu 新版本要求gcc 7.4 以上，CentOS官方源自带的gcc是4.8.5 yum -y groupinstall \"Development Tools\" yum install -y python3 python3-pip ## centos8: yum install -y python39 python39-pip yum install -y numactl numactl-devel pip3 install meson ninja pyelftools -i https://pypi.tuna.tsinghua.edu.cn/simple yum install -y glib2 glib2-devel pixman-devel zlib zlib-devel ## 安装RedHat的软件集合（SCLs） yum install centos-release-scl -y yum search devtoolset devtoolset-10 devtoolset-11 devtoolset-7 devtoolset-8 devtoolset-9 yum remove gcc -y ## 安装GCC8 yum install devtoolset-8 ## or 安装GCC11 # yum install devtoolset-11 ## 临时生效，执行： scl enable devtoolset-8 bash # scl enable devtoolset-11 bash gcc -v gcc version 8.3.1 20190311 (Red Hat 8.3.1-3) (GCC) [root@vps-8 ~]# which gcc /opt/rh/devtoolset-8/root/usr/bin/gcc ## 永久生效，执行： ## echo 'source scl_source enable devtoolset-8' \u003e\u003e ~/.bashrc ## BUILD QEMU # wget https://download.qemu.org/qemu-7.2.0.tar.xz wget https://download.qemu.org/qemu-6.2.0.tar.xz tar xvJf qemu-6.2.0.tar.xz cd qemu-6.2.0 ./configure make -j 24 make install /usr/bin/kvm -\u003e /usr/libexec/qemu-kvm /usr/local/bin/qemu-system-x86_64 [root@vps-22 ~]# /usr/libexec/qemu-kvm --version QEMU emulator version 1.5.3 (qemu-kvm-1.5.3-175.el7_9.6), Copyright (c) 2003-2008 Fabrice Bellard [root@vps-8 ~]# /usr/libexec/qemu-kvm --version QEMU emulator version 2.6.0 (qemu-kvm-rhev-2.6.0-29.el7), Copyright (c) 2003-2008 Fabrice Bellard [root@vps-8 ~]# /usr/local/bin/qemu-system-x86_64 --version QEMU emulator version 6.2.0 Copyright (c) 2003-2021 Fabrice Bellard and the QEMU Project developers [root@vps-9 ~]# /usr/local/bin/qemu-system-x86_64 --version QEMU emulator version 7.2.0 Copyright (c) 2003-2022 Fabrice Bellard and the QEMU Project developers ",
    "description": "",
    "tags": null,
    "title": "9. centos7 upgrade qemu",
    "uri": "/dpdk/9.centos7-upgrade-qemu/index.html"
  },
  {
    "content": "date: 2023-02-04 https://wiki.fd.io/view/VPP/Progressive_VPP_Tutorial https://s3-docs.fd.io/vpp/22.10/\nvpp u2204 vmware : 网卡均为vmxnet3 https://packagecloud.io/fdio apt install driverctl driverctl list-devices | grep vmxnet3 0000:0b:00.0 vmxnet3 0000:13:00.0 vmxnet3 0000:1b:00.0 vmxnet3 driverctl set-override 0000:13:00.0 vfio-pci # 重启后仍然绑定 driverctl unset-override 0000:13:00.0 # 解除绑定 dpdk-devbind.py -b vfio-pci 0000:13:00.0 # 重启后解除绑定 apt install dpdk dpdk-dev -y ## 22.10 curl -s https://packagecloud.io/install/repositories/fdio/release/script.deb.sh | sudo bash -vx cat /etc/apt/sources.list.d/fdio_release.list apt-get update apt-get install vpp vpp-plugin-core vpp-plugin-dpdk # apt-get install vpp-api-python python3-vpp-api vpp-dbg vpp-dev # Uninstall the Packages # apt-get remove --purge vpp* # systemctl enable vpp systemctl status vpp vi /lib/systemd/system/vpp.service ExecStartPre=-/sbin/modprobe vfio-pci vi /etc/vpp/startup.conf dpdk { uio-driver vfio-pci dev 0000:13:00.0 dev 0000:1b:00.0 } systemctl daemon-reload \u0026\u0026 systemctl restart vpp \u0026\u0026 systemctl status vpp vppctl show ver vppctl 提示符： vpp# set interface state GigabitEthernet13/0/0 up set interface state GigabitEthernet1b/0/0 up set interface ip address GigabitEthernet13/0/0 1.1.1.1/24 set interface ip address GigabitEthernet1b/0/0 2.1.1.1/24 show interface ubuntu0: ubuntu1: ubuntu0 与 ubuntu1 可以互相ping通。 ubuntu0: iperf -s -i 1\nubuntu1: iperf -t 10 -i 1 -c 1.1.1.2 dpdk有2-3G，网卡是10G\n不用dpdk，重启后：\nip l s ens224 up ip l s ens256 up ip a a 1.1.1.1/24 dev ens224 ip a a 2.1.1.1/24 dev ens256 路由方式1G，这个对的。 clean:\nip a flush ens224 ip a flush ens256 ip l s ens224 down ip l s ens256 down ",
    "description": "",
    "tags": null,
    "title": "0. vpp-u2204",
    "uri": "/vpp/0.vpp-u2204/index.html"
  },
  {
    "content": "date: 2023-02-04\n# vm.xml \u003cinterface type='bridge'\u003e \u003csource bridge='br0'/\u003e \u003cmodel type='vmxnet3'/\u003e \u003c/interface\u003e \u003cinterface type='bridge'\u003e \u003csource bridge='br0'/\u003e \u003cmodel type='vmxnet3'/\u003e \u003c/interface\u003e ## HOST : 10.1.1.8 ../startVMs/startvm.sh vpp_5.151 jammy-server-cloudimg-amd64.20221210.pw1-10.1.5.3.img 4 8 ######### 0.prepare apt update apt -y full-upgrade ln -sf ../usr/share/zoneinfo/Asia/Shanghai /etc/localtime [ -f /var/run/reboot-required ] \u0026\u0026 reboot -f ######### 1. 启用rc.local cat \u003c\u003c EOF \u003e\u003e /etc/rc.local #!/bin/bash echo 1 \u003e /sys/module/vfio/parameters/enable_unsafe_noiommu_mode EOF chmod +x /etc/rc.local cat \u003c\u003c EOF \u003e\u003e /lib/systemd/system/rc-local.service [Install] WantedBy=multi-user.target EOF cat /lib/systemd/system/rc-local.service # 启用服务 systemctl enable rc-local systemctl start rc-local systemctl status rc-local # 查看是否成功 cat /sys/module/vfio/parameters/enable_unsafe_noiommu_mode echo \"vfio-pci\" \u003e /etc/modules-load.d/95-vpp.conf ######### 2. hugepages cat \u003c\u003cEOF \u003e\u003e /etc/sysctl.conf vm.nr_hugepages = 2048 EOF sysctl -p ######### 3. vpp \u0026\u0026 dpdk lshw -businfo -c network apt install dpdk dpdk-dev -y ## https://packagecloud.io/fdio #curl -s https://packagecloud.io/install/repositories/fdio/release/script.deb.sh | sudo bash curl -s https://packagecloud.io/install/repositories/fdio/2210/script.deb.sh | sudo bash cat /etc/apt/sources.list.d/fdio_2210.list apt update apt install vpp vpp-plugin-core vpp-plugin-dpdk -y # systemctl enable vpp # systemctl disable vpp systemctl status vpp mkdir -p /var/log/vpp cat /etc/sysctl.d/80-vpp.conf vi /lib/systemd/system/vpp.service ExecStartPre=-/sbin/modprobe vfio-pci ExecStartPre=-/bin/bash -c 'echo 1 \u003e /sys/module/vfio/parameters/enable_unsafe_noiommu_mode \u0026\u0026 sleep 2' vi /etc/vpp/startup.conf dpdk { uio-driver vfio-pci dev 0000:00:04.0 } systemctl daemon-reload \u0026\u0026 systemctl restart vpp ## CPU0 100% vppctl show ver vppctl show interface apt install docker.io -y docker pull ubuntu:22.04 vi Dockerfile FROM ubuntu:22.04 RUN sed -i 's/archive.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.list \u0026\u0026 \\ sed -i 's/security.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.list RUN apt-get update -y \u0026\u0026 apt-get install -y tzdata \u0026\u0026 \\ ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \u0026\u0026 \\ dpkg-reconfigure -f noninteractive tzdata RUN apt-get install dpdk kmod curl vim -y RUN curl -s https://packagecloud.io/install/repositories/fdio/release/script.deb.sh | bash \u0026\u0026 \\ apt-get update -y \u0026\u0026 apt-get install vpp vpp-plugin-core vpp-plugin-dpdk -y RUN mkdir -p /var/log/vpp ### vpp v22.10-release ### dpdk 21.11 docker build -t amwork2010/vppdpdk:22.10 . docker run --privileged \\ -v /sys/bus/pci/devices:/sys/bus/pci/devices \\ -v /sys/kernel/mm/hugepages:/sys/kernel/mm/hugepages \\ -v /sys/devices/system/node:/sys/devices/system/node \\ -v /lib/modules:/lib/modules \\ -v /dev:/dev \\ -it amwork2010/vppdpdk:22.10 bash docker run --privileged \\ -v /sys/bus/pci/devices:/sys/bus/pci/devices \\ -v /sys/kernel/mm/hugepages:/sys/kernel/mm/hugepages \\ -v /sys/devices/system/node:/sys/devices/system/node \\ -v /lib/modules:/lib/modules \\ -v /dev:/dev \\ -d amwork2010/vppdpdk:22.10 sleep infinity vi /etc/vpp/startup.conf dpdk { uio-driver vfio-pci dev 0000:00:04.0 } vpp -c /etc/vpp/startup.conf \u0026 #### docker run --privileged \\ -v /lib/modules:/lib/modules \\ -d amwork2010/vppdpdk:22.10 sleep infinity # 也可以banding , kmod ： the kmod package would provide modinfo, modprobe and other related tools. # root@714c8d6fc89e:/# modprobe vfio-pci # modprobe: FATAL: Module vfio-pci not found in directory /lib/modules/5.15.0-58-generic # so: -v /lib/modules:/lib/modules # vpp 启动 带不到dpdk网卡，docker stop ， 再start，再 vpp -c /etc/vpp/startup.conf \u0026 才可以banding ",
    "description": "",
    "tags": null,
    "title": "0. vpp-qemu",
    "uri": "/vpp/0.vpp-qemu/index.html"
  },
  {
    "content": "date: 2023-02-04 https://wiki.fd.io/view/VPP/Progressive_VPP_Tutorial https://s3-docs.fd.io/vpp/22.10/\nRocky Linux 8.7\n# rocky87 vi /etc/default/grub GRUB_CMDLINE_LINUX=\"........... default_hugepagesz=1G hugepagesz=1G hugepages=4 iommu=pt intel_iommu=on\" grub2-mkconfig -o /boot/grub2/grub.cfg reboot [root@rocky87 ~]# lshw -c network -businfo Bus info Device Class Description ======================================================== pci@0000:04:00.0 ens161 network VMXNET3 Ethernet Controller pci@0000:0b:00.0 ens192 network VMXNET3 Ethernet Controller pci@0000:13:00.0 ens224 network VMXNET3 Ethernet Controller pci@0000:1b:00.0 ens256 network VMXNET3 Ethernet Controller yum install -y driverctl driverctl list-devices | grep -i vmxnet3 [root@rocky87 ~]# driverctl list-devices | grep -i vmxnet3 0000:04:00.0 vmxnet3 0000:0b:00.0 vmxnet3 0000:13:00.0 vmxnet3 0000:1b:00.0 vmxnet3 yum install -y epel-release sed -e 's|^metalink=|#metalink=|g' \\ -e 's|^#baseurl=https\\?://download.fedoraproject.org/pub/epel/|baseurl=https://mirrors.ustc.edu.cn/epel/|g' \\ -e 's|^#baseurl=https\\?://download.example/pub/epel/|baseurl=https://mirrors.ustc.edu.cn/epel/|g' \\ -i.bak \\ /etc/yum.repos.d/epel.repo yum install -y mbedtls # https://packagecloud.io/fdio # curl -s https://packagecloud.io/install/repositories/fdio/release/script.rpm.sh | sudo bash curl -s https://packagecloud.io/install/repositories/fdio/2106/script.rpm.sh | sudo bash yum install -y vpp vpp-plugins vpp-devel vpp-debuginfo vpp-api-python3 vpp-api-lua vpp-ext-deps #yum install -y vpp vpp-plugins vpp-devel vpp-debuginfo vpp-api-python3 vpp-api-lua vpp-ext-deps ### auto install vpp-lib vpp-selinux-policy [root@rocky87 yum.repos.d]# yum install vpp vpp-plugins vpp-devel vpp-debuginfo vpp-api-python3 vpp-api-lua vpp-ext-deps Last metadata expiration check: 0:02:59 ago on Sun 01 Jan 2023 10:30:25 AM CST. Error: Problem: cannot install the best candidate for the job - nothing provides libmbedcrypto.so.3()(64bit) needed by vpp-plugins-21.06-release.x86_64 - nothing provides libmbedtls.so.12()(64bit) needed by vpp-plugins-21.06-release.x86_64 - nothing provides libmbedx509.so.0()(64bit) needed by vpp-plugins-21.06-release.x86_64 (try to add '--skip-broken' to skip uninstallable packages or '--nobest' to use not only best candidate packages) [root@rocky87 yum.repos.d]# rpm -ql mbedtls ... /usr/lib64/libmbedcrypto.so.2.28.1 /usr/lib64/libmbedcrypto.so.7 /usr/lib64/libmbedtls.so.14 /usr/lib64/libmbedtls.so.2.28.1 /usr/lib64/libmbedx509.so.1 /usr/lib64/libmbedx509.so.2.28.1 ... cd /usr/lib64/ ln -s libmbedcrypto.so.7 libmbedcrypto.so.3 ln -s libmbedtls.so.14 libmbedtls.so.12 ln -s libmbedx509.so.1 libmbedx509.so.0 yum install -y vpp vpp-devel vpp-debuginfo vpp-api-python3 vpp-api-lua vpp-ext-deps wget --content-disposition https://packagecloud.io/fdio/2106/packages/el/8/vpp-plugins-21.06.0-3~gbb25fbf28~b50.x86_64.rpm/download.rpm?distro_version_id=205 rpm -ivh vpp-plugins-21.06.0-3~gbb25fbf28~b50.x86_64.rpm --nodeps 修改配置同ubuntu ip l s ens224 down ip l s ens256 down systemctl restart vpp systemctl status vpp 自己build 失败！make install-dep 依赖包安装不全，名字也对不上，比如：python36-ply实际能安装python3-ply，python-virtualenv 实际 python3-virtualenv devtoolset-9 devtoolset-9-libasan-devel 根本没有 gcc9的，安装的是gcc version 8.5.0 20210514 (Red Hat 8.5.0-15) (GCC) # build OK, 见下篇文档 ",
    "description": "",
    "tags": null,
    "title": "0. vpp-rocky87",
    "uri": "/vpp/0.vpp-rocky87/index.html"
  },
  {
    "content": "date: 2023-02-04 https://wiki.fd.io/view/VPP/Progressive_VPP_Tutorial https://s3-docs.fd.io/vpp/22.10/\nRocky Linux 8.7\nvi /etc/default/grub GRUB_CMDLINE_LINUX=\"..... default_hugepagesz=1G hugepagesz=1G hugepages=4 iommu=pt intel_iommu=on\" grub2-mkconfig -o /boot/grub2/grub.cfg reboot cp /etc/os-release /etc/os-release.bak vi /etc/os-release ID=\"rocky\" --\u003e centos VERSION_ID=\"8.7\" --\u003e VERSION_ID=\"8\" yum -y groupinstall \"Development Tools\" yum -y install git export https_proxy=http://10.1.1.12:8118 export http_proxy=http://10.1.1.12:8118 git clone https://github.com/FDio/vpp.git cd vpp git branch -a git checkout -b 2210 origin/stable/2210 git branch -a make install-dep make install-ext-dep make pkg-rpm #make build #make build-release cd /root mkdir -p rpm cd rpm/ mv /root/vpp/build/external/vpp-ext-deps-22.10-9.x86_64.rpm ./ mv /root/vpp/build-root/vpp-*rpm ./ [root@rocky87 rpm]# ll total 201588 -rw-r--r-- 1 root root 255504 Jan 27 08:39 vpp-22.10.0-3~gb89dcf824.x86_64.rpm -rw-r--r-- 1 root root 31608 Jan 27 08:39 vpp-api-lua-22.10.0-3~gb89dcf824.x86_64.rpm -rw-r--r-- 1 root root 71388 Jan 27 08:39 vpp-api-python3-22.10.0-3~gb89dcf824.x86_64.rpm -rw-r--r-- 1 root root 1212796 Jan 27 08:39 vpp-debuginfo-22.10.0-3~gb89dcf824.x86_64.rpm -rw-r--r-- 1 root root 5591264 Jan 27 08:39 vpp-debugsource-22.10.0-3~gb89dcf824.x86_64.rpm -rw-r--r-- 1 root root 1886688 Jan 27 08:39 vpp-devel-22.10.0-3~gb89dcf824.x86_64.rpm -rw-r--r-- 1 root root 84047680 Jan 27 08:03 vpp-ext-deps-22.10-9.x86_64.rpm -rw-r--r-- 1 root root 6461424 Jan 27 08:39 vpp-lib-22.10.0-3~gb89dcf824.x86_64.rpm -rw-r--r-- 1 root root 50819564 Jan 27 08:39 vpp-lib-debuginfo-22.10.0-3~gb89dcf824.x86_64.rpm -rw-r--r-- 1 root root 11108604 Jan 27 08:39 vpp-plugins-22.10.0-3~gb89dcf824.x86_64.rpm -rw-r--r-- 1 root root 44888316 Jan 27 08:40 vpp-plugins-debuginfo-22.10.0-3~gb89dcf824.x86_64.rpm -rw-r--r-- 1 root root 18520 Jan 27 08:39 vpp-selinux-policy-22.10.0-3~gb89dcf824.x86_64.rpm yum install *.rpm # 报错，conflicts with files，有冲突 yum install *.rpm --downloadonly # 先把需要的依赖包安装 cd /var/cache/dnf/..... yum install .... # 用rpm强制安装 cd /root/rpm rpm -ivh --force *.rpm systemctl status vpp vi /lib/systemd/system/vpp.service ExecStartPre=-/sbin/modprobe vfio-pci vi /etc/vpp/startup.conf dpdk { uio-driver vfio-pci dev 0000:13:00.0 dev 0000:1b:00.0 } plugins { path /usr/lib/vpp_plugins } systemctl daemon-reload \u0026\u0026 systemctl restart vpp \u0026\u0026 systemctl status vpp /opt/vpp/external/x86_64/bin/dpdk-devbind.py -s vppctl show ver vppctl show int vppctl show plugins export PATH=$PATH:/opt/vpp/external/x86_64/bin/ vmware:\n### vmware modprobe vfio-pci ip l s ens224 down ip l s ens256 down dpdk-devbind.py -b vfio-pci 0000:13:00.0 0000:1b:00.0 vi /lib/systemd/system/vpp.service ExecStartPre=-/usr/sbin/ip l s ens224 down ExecStartPre=-/usr/sbin/ip l s ens256 down ExecStartPre=-/sbin/modprobe vfio-pci ## 查看网卡信息 /opt/vpp/external/x86_64/bin/dpdk-devbind.py -s vi /etc/vpp/startup.conf dpdk { uio-driver vfio-pci dev 0000:13:00.0 dev 0000:1b:00.0 } plugins { path /usr/lib/vpp_plugins } systemctl daemon-reload \u0026\u0026 systemctl restart vpp \u0026\u0026 systemctl status vpp /opt/vpp/external/x86_64/bin/dpdk-devbind.py -s qemu:\n### qemu ### 以下可以不用做 BEGIN echo \"vfio-pci\" \u003e /etc/modules-load.d/95-vpp.conf cat \u003c\u003c EOF \u003e\u003e /etc/rc.local echo 1 \u003e /sys/module/vfio/parameters/enable_unsafe_noiommu_mode EOF chmod +x /etc/rc.local ### 可以不用做 END vi /lib/systemd/system/vpp.service After=syslog.target network.target auditd.service NetworkManager-wait-online.service ExecStartPre=-/usr/sbin/ip l s eth1 down ExecStartPre=-/usr/sbin/ip l s eth2 down ExecStartPre=-/sbin/modprobe vfio-pci ExecStartPre=-/bin/bash -c 'echo 1 \u003e /sys/module/vfio/parameters/enable_unsafe_noiommu_mode \u0026\u0026 sleep 2' ## 查看网卡信息 export PATH=$PATH:/opt/vpp/external/x86_64/bin/ /opt/vpp/external/x86_64/bin/dpdk-devbind.py -s vi /etc/vpp/startup.conf dpdk { uio-driver vfio-pci dev 0000:00:04.0 dev 0000:00:05.0 } plugins { path /usr/lib/vpp_plugins } systemctl daemon-reload \u0026\u0026 systemctl restart vpp \u0026\u0026 systemctl status vpp /opt/vpp/external/x86_64/bin/dpdk-devbind.py -s vppctl show ver vppctl show int vppctl show plugins top -H # CPU0 100% ",
    "description": "",
    "tags": null,
    "title": "1. vpp-rocky87-build",
    "uri": "/vpp/1.vpp-rocky87-build/index.html"
  },
  {
    "content": "date: 2023-02-04\nCreate a veth interface https://s3-docs.fd.io/vpp/22.10/gettingstarted/progressivevpp/index.html ip link add name ns1host type veth peer name ns1vpp ip netns add vns1 ip link set ns1host netns vns1 ip netns exec vns1 ifconfig ns1host 1.1.1.2/24 up ip netns exec vns1 route add -net 2.2.2.0/24 gw 1.1.1.1 ip link add name ns2host type veth peer name ns2vpp ip netns add vns2 ip link set ns2host netns vns2 ip netns exec vns2 ifconfig ns2host 2.2.2.2/24 up ip netns exec vns2 route add -net 1.1.1.0/24 gw 2.2.2.1 vppctl\ncreate host-interface name ns1vpp create host-interface name ns2vpp show interface set int state host-ns1vpp up set int state host-ns2vpp up set int ip address host-ns1vpp 1.1.1.1/24 set int ip address host-ns2vpp 2.2.2.1/24 show ip fib show int addr trace add af-packet-input 10 show trace clear trace ip netns exec vns1 ip a ip netns exec vns1 ping -c 1 2.2.2.2\nshow ip neighbors show ip fib show hardware-interfaces host-ns1vpp --\u003e 02:fe:f8:9f:d7:c3 host-ns2vpp --\u003e 02:fe:5b:e3:0b:b4 ip netns exec vns1 ip a ns1host 32:52:66:6e:76:44 ip netns exec vns2 ip a ns2host f6:b5:34:dc:25:c5 ICMP: 1.1.1.2 -\u003e 2.2.2.2\n32:52:66:6e:76:44 -\u003e 02:fe:f8:9f:d7:c3\n02:fe:5b:e3:0b:b4 -\u003e f6:b5:34:dc:25:c5\nhttps://wiki.fd.io/view/VPP/Configure_VPP_As_A_Router_Between_Namespaces\n# 1.Setup #!/bin/bash if [ $USER != \"root\" ] ; then echo \"Restarting script with sudo...\" sudo $0 ${*} exit fi # delete previous incarnations if they exist ip link del dev veth_vpp1 ip link del dev veth_vpp2 ip netns del vpp1 ip netns del vpp2 #create namespaces ip netns add vpp1 ip netns add vpp2 # create and configure 1st veth pair ip link add name veth_vpp1 type veth peer name vpp1 ip link set dev vpp1 up ip link set dev veth_vpp1 up netns vpp1 ip netns exec vpp1 \\ bash -c \" ip link set dev lo up ip addr add 172.16.1.2/24 dev veth_vpp1 ip route add 172.16.2.0/24 via 172.16.1.1 \" # create and configure 2st veth pair ip link add name veth_vpp2 type veth peer name vpp2 ip link set dev vpp2 up ip link set dev veth_vpp2 up netns vpp2 ip netns exec vpp2 \\ bash -c \" ip link set dev lo up ip addr add 172.16.2.2/24 dev veth_vpp2 ip route add 172.16.1.0/24 via 172.16.2.1 \" # 2.Configure Interfaces sudo vppctl create host-interface name vpp1 sudo vppctl create host-interface name vpp2 sudo vppctl set int state host-vpp1 up sudo vppctl set int state host-vpp2 up sudo vppctl set int ip address host-vpp1 172.16.1.1/24 sudo vppctl set int ip address host-vpp2 172.16.2.1/24 # 3.Test $ sudo ip netns exec vpp1 ping 172.16.2.1 -c 1 PING 172.16.2.2 (172.16.2.2) 56(84) bytes of data. 64 bytes from 172.16.2.2: icmp_seq=1 ttl=63 time=0.135 ms --- 172.16.2.2 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 0.135/0.135/0.135/0.000 ms vpp# show ip arp Time FIB IP4 Stat Ethernet Interface 1050.5729 0 172.16.1.2 5a:df:31:28:dc:5c host-vpp1 1050.5768 0 172.16.2.2 12:fa:19:cb:39:e3 host-vpp2 vpp# show interface vpp# show ip fib host1 : 10.1.5.151 host2 : 10.1.5.152 # 151 set interface state GigabitEthernet0/4/0 up set interface ip address GigabitEthernet0/4/0 1.1.1.1/24 # 152 set interface state GigabitEthernet0/4/0 up set interface ip address GigabitEthernet0/4/0 1.1.1.2/24 ping ok! memif https://s3-docs.fd.io/vpp/22.10/gettingstarted/progressivevpp/twovppinstances.html routing https://s3-docs.fd.io/vpp/22.10/gettingstarted/progressivevpp/routing.html switching https://s3-docs.fd.io/vpp/22.10/gettingstarted/progressivevpp/switching.html ",
    "description": "",
    "tags": null,
    "title": "2. VPP Tutorial",
    "uri": "/vpp/2.vpp-tutorial/index.html"
  },
  {
    "content": "date: 2023-02-05 VM: 10.1.5.153 docker pull frrdocker/dockervpp:vpp1 docker run --privileged --name VPP1 --cap-add=NET_ADMIN --cap-add=SYS_ADMIN --rm -it frrdocker/dockervpp:vpp1 vpp -c /etc/vpp/startup.conf \u0026 docker run --privileged --name VPP2 --cap-add=NET_ADMIN --cap-add=SYS_ADMIN --rm -it frrdocker/dockervpp:vpp1 vpp -c /etc/vpp/startup.conf \u0026 docker run --privileged --name VPP3 --cap-add=NET_ADMIN --cap-add=SYS_ADMIN --rm -it frrdocker/dockervpp:vpp1 vpp -c /etc/vpp/startup.conf \u0026 vi 04-docker-network-remove-default.sh #! bi/bash VPP1=$(docker exec -it VPP1 bash -c hostname | tr -d '\\r\\n') VPP2=$(docker exec -it VPP2 bash -c hostname | tr -d '\\r\\n') VPP3=$(docker exec -it VPP3 bash -c hostname | tr -d '\\r\\n') echo $VPP1 echo $VPP2 echo $VPP3 ### Disconnect VPP1 from default bridge docker0 docker network disconnect bridge $VPP1 ### Disconnect VPP2 from default bridge docker0 docker network disconnect bridge $VPP2 ### Disconnect VPP3 from default bridge docker0 docker network disconnect bridge $VPP3 bash -xv 04-docker-network-remove-default.sh docker network create --driver=bridge --subnet=11.11.0.0/16 vpp_subnet1 docker network create --driver=bridge --subnet=12.12.0.0/16 vpp_subnet2 #! bi/bash VPP1=$(docker exec -it VPP1 bash -c hostname | tr -d '\\r\\n') VPP2=$(docker exec -it VPP2 bash -c hostname | tr -d '\\r\\n') VPP3=$(docker exec -it VPP3 bash -c hostname | tr -d '\\r\\n') echo $VPP1 echo $VPP2 echo $VPP3 ### Connect vpp_subnet1 to VPP1 and VPP2 docker network connect vpp_subnet1 $VPP1 docker network connect vpp_subnet1 $VPP2 ### Connect vpp_subnet2 to VPP2 and VPP3 docker network connect vpp_subnet2 $VPP2 docker network connect vpp_subnet2 $VPP3 # VPP1 ifconfig eth1 0.0.0.0 vppctl create host-interface name eth1 show int set interface state host-eth1 up set interface ip address host-eth1 11.11.0.11/16 show int addr ip route add 12.12.0.0/16 via 11.11.0.12 # VPP2 ifconfig eth1 0.0.0.0 ifconfig eth2 0.0.0.0 vppctl create host-interface name eth1 create host-interface name eth2 set interface state host-eth1 up set interface state host-eth2 up set interface ip address host-eth1 11.11.0.12/16 set interface ip address host-eth1 12.12.0.21/16 # VPP3 ifconfig eth1 0.0.0.0 vppctl create host-interface name eth1 set interface state host-eth1 up set interface ip address host-eth1 12.12.0.22/16 ip route add 11.11.0.0/16 via 12.12.0.21 ",
    "description": "",
    "tags": null,
    "title": "3. frrdocker-dockervpp",
    "uri": "/vpp/3.frrdocker-dockervpp/index.html"
  },
  {
    "content": "date: 2023-02-05 https://github.com/MarioDoman/VPP_TAP_INT_WITH_Containers echo \"Remove old netns simlink\" sudo rm -Rf /var/run/netns sudo mkdir /var/run/netns # veth pair for docker1 sudo ip link add veth10 type veth peer name veth11 sudo ip link set veth10 up sudo ip link set veth11 up # veth pair for docker2 sudo ip link add veth20 type veth peer name veth21 sudo ip link set veth20 up sudo ip link set veth21 up #Create docker containers docker pull busybox docker run -d --name \"docker1\" busybox sleep 36000 docker run -d --name \"docker2\" busybox sleep 36000 #Wait for containers sleep 10 #Expose containers to the 'ip netns exec' tools pid1=`docker inspect -f '{{.State.Pid}}' docker1` ln -s /proc/$pid1/ns/net /var/run/netns/docker1 pid2=`docker inspect -f '{{.State.Pid}}' docker2` ln -s /proc/$pid2/ns/net /var/run/netns/docker2 ip netns ls # Move the veth10 into docker1 network namespace respectivley. ip link set veth10 netns docker1 ip netns exec docker1 ip addr add 192.168.1.2/24 dev veth10 ip netns exec docker1 ip link set veth10 up ip netns exec docker1 ip route add 192.168.2.0/24 via 192.168.1.1 ip netns exec docker1 ip route add 192.168.3.0/24 via 192.168.1.1 ip netns exec docker1 ip a ip netns exec docker1 ip r ip netns exec docker1 route -n # Move the veth20 into docker1 network namespace respectivley. ip link set veth20 netns docker2 ip netns exec docker2 ip addr add 192.168.3.2/24 dev veth20 ip netns exec docker2 ip link set veth20 up ip netns exec docker2 ip route add 192.168.2.0/24 via 192.168.3.1 ip netns exec docker2 ip route add 192.168.1.0/24 via 192.168.3.1 # connect veth on the host to vpp vppctl create host-interface name veth11 vppctl set int ip address host-veth11 192.168.1.1/24 vppctl set int state host-veth11 up # connect veth on the host to vpp vppctl create host-interface name veth21 vppctl set int ip address host-veth21 192.168.3.1/24 vppctl set int state host-veth21 up # Create vpp tap interface vppctl create tap vppctl set interface state tap0 up vppctl set interface ip address tap0 192.168.2.1/24 #Assign tap interface address on kernel interface sudo ip addr add 192.168.2.2/24 dev tap0 sudo ip link set tap0 up # Add routing from host to containers via tap ip route add 192.168.1.0/24 via 192.168.2.1 ip route add 192.168.3.0/24 via 192.168.2.1 # Start iperf server on hostvm #iperf -sDB 192.168.2.2 #TEST! echo \"Pinging container1 via host \u003e TAP-VPP \u003e Container1\" ping -c3 192.168.1.2 echo \"Pinging container2 via host \u003e TAP-VPP \u003e Container2\" ping -c3 192.168.3.2 #sudo docker exec docker1 apt update #sudo docker exec docker1 apt -y install iputils-ping net-tools iperf echo \"Ping from container1 via container \u003e VPP-TAP \u003e Host\" docker exec docker1 ping -c3 192.168.2.2 #docker exec docker1 iperf -s -D #sudo docker exec docker2 apt update #sudo docker exec docker2 apt -y install iputils-ping net-tools iperf #echo \"Ping from container2 via container \u003e VPP-TAP \u003e Host\" docker exec docker2 ping -c3 192.168.2.2 #docker exec docker2 iperf -s -D echo \"Ping from Container1 to Container2 via VPP\" docker exec docker1 ping -c3 192.168.3.2 # Create vpp tap interface vppctl create tap vppctl set interface state tap1 up vppctl set interface ip address tap1 192.168.2.1/24 #Assign tap interface address on kernel interface sudo ip addr add 192.168.2.2/24 dev tap0 sudo ip link set tap1 up ",
    "description": "",
    "tags": null,
    "title": "4. VPP_TAP_INT_Containers1",
    "uri": "/vpp/4.vpp-containers1/index.html"
  },
  {
    "content": "date: 2023-02-05 docker pull alpine:3.17.1 cat \u003c\u003c EOF \u003e Dockerfile FROM alpine:3.17.1 ENV TZ Asia/Shanghai RUN sed -i 's/dl-cdn.alpinelinux.org/mirrors.ustc.edu.cn/g' /etc/apk/repositories RUN apk add --update --no-cache bash tcpdump iperf busybox-extras iproute2 iputils tzdata RUN cp /usr/share/zoneinfo/\\${TZ} /etc/localtime \\ \u0026\u0026 echo \\${TZ} \u003e /etc/timezone \u0026\u0026 rm -rf /var/cache/apk/* EOF docker build -t amwork2010/alpine:iperf . # 153 vppctl set interface state GigabitEthernet0/4/0 up set interface ip address GigabitEthernet0/4/0 1.1.1.1/24 create tap set interface state tap0 up set interface ip address tap0 2.2.2.1/24 ip route add 3.3.3.0/24 via 1.1.1.2 #Assign tap interface address on kernel interface #sudo ip addr add 2.2.2.2/24 dev tap0 #sudo ip link set tap0 up #ip route add 3.3.3.0/24 via 2.2.2.1 #ping 3.3.3.1 --\u003e OK ip addr flush dev tap0 #Create docker containers docker pull amwork2010/alpine:iperf docker run -d --name \"docker\" amwork2010/alpine:iperf sleep 36000 #Wait for containers sleep 10 pid1=`docker inspect -f '{{.State.Pid}}' docker` ln -s /proc/$pid1/ns/net /var/run/netns/docker docker network disconnect bridge $(docker exec -it docker bash -c hostname | tr -d '\\r\\n') ip link set tap0 netns docker ip netns exec docker ip addr add 2.2.2.2/24 dev tap0 ip netns exec docker ip link set tap0 up ip netns exec docker ip route add 3.3.3.0/24 via 2.2.2.1 # 154 vppctl set interface state GigabitEthernet0/4/0 up set interface ip address GigabitEthernet0/4/0 1.1.1.2/24 create tap set interface state tap0 up set interface ip address tap0 3.3.3.1/24 ip route add 2.2.2.0/24 via 1.1.1.1 # ip route del 2.2.2.0/24 via 1.1.1.2 #Create docker containers docker pull amwork2010/alpine:iperf docker run -d --name \"docker\" amwork2010/alpine:iperf sleep 36000 #Wait for containers sleep 10 pid2=`docker inspect -f '{{.State.Pid}}' docker` ln -s /proc/$pid2/ns/net /var/run/netns/docker docker network disconnect bridge $(docker exec -it docker bash -c hostname | tr -d '\\r\\n') ip link set tap0 netns docker ip netns exec docker ip addr add 3.3.3.2/24 dev tap0 ip netns exec docker ip link set tap0 up ip netns exec docker ip route add 2.2.2.0/24 via 3.3.3.1 ip netns exec docker ping 2.2.2.2 # ping OK ",
    "description": "",
    "tags": null,
    "title": "5. VPP_TAP_INT_Containers2",
    "uri": "/vpp/5.vpp-containers2/index.html"
  },
  {
    "content": "date: 2023-02-05 VPP和Linux内核协议栈通信方法 https://blog.csdn.net/turbock/article/details/103912015 10.1.5.153\n2: ens3: \u003cBROADCAST,MULTICAST,UP,LOWER_UP\u003e mtu 1500 qdisc mq state UP group default qlen 1000 link/ether 52:54:00:f8:00:83 brd ff:ff:ff:ff:ff:ff altname enp0s3 inet 10.1.5.153/21 brd 10.1.7.255 scope global ens3 valid_lft forever preferred_lft forever inet6 fe80::5054:ff:fef8:83/64 scope link valid_lft forever preferred_lft forever 3: ens4: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 52:54:00:0d:bb:f4 brd ff:ff:ff:ff:ff:ff altname enp0s4 4: ens5: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 52:54:00:94:18:27 brd ff:ff:ff:ff:ff:ff altname enp0s5 ip l s ens3 down systemctl start vpp # vpp纳管ens4 ip a 2: ens3: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc mq state DOWN group default qlen 1000 link/ether 52:54:00:f8:00:83 brd ff:ff:ff:ff:ff:ff altname enp0s3 4: ens5: \u003cBROADCAST,MULTICAST\u003e mtu 1500 qdisc noop state DOWN group default qlen 1000 link/ether 52:54:00:94:18:27 brd ff:ff:ff:ff:ff:ff altname enp0s5 vppctl #创建tap0网卡 create tap ? create tap host-ip4-addr 10.1.5.153/21 host-ip4-gw 10.1.1.1 host-mac-addr 52:54:00:0d:bb:f4 host-if-name ens4 #将网络接口tap0和g0绑定到网桥1上,并启动。可以不设ip set int l2 bridge tap0 1 set int l2 bridge GigabitEthernet0/4/0 1 set int state GigabitEthernet0/4/0 up set int state tap0 up ip a a 10.1.5.153/21 dev ens4 # FROM other VM ping OK ssh 10.1.1.5.153 --\u003e OK ip netns add ns0 ip link add vpp0 type veth peer name vethns0 ip link set vethns0 netns ns0 ip netns exec ns0 ip link set lo up ip netns exec ns0 ip link set vethns0 up ip netns exec ns0 ip addr add 192.168.1.1/24 dev vethns0 ip netns exec ns0 ethtool -K vethns0 rx off tx off vpp# set int l2 bridge GigabitEthernet4/0/0 1 vpp# set int state GigabitEthernet4/0/0 up vpp# create host-interface name vpp0 vpp# set interface state host-vpp0 up vpp# set interface l2 bridge host-vpp0 1 ",
    "description": "",
    "tags": null,
    "title": "6. VPP_kernel_interaction",
    "uri": "/vpp/6.vpp_kernel/index.html"
  },
  {
    "content": "date: 2023-02-05\nHOST : 10.1.1.8 –\u003e VM: 10.1.5.155 https://s3-docs.fd.io/vpp/22.10/usecases/vhost/index.html https://wiki.fd.io/view/VPP/Use_VPP_to_connect_VMs_Using_Vhost-User_Interface\n# vm.xml \u003cinterface type='bridge'\u003e \u003csource bridge='br0'/\u003e \u003cmodel type='vmxnet3'/\u003e \u003c/interface\u003e \u003cinterface type='bridge'\u003e \u003csource bridge='br0'/\u003e \u003cmodel type='vmxnet3'/\u003e \u003c/interface\u003e ## HOST : 10.1.1.8 ../startVMs/startvm.sh vpp_5.155 jammy-server-cloudimg-amd64.20221210.pw1-10.1.5.3.img 4 8 ######### 0.prepare apt update apt -y full-upgrade ln -sf ../usr/share/zoneinfo/Asia/Shanghai /etc/localtime [ -f /var/run/reboot-required ] \u0026\u0026 reboot -f ######### 1. 启用rc.local cat \u003c\u003c EOF \u003e\u003e /etc/rc.local #!/bin/bash echo 1 \u003e /sys/module/vfio/parameters/enable_unsafe_noiommu_mode EOF chmod +x /etc/rc.local cat \u003c\u003c EOF \u003e\u003e /lib/systemd/system/rc-local.service [Install] WantedBy=multi-user.target EOF cat /lib/systemd/system/rc-local.service # 启用服务 systemctl enable rc-local systemctl start rc-local systemctl status rc-local cat /sys/module/vfio/parameters/enable_unsafe_noiommu_mode ######### 2. vfio \u0026\u0026 hugepages (可不做，具体见下80-vpp.conf vpp.service) echo \"vfio-pci\" \u003e /etc/modules-load.d/95-vpp.conf cat \u003c\u003cEOF \u003e\u003e /etc/sysctl.conf vm.nr_hugepages = 2048 EOF sysctl -p ######### 3. vpp 22.10 \u0026\u0026 dpdk lshw -businfo -c network #apt install dpdk dpdk-dev -y apt install dpdk -y ## https://packagecloud.io/fdio #curl -s https://packagecloud.io/install/repositories/fdio/release/script.deb.sh | sudo bash curl -s https://packagecloud.io/install/repositories/fdio/2210/script.deb.sh | sudo bash cat /etc/apt/sources.list.d/fdio_2210.list apt update apt install vpp vpp-plugin-core vpp-plugin-dpdk -y # systemctl enable vpp # systemctl disable vpp systemctl status vpp mkdir -p /var/log/vpp vi /etc/sysctl.d/80-vpp.conf vm.nr_hugepages=2048 vi /lib/systemd/system/vpp.service ExecStartPre=-/sbin/modprobe vfio-pci ExecStartPre=-/bin/bash -c 'echo 1 \u003e /sys/module/vfio/parameters/enable_unsafe_noiommu_mode \u0026\u0026 sleep 2' vi /etc/vpp/startup.conf dpdk { uio-driver vfio-pci dev 0000:00:04.0 } systemctl daemon-reload \u0026\u0026 systemctl restart vpp ## CPU0 100% vppctl show ver vppctl show interface apt update egrep -c '(vmx|svm)' /proc/cpuinfo grep -E --color '(vmx|svm)' /proc/cpuinfo apt install -y cpu-checker kvm-ok apt install -y qemu-kvm virt-manager libvirt-daemon-system virtinst libvirt-clients qemu-utils bridge-utils systemctl status libvirtd cat \u003e\u003e /etc/libvirt/qemu.conf \u003c\u003c EOF user = \"root\" group = \"root\" EOF systemctl restart libvirtd.service vppctl # create vhost-user socket /tmp/vm00.sock show vhost-user show int set interface state VirtualEthernet0/0/0 up set interface state GigabitEthernet0/4/0 up set interface l2 bridge VirtualEthernet0/0/0 100 set interface l2 bridge GigabitEthernet0/4/0 100 show bridge # clean # delete vhost-user VirtualEthernet0/0/0 mv alpine-virt-3.16.1-iperf3-x86_64.qcow2 disk virsh define vm.xml virsh start alpine1 virsh console alpine1 ip link set eth0 up ip addr add 10.1.5.188/21 dev eth0 ip a ping 10.1.1.1 ### OK vm.xml\n\u003cdomain type='kvm'\u003e \u003cname\u003ealpine1\u003c/name\u003e \u003cvcpu placement='static'\u003e2\u003c/vcpu\u003e \u003cmemory\u003e1048576\u003c/memory\u003e \u003cmemoryBacking\u003e \u003chugepages\u003e \u003cpage size='2048' unit='KiB'/\u003e \u003c/hugepages\u003e \u003c/memoryBacking\u003e \u003cos\u003e \u003ctype arch='x86_64' machine='pc'\u003ehvm\u003c/type\u003e \u003cbootmenu enable='yes'/\u003e \u003c/os\u003e \u003cfeatures\u003e \u003cacpi/\u003e \u003capic/\u003e \u003cpae/\u003e \u003c/features\u003e \u003ccpu mode=\"host-passthrough\"\u003e \u003cnuma\u003e \u003ccell id='0' cpus='0-1' memory='1048576' unit='KiB' memAccess='shared'/\u003e \u003c/numa\u003e \u003c/cpu\u003e \u003cclock offset='utc'/\u003e \u003con_poweroff\u003edestroy\u003c/on_poweroff\u003e \u003con_reboot\u003erestart\u003c/on_reboot\u003e \u003con_crash\u003edestroy\u003c/on_crash\u003e \u003cdevices\u003e \u003cemulator\u003e/usr/bin/kvm\u003c/emulator\u003e \u003cdisk type='file' device='disk'\u003e \u003cdriver name='qemu' type='qcow2'/\u003e \u003csource file='/root/vms/alpine1/disk'/\u003e \u003ctarget dev='vda' bus='virtio'/\u003e \u003cboot order='1'/\u003e \u003c/disk\u003e \u003cinterface type='vhostuser'\u003e \u003cmac address='00:00:00:00:00:01'/\u003e \u003csource type='unix' path='/tmp/vm00.sock' mode='server'/\u003e \u003ctarget dev='vnet1'/\u003e \u003cmodel type='virtio'/\u003e \u003cdriver queues='2'\u003e \u003chost mrg_rxbuf='off'/\u003e \u003c/driver\u003e \u003c/interface\u003e \u003cserial type='pty'\u003e \u003ctarget port='0'/\u003e \u003c/serial\u003e \u003cconsole type='pty'\u003e \u003ctarget type='serial' port='0'/\u003e \u003c/console\u003e \u003cgraphics type='vnc' port='-1' autoport='yes' listen='0.0.0.0'/\u003e \u003cvideo\u003e \u003cmodel type='cirrus' vram='65536' heads='1'/\u003e \u003c/video\u003e \u003cinput type='tablet' bus='usb'/\u003e \u003cinput type='mouse' bus='ps2'/\u003e \u003c/devices\u003e \u003c/domain\u003e ",
    "description": "",
    "tags": null,
    "title": "7. vpp qemu vhost",
    "uri": "/vpp/7.vpp_qemu_vhost/index.html"
  },
  {
    "content": "date: 2023-02-05\nvpp常用命令与基本操作\nvpp的基本操作： 1，up/down vpp# set int state G0 up vpp# set int state G0 down 2，配置IP vpp#set int ip address G0 192.168.59.134/24 3，配置mac vpp#set int mac address G0 00:00:00:00:00:00 4,查看基本信息 vpp# show int vpp# show int addr 5，配置路由 vpp# ip route add 0.0.0.0/0 via 0.0.0.0 vpp# ip table [add|del] \u003ctable-id\u003e //创建IPv4 table表 vpp# ip6 table [add|del] \u003ctable-id\u003e //创建IPv6 table表 vpp# show ip fib //查看路由 vpp# show ip arp //查看arp 表 6，创建桥 vpp# create bridge-domain 100 //桥100 vpp# show bridge-domain 100 detail //查看桥下信息 7，创建vlan vpp# create sub-interfaces G1 100 //G1.100 vpp# delete sub-interface G1.100 //del 8，将vlan放桥下 vpp#set int l2 bridge G1.100 100 9，删除桥,再删除桥 vpp#set int l3 G0 vpp#create bridge-domain 100 del 10，配置路由 vpp#ip route add 0.0.0.0/0 via 端口IP //删除路由 vpp# ip route del 0.0.0.0/0 via 端口IP //查看路由 vpp#show ip fib 11，配置snat vpp# nat44 add interface address G0 //将G0 作为地址池 vpp# set interface nat44 in G1 out G2 // 设置出入口 vpp# show nat44 address //查看地址池 vpp# show nat44 interfaces //查看接口 vpp# nat44 forwarding enable|disable //nat转发的启动/禁止 12，配置dnat vpp# set interface nat44 in \u003cintfc\u003e out \u003cintfc\u003e [output-feature] [del] vpp# nat44 add address \u003cip4-range-start\u003e [- \u003cip4-range-end\u003e] [tenant-vrf \u003cvrf-id\u003e] vpp# nat44 add static mapping tcp|udp|icmp local \u003cip4-addr\u003e [\u003cport\u003e] external (\u003cip4-addr\u003e|\u003cintfc\u003e) [\u003cport\u003e] [vrf \u003ctable-id\u003e] [twice-nat] [out2in-only] [del] 13，配置wireguard vpp# wireguard create listen-port 8899 private-key 私钥内容 src 公网IP //创建加配置 vpp# show wireguard interface vpp# show wireguard peer //添加peer vpp# wireguard peer add \u003cwg_int\u003e public-key \u003cpub_key_other\u003eendpoint \u003cip4_dst\u003e allowed-ip \u003cprefix\u003edst-port [port_dst] persistent-keepalive [keepalive_interval] vpp# wireguard peer remove \u003cindex\u003e //删除peer 14，配置loopback vpp# create loopback interface mac 00:0c:29:1f:ce:07 instance 100 vpp# delete loopback interface intfc loop100 15，配置gre vpp# create gre tunnel src 1.1.1.2/24 dst 2.2.2.3/24 instance 100 outer-fib-id 0 //创建gre100 vpp# create gre tunnel src 1.1.1.2/24 dst 2.2.2.3/24 instance 100 outer-fib-id 0 del //删除gre100 vpp# show gre tunnel 16，配置memif vpp# create memif id 0 /run/vpp/contiv/memif1.sock vpp# create interface memif id 0 socket-id 1 master mode ip secret vpp123 vpp# set int state memif0 up vpp# set int ip address memif0 192.168.1.1/24 vpp# ping 192.168.1.2 //另一个vpp 的memif 设为192.168.1.2/24 vpp# delete interface memif memif1/0 //删除操作 vpp# delete memif socket id 0 //删除操作 17，查看版本 vpp# show version 18，查看启动插件 vpp# show plugin 19, 查看vpp线程 vpp# show threads vpp+ show run 20, 创建tap # create tap id 10 host-if-name host //创建tap10 linux主机上的接口为host # set int state tap10 up //up # set int ip address tap10 192.168.100.100/24 //设IP ",
    "description": "",
    "tags": null,
    "title": "8. vpp commands",
    "uri": "/vpp/8.vpp_command/index.html"
  },
  {
    "content": "date: 2023-02-05\nhttps://projectcalico.docs.tigera.io/getting-started/kubernetes/vpp/getting-started VMware VM u2204 3台，双网卡: vmxnet3\nhostnamectl set-hostname vpp61 cat \u003c\u003c EOF \u003e\u003e /etc/hosts 192.168.68.61 vpp61 192.168.68.62 vpp62 192.168.68.63 vpp63 EOF apt update apt -y full-upgrade [ -f /var/run/reboot-required ] \u0026\u0026 reboot -f ln -sf ../usr/share/zoneinfo/Asia/Shanghai /etc/localtime vi /etc/default/grub GRUB_CMDLINE_LINUX=\"iommu=pt intel_iommu=on\" #GRUB_CMDLINE_LINUX=\"default_hugepagesz=1G hugepagesz=1G hugepages=4 iommu=pt intel_iommu=on\" update-grub echo \"vfio-pci\" \u003e /etc/modules-load.d/95-vpp.conf echo \"vm.nr_hugepages = 1024\" \u003e\u003e /etc/sysctl.conf reboot cat /proc/cmdline | grep -e iommu=pt -e intel_iommu=on -e huge dmesg | grep -e DMAR -e IOMMU cat /proc/meminfo | grep Huge lscpu | grep NUMA lshw -businfo -c network pci@0000:0b:00.0 network VMXNET3 Ethernet Controller pci@0000:13:00.0 network VMXNET3 Ethernet Controller apt install driverctl -y driverctl list-devices ... 0000:0b:00.0 vmxnet3 ... 0000:13:00.0 vmxnet3 swapoff -a sed -i '/swap/ s/^\\(.*\\)$/#\\1/g' /etc/fstab free -h tee /etc/modules-load.d/containerd.conf \u003c\u003cEOF overlay br_netfilter EOF tee /etc/modules-load.d/ipvs.conf \u003c\u003cEOF ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh br_netfilter bridge nf_conntrack EOF cat \u003c\u003cEOF \u003e\u003e /etc/sysctl.conf net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 vm.swappiness=0 EOF sysctl -p export https_proxy=http://10.1.1.12:8118 wget https://github.com/containerd/containerd/releases/download/v1.6.14/cri-containerd-cni-1.6.14-linux-amd64.tar.gz tar zxvf cri-containerd-cni-1.6.14-linux-amd64.tar.gz -C / mv /etc/cni/net.d/10-containerd-net.conflist /etc/cni/net.d/10-containerd-net.conflist.bak mkdir -p /etc/containerd containerd config default \u003e /etc/containerd/config.toml sed -i 's#registry.k8s.io#registry.aliyuncs.com/google_containers#g' /etc/containerd/config.toml # pause:3.6 ---\u003e\u003e\u003e pause:3.8 sed -i 's/pause:3.6/pause:3.8/g' /etc/containerd/config.toml sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml systemctl daemon-reload systemctl enable containerd.service systemctl restart containerd.service systemctl status containerd.service apt install -y apt-transport-https socat curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat \u003c\u003cEOF \u003e/etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF apt update apt install -y kubeadm=1.25.5-00 kubelet=1.25.5-00 kubectl=1.25.5-00 systemctl enable kubelet.service --now kubeadm config print init-defaults --component-configs KubeletConfiguration # cgroupDriver: systemd # # kubeadm config images pull kubeadm config images pull --kubernetes-version=v1.25.5 --image-repository registry.aliyuncs.com/google_containers kubeadm init --kubernetes-version=v1.25.5 \\ --image-repository registry.aliyuncs.com/google_containers \\ --pod-network-cidr=10.244.0.0/16 \\ --apiserver-advertise-address=192.168.68.61 cat \u003c\u003cEOF \u003e\u003e ~/.bashrc alias kp='kubectl get pod -o wide --all-namespaces' alias wkp='watch -n 1 kubectl get pod -o wide --all-namespaces' alias ks='kubectl get svc -o wide --all-namespaces' alias kn='kubectl get node -o wide --all-namespaces' alias k='kubectl' EOF #kubectl taint nodes --all node-role.kubernetes.io/control-plane- #kubectl taint nodes --all node-role.kubernetes.io/master- wget https://raw.githubusercontent.com/projectcalico/calico/v3.24.5/manifests/tigera-operator.yaml cat \u003c\u003c EOF \u003e calicovpp.img.pull.sh time crictl pull docker.io/calico/apiserver:v3.24.5 time crictl pull docker.io/calico/cni:v3.24.5 time crictl pull docker.io/calico/kube-controllers:v3.24.5 time crictl pull docker.io/calico/node:v3.24.5 time crictl pull docker.io/calico/pod2daemon-flexvol:v3.24.5 time crictl pull docker.io/calico/typha:v3.24.5 time crictl pull docker.io/calicovpp/agent:v3.24.0 time crictl pull docker.io/calicovpp/vpp:v3.24.0 time crictl pull quay.io/tigera/operator:v1.28.5 EOF bash -xv calicovpp.img.pull.sh kubectl create -f tigera-operator.yaml wget https://raw.githubusercontent.com/projectcalico/vpp-dataplane/v3.24.0/yaml/calico/installation-default.yaml vi installation-default.yaml apiVersion: operator.tigera.io/v1 kind: Installation metadata: name: default spec: # Configures Calico networking. calicoNetwork: linuxDataplane: VPP ipPools: - blockSize: 24 cidr: 10.244.0.0/16 encapsulation: None ##### NO IPIP \u0026\u0026 NO VXLAN --- kubectl create -f installation-default.yaml # If you have configured hugepages on your machines wget https://raw.githubusercontent.com/projectcalico/vpp-dataplane/v3.24.0/yaml/generated/calico-vpp.yaml vi calico-vpp.yaml ... socksvr { socket-name /var/run/vpp/vpp-api.sock } dpdk { dev 0000:13:00.0 { num-rx-queues 1 num-tx-queues 1 } } plugins { plugin default { enable } plugin dpdk_plugin.so { enable } plugin calico_plugin.so { enable } plugin ping_plugin.so { enable } plugin dispatch_trace_plugin.so { enable } } buffers { buffers-per-numa 131072 } vpp_dataplane_interface: ens192 vpp_uplink_driver: \"none\" ... kubectl create -f calico-vpp.yaml curl https://raw.githubusercontent.com/projectcalico/vpp-dataplane/v3.24.0/test/scripts/vppdev.sh | tee /usr/local/bin/calivppctl chmod +x /usr/local/bin/calivppctl driverctl list-devices 0000:0b:00.0 vmxnet3 0000:13:00.0 vfio-pci #### vfio-pci dpdk 纳管 calivppctl vppctl [NODENAME] calivppctl vppctl vpp62 http://www.tnblog.net/hb/article/details/7885 https://tnblog.net/hb/article/details/7233 https://slideplayer.com/slide/17437344/\n",
    "description": "",
    "tags": null,
    "title": "9. calico vpp vmware",
    "uri": "/vpp/9.calico_vpp_vmware/index.html"
  },
  {
    "content": "date: 2023-02-05\nvm.xml\n... \u003cinterface type='bridge'\u003e \u003csource bridge='br0'/\u003e \u003cmodel type='vmxnet3'/\u003e \u003c/interface\u003e \u003cinterface type='bridge'\u003e \u003csource bridge='br0'/\u003e \u003cmodel type='vmxnet3'/\u003e \u003c/interface\u003e ... ## HOST : 10.1.1.12 /opt/startVMs/startvm2.sh calicovpp_5.117 jammy-server-cloudimg-amd64.20221210.pw1-10.1.5.3.img 4 8 /opt/startVMs/startvm2.sh calicovpp_5.118 jammy-server-cloudimg-amd64.20221210.pw1-10.1.5.3.img 4 8 /opt/startVMs/startvm2.sh calicovpp_5.119 jammy-server-cloudimg-amd64.20221210.pw1-10.1.5.3.img 4 8 root@junnan-gpu:~# kvm --version QEMU emulator version 6.2.0 (Debian 1:6.2+dfsg-2ubuntu6.6) Copyright (c) 2003-2021 Fabrice Bellard and the QEMU Project developers kroot@junnan-gpu:~# kvm -device ? ... Network devices: name \"e1000\", bus PCI, alias \"e1000-82540em\", desc \"Intel Gigabit Ethernet\" name \"e1000-82544gc\", bus PCI, desc \"Intel Gigabit Ethernet\" name \"e1000-82545em\", bus PCI, desc \"Intel Gigabit Ethernet\" ... name \"virtio-net-pci\", bus PCI, alias \"virtio-net\" name \"virtio-net-pci-non-transitional\", bus PCI name \"virtio-net-pci-transitional\", bus PCI name \"vmxnet3\", bus PCI, desc \"VMWare Paravirtualized Ethernet v3\" ... 开始用e1000，不用DPDK，成功；采用DPDK，网络断掉，不成功。 采用vmxnet3，DPDK 成功！ ## VM : 10.1.5.117 , 10.1.5.118 , 10.1.5.119 ######### 启用rc.local cat \u003c\u003c EOF \u003e\u003e /etc/rc.local #!/bin/bash echo 1 \u003e /sys/module/vfio/parameters/enable_unsafe_noiommu_mode EOF chmod +x /etc/rc.local cat \u003c\u003c EOF \u003e\u003e /lib/systemd/system/rc-local.service [Install] WantedBy=multi-user.target EOF cat /lib/systemd/system/rc-local.service # 启用服务 systemctl enable rc-local systemctl start rc-local systemctl status rc-local cat /sys/module/vfio/parameters/enable_unsafe_noiommu_mode hostnamectl set-hostname calvpp1 cat \u003c\u003c EOF \u003e\u003e /etc/hosts 10.1.5.111 calvpp1 10.1.5.112 calvpp2 10.1.5.113 calvpp3 EOF ln -sf ../usr/share/zoneinfo/Asia/Shanghai /etc/localtime apt update \u0026\u0026 \\ apt -y full-upgrade \u0026\u0026 \\ [ -f /var/run/reboot-required ] \u0026\u0026 reboot -f echo \"vfio-pci\" \u003e /etc/modules-load.d/95-vpp.conf swapoff -a sed -i '/swap/ s/^\\(.*\\)$/#\\1/g' /etc/fstab free -h tee /etc/modules-load.d/containerd.conf \u003c\u003cEOF overlay br_netfilter EOF tee /etc/modules-load.d/ipvs.conf \u003c\u003cEOF ip_vs ip_vs_rr ip_vs_wrr ip_vs_sh br_netfilter bridge nf_conntrack EOF cat \u003c\u003cEOF \u003e\u003e /etc/sysctl.conf net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 vm.swappiness=0 vm.nr_hugepages = 1024 EOF sysctl -p cat /proc/meminfo | grep Huge apt install driverctl -y lshw -businfo -c network driverctl list-devices export https_proxy=http://10.1.1.12:8118 wget https://github.com/containerd/containerd/releases/download/v1.6.14/cri-containerd-cni-1.6.14-linux-amd64.tar.gz tar zxvf cri-containerd-cni-1.6.14-linux-amd64.tar.gz -C / mv /etc/cni/net.d/10-containerd-net.conflist /etc/cni/net.d/10-containerd-net.conflist.bak mkdir -p /etc/containerd containerd config default \u003e /etc/containerd/config.toml sed -i 's#registry.k8s.io#registry.aliyuncs.com/google_containers#g' /etc/containerd/config.toml # pause:3.6 ---\u003e\u003e\u003e pause:3.8 sed -i 's/pause:3.6/pause:3.8/g' /etc/containerd/config.toml sed -i 's/SystemdCgroup = false/SystemdCgroup = true/g' /etc/containerd/config.toml systemctl daemon-reload systemctl enable containerd.service systemctl restart containerd.service systemctl status containerd.service apt install -y apt-transport-https socat curl https://mirrors.aliyun.com/kubernetes/apt/doc/apt-key.gpg | apt-key add - cat \u003c\u003cEOF \u003e/etc/apt/sources.list.d/kubernetes.list deb https://mirrors.aliyun.com/kubernetes/apt/ kubernetes-xenial main EOF apt update apt install -y kubeadm=1.25.5-00 kubelet=1.25.5-00 kubectl=1.25.5-00 systemctl enable kubelet.service --now kubeadm config print init-defaults --component-configs KubeletConfiguration # cgroupDriver: systemd # # kubeadm config images pull kubeadm config images pull --kubernetes-version=v1.25.5 --image-repository registry.aliyuncs.com/google_containers kubeadm init --kubernetes-version=v1.25.5 \\ --image-repository registry.aliyuncs.com/google_containers \\ --pod-network-cidr=10.244.0.0/16 \\ --apiserver-advertise-address=10.1.5.111 cat \u003c\u003cEOF \u003e\u003e ~/.bashrc alias kp='kubectl get pod -o wide --all-namespaces' alias wkp='watch -n 1 kubectl get pod -o wide --all-namespaces' alias ks='kubectl get svc -o wide --all-namespaces' alias kn='kubectl get node -o wide --all-namespaces' alias k='kubectl' EOF #kubectl taint nodes --all node-role.kubernetes.io/control-plane- #kubectl taint nodes --all node-role.kubernetes.io/master- wget https://raw.githubusercontent.com/projectcalico/calico/v3.24.5/manifests/tigera-operator.yaml cat \u003c\u003c EOF \u003e calicovpp.img.pull.sh time crictl pull docker.io/calico/apiserver:v3.24.5 time crictl pull docker.io/calico/cni:v3.24.5 time crictl pull docker.io/calico/kube-controllers:v3.24.5 time crictl pull docker.io/calico/node:v3.24.5 time crictl pull docker.io/calico/pod2daemon-flexvol:v3.24.5 time crictl pull docker.io/calico/typha:v3.24.5 time crictl pull docker.io/calicovpp/agent:v3.24.0 time crictl pull docker.io/calicovpp/vpp:v3.24.0 time crictl pull quay.io/tigera/operator:v1.28.5 EOF bash -xv calicovpp.img.pull.sh wget https://raw.githubusercontent.com/projectcalico/calico/v3.24.5/manifests/tigera-operator.yaml wget https://raw.githubusercontent.com/projectcalico/vpp-dataplane/v3.24.0/yaml/calico/installation-default.yaml wget https://raw.githubusercontent.com/projectcalico/vpp-dataplane/v3.24.0/yaml/generated/calico-vpp.yaml curl https://raw.githubusercontent.com/projectcalico/vpp-dataplane/v3.24.0/test/scripts/vppdev.sh | tee /usr/local/bin/calivppctl chmod +x /usr/local/bin/calivppctl kubectl create -f tigera-operator.yaml vi installation-default.yaml apiVersion: operator.tigera.io/v1 kind: Installation metadata: name: default spec: # Configures Calico networking. calicoNetwork: linuxDataplane: VPP ipPools: - blockSize: 24 cidr: 10.244.0.0/16 encapsulation: None ##### NO IPIP \u0026\u0026 NO VXLAN --- kubectl create -f installation-default.yaml root@calvpp1:~/calico# lshw -businfo -c network Bus info Device Class Description ======================================================== pci@0000:00:03.0 ens3 network VMXNET3 Ethernet Controller pci@0000:00:04.0 ens4 network VMXNET3 Ethernet Controller root@calvpp1:~/calico# driverctl list-devices | grep vmxnet3 0000:00:03.0 vmxnet3 0000:00:04.0 vmxnet3 vi calico-vpp.yaml ... socksvr { socket-name /var/run/vpp/vpp-api.sock } dpdk { dev 0000:00:04.0 { num-rx-queues 1 num-tx-queues 1 } } plugins { plugin default { enable } plugin dpdk_plugin.so { enable } plugin calico_plugin.so { enable } plugin ping_plugin.so { enable } plugin dispatch_trace_plugin.so { enable } } buffers { buffers-per-numa 131072 } vpp_dataplane_interface: ens3 vpp_uplink_driver: \"none\" ... kubectl create -f calico-vpp.yaml ### 采用e1000，DPDK不成功，网络丢失，换e1000为vmxnet3，成功！ ### NO DPDK 成功！ root@calvpp1:~/calico# diff 2.installation-default.yaml installation-default.yaml.org 11,14d10 \u003c ipPools: \u003c - blockSize: 24 \u003c cidr: 10.244.0.0/16 \u003c encapsulation: None root@calvpp1:~/calico# diff 3.calico-vpp.yaml calico-vpp.yaml.org 164c164 \u003c vpp_dataplane_interface: ens3 --- \u003e vpp_dataplane_interface: eth1 ### NO DPDK 成功！ ########## #export https_proxy=http://10.1.1.12:8118 wget https://raw.githubusercontent.com/projectcalico/calico/v3.24.5/manifests/tigera-operator.yaml wget https://raw.githubusercontent.com/projectcalico/vpp-dataplane/v3.24.0/yaml/calico/installation-default.yaml wget https://raw.githubusercontent.com/projectcalico/vpp-dataplane/v3.24.0/yaml/generated/calico-vpp.yaml curl https://raw.githubusercontent.com/projectcalico/vpp-dataplane/v3.24.0/test/scripts/vppdev.sh | tee /usr/local/bin/calivppctl chmod +x /usr/local/bin/calivppctl kubectl create -f tigera-operator.yaml kubectl create -f installation-default.yaml kubectl create -f calico-vpp.yaml lshw -businfo -c network driverctl list-devices ip tuntap list iperf -s -i 1 iperf -t 10 -i 1 -c 10.1.5.113 ",
    "description": "",
    "tags": null,
    "title": "10. calico vpp qemu",
    "uri": "/vpp/10.calico_vpp_qemu/index.html"
  },
  {
    "content": "date: 2023-02-05 http://mirrors.ustc.edu.cn/ubuntu-cloud-images/jammy/ http://mirrors.ustc.edu.cn/ubuntu-cloud-images/jammy/20221219/jammy-server-cloudimg-amd64.ova 等待虚拟机导入完成，自动启动，完成初始化, 大约需要耐心等待1-3分钟时间，然后login： 大约需要耐心等待1-3分钟时间，然后login： 大约需要耐心等待1-3分钟时间，然后login： 默认登陆用户名是ubuntu, 密码即为刚才设定的密码. 首次登陆会要求修改密码。\n关闭虚拟机 jammy20221219 导入的虚拟机磁盘只有10G，扩到100G 删除CD等无用设备，进入系统，fdisk—\u003e resize 设置ssh远程登录： vi /etc/ssh/sshd_config # 修改如下配置, 将no改为yes PasswordAuthentication yes PermitRootLogin yes # permit root login systemctl restart sshd hostnamectl set-hostname jammy ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime sed -i 's/.*swap.*/#\u0026/' /etc/fstab cat \u003c\u003cEOF \u003e\u003e /etc/sysctl.conf net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 vm.swappiness=0 EOF sysctl -p cat \u003e\u003e /etc/systemd/system.conf \u003c\u003c EOF DefaultLimitNOFILE=65536 DefaultLimitNPROC=65536 DefaultLimitMEMLOCK=infinity EOF cat \u003c\u003c EOF \u003e\u003e /etc/security/limits.conf * soft nproc 65536 * soft nofile 65536 root soft nproc 65536 root soft nofile 65536 EOF vi /etc/default/motd-news ENABLED=0 #vi /etc/netplan/01-netcfg.yaml nano /etc/netplan/01-netcfg.yaml network: version: 2 renderer: networkd ethernets: ens3: addresses: - 10.1.5.2/21 nameservers: addresses: [114.114.114.114, 8.8.8.8] routes: - to: default via: 10.1.1.1 netplan apply sed -i 's/archive.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.list sed -i 's/security.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.list apt update apt remove cups -y apt autoremove -y apt update apt upgrade -y apt install chrony -y systemctl enable chronyd --new chronyc sources -v fdisk -l df -hT fdisk /dev/sda\nroot@ubuntuguest:~# fdisk /dev/sda Command (m for help): p ... Device Start End Sectors Size Type /dev/sda1 227328 20971486 20744159 9.9G Linux filesystem /dev/sda14 2048 10239 8192 4M BIOS boot /dev/sda15 10240 227327 217088 106M EFI System ... Command (m for help): d Partition number (1,14,15, default 15): 1 Partition 1 has been deleted. Command (m for help): n Partition number (1-13,16-128, default 1): First sector (34-209715166, default 227328): Last sector, +/-sectors or +/-size{K,M,G,T,P} (227328-209715166, default 209715166): Created a new partition 1 of type 'Linux filesystem' and of size 99.9 GiB. Partition #1 contains a ext4 signature. Do you want to remove the signature? [Y]es/[N]o: y The signature will be removed by a write command. Command (m for help): p Disk /dev/sda: 100 GiB, 107374182400 bytes, 209715200 sectors Disk model: VMware Virtual S Units: sectors of 1 * 512 = 512 bytes Sector size (logical/physical): 512 bytes / 512 bytes I/O size (minimum/optimal): 512 bytes / 512 bytes Disklabel type: gpt Disk identifier: 1E731F92-2DD4-402F-87EB-C5D722AE131B Device Start End Sectors Size Type /dev/sda1 227328 209715166 209487839 99.9G Linux filesystem /dev/sda14 2048 10239 8192 4M BIOS boot /dev/sda15 10240 227327 217088 106M EFI System Filesystem/RAID signature on partition 1 will be wiped. Partition table entries are not in disk order. Command (m for help): w The partition table has been altered. Syncing disks. resize2fs /dev/sda1 root@ubuntuguest:~# resize2fs /dev/sda1 resize2fs 1.46.5 (30-Dec-2021) Filesystem at /dev/sda1 is mounted on /; on-line resizing required old_desc_blocks = 2, new_desc_blocks = 13 The filesystem on /dev/sda1 is now 26185979 (4k) blocks long. root@ubuntuguest:~# df -hT Filesystem Type Size Used Avail Use% Mounted on tmpfs tmpfs 792M 1.4M 791M 1% /run /dev/sda1 ext4 97G 1.5G 96G 2% / tmpfs tmpfs 3.9G 0 3.9G 0% /dev/shm tmpfs tmpfs 5.0M 0 5.0M 0% /run/lock /dev/sda15 vfat 105M 5.3M 100M 5% /boot/efi tmpfs tmpfs 792M 4.0K 792M 1% /run/user/0 ",
    "description": "",
    "tags": null,
    "title": "1. import ubuntu-cloud-images",
    "uri": "/ubuntu/1.ubuntu-cloud-images/index.html"
  },
  {
    "content": "date: 2023-02-06 http://mirrors.ustc.edu.cn/ubuntu-cloud-images/jammy/\n# 10.1.1.12 : /u01/img cd /u01/img wget http://mirrors.ustc.edu.cn/ubuntu-cloud-images/jammy/20230110/jammy-server-cloudimg-amd64.img mv jammy-server-cloudimg-amd64.img jammy-server-cloudimg-amd64.20230110.img qemu-img info jammy-server-cloudimg-amd64.20230110.img qemu-img resize jammy-server-cloudimg-amd64.20230110.img 100G virt-customize -a jammy-server-cloudimg-amd64.20230110.img --root-password password:1 ../startVMs/startvm.sh jammy jammy-server-cloudimg-amd64.20230110.img 2 4 virsh console jammy fdisk -l fdisk /dev/vda p d 1 p n 1 y p w resize2fs /dev/vda1 cat \u003c\u003c EOF \u003e /etc/netplan/01-netcfg.yaml network: version: 2 renderer: networkd ethernets: ens3: addresses: - 10.1.5.3/21 nameservers: addresses: [114.114.114.114, 8.8.8.8] routes: - to: default via: 10.1.1.1 EOF netplan apply sed -i 's/archive.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.list sed -i 's/security.ubuntu.com/mirrors.ustc.edu.cn/g' /etc/apt/sources.list apt update apt install -y openssh-server net-tools inetutils-ping systemctl status sshd #systemctl start sshd /usr/sbin/sshd -T ssh-keygen -A vi /etc/ssh/sshd_config PermitRootLogin yes PasswordAuthentication yes systemctl restart sshd netstat -ltnp apt remove cups -y apt autoremove -y apt update apt upgrade -y apt install chrony -y systemctl enable chronyd --new chronyc sources -v ln -sf ../usr/share/zoneinfo/Asia/Shanghai /etc/localtime sed -i 's/.*swap.*/#\u0026/' /etc/fstab cat \u003c\u003cEOF \u003e\u003e /etc/sysctl.conf net.ipv4.ip_forward = 1 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 vm.swappiness=0 EOF sysctl -p cat \u003e\u003e /etc/systemd/system.conf \u003c\u003c EOF DefaultLimitNOFILE=65536 DefaultLimitNPROC=65536 DefaultLimitMEMLOCK=infinity EOF cat \u003c\u003c EOF \u003e\u003e /etc/security/limits.conf * soft nproc 65536 * soft nofile 65536 root soft nproc 65536 root soft nofile 65536 EOF rm -rf /var/lib/apt/lists/* cat /var/lib/ubuntu-release-upgrader/release-upgrade-available rm /var/lib/ubuntu-release-upgrader/release-upgrade-available /usr/lib/ubuntu-release-upgrader/release-upgrade-motd cat /var/lib/ubuntu-release-upgrader/release-upgrade-available sed -i 's/ENABLED=1/ENABLED=0/g' /etc/default/motd-news ",
    "description": "",
    "tags": null,
    "title": "2. qemu u2204-cloudimg",
    "uri": "/ubuntu/2.qemu-u2204-cloudimg/index.html"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Categories",
    "uri": "/categories/index.html"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "ubuntu",
    "uri": "/categories/ubuntu/index.html"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "vpp",
    "uri": "/categories/vpp/index.html"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "dpdk",
    "uri": "/categories/dpdk/index.html"
  },
  {
    "content": "blogs of amwork2010 Email: amwork2010@foxmail.com\n日期: 2023-02-04\n",
    "description": "",
    "tags": null,
    "title": "blogs of amwork2010",
    "uri": "/index.html"
  },
  {
    "content": "",
    "description": "",
    "tags": null,
    "title": "Tags",
    "uri": "/tags/index.html"
  }
]
